Book,Chapter,Text,Concepts
AIMA,chapter16_3,"It is a straightforward matter to calculate the distribution of the maximum of the k estimates and hence quantify the extent of our disappointment. (This calculation is a special case of computing an order statistic, the distribution of any particular ranked element of a sample.) Suppose that each estimate Xi has a probability density function f(x) and cumulative distribution F(x).",order statistic
AIMA,chapter5_1,"The games most commonly studied within AI (such as chess and Go) are what game theorists call deterministic, two-player, turn-taking, perfect information, zero-sum games. “Perfect information” is a synonym for “fully observable, ”1 and “zero-sum” means that what is good for one player is just as bad for the other: there is no “win-win” outcome. For games we often use the term move as a synonym for “action” and position as a synonym for “state.”","Perfect information, Zero-sum games, Position, Move"
SLP,,"In this section, we briefly summarize some of the semantic properties of embeddings that have been studied.",semantic properties of embeddings
AIMA,chapter7_4,"Having specified the syntax of propositional logic, we now specify its semantics. The semantics defines the rules for determining the truth of a sentence with respect to a particular model. In propositional logic, a model simply sets the truth value—true or false— for every proposition symbol.",Truth value
SLP,,"Vector or distributional models of meaning are generally based on a co-occurrence matrix, a way of representing how often words co-occur. We'll look at two popular matrices: the term-document matrix and the term-term matrix.",term-document matrix
AIMA,chapter12_1,"there has been endless debate over the source and status of probability numbers. thefrequentist position is that the numbers can come only from experiments: if we test 100people and find that 10 of them have a cavity, then we can say that the probability of acavity is approximately 0.1. in this view, the assertion “the probability of a cavity is 0.1”means that 0.1 is the fraction that would be observed in the limit of infinitely many samplefrom any finite sample, we can estimate the true fraction and also calculate how accurateait eehmatea ic heals tr ha:",frequentist
AIMA,chapter7_1,"Like all our agents, it takes a percept as input and returns an action. The agent maintains a knowledge base, KB, which may initially contain some background knowledge. Because of the definitions of TELL and ASK, however, the knowledge-based  agent is not an arbitrary program for calculating actions. It is amenable to a description at  the knowledge level, where we need specify only what the agent knows and what its goals  are, in order to determine its behavior.","Background knowledge, Knowledge level"
AIMA,chapter17_1,". If the environment contains terminal states and if the agent is guaranteed to get to one eventually, then we will never need to compare infinite sequences. A policy that is guaranteed to reach a terminal state is called a proper policy. With proper policies, we can use γ = 1 (i.e., additive undiscounted rewards). The first three policies shown in Figure 17.2(b)  are proper, but the fourth is improper. It gains infinite total reward by staying away from the terminal states when the reward for transitions between nonterminal states is positive. The existence of improper policies can cause the standard algorithms for solving MDPs to fail with additive rewards, and so provides a good reason for using discounted rewards. Infinite sequences can be compared in terms of the average reward obtained per time step.","Proper policy, Average reward"
AIMA,chapter17_3,"There are several different definitions of bandit problems; one of the cleanest and most general is as follows: Each arm Mi is a Markov reward process or MRP, that is, an MDP with only one possible action ai. It has states Si, transition model Pi(s′|s, ai), and reward Ri(s, ai, s′). The arm defines a distribution over sequences of rewards Ri, 0, Ri, 1, Ri, 2, …, where each Ri, t is a random variable. The overall bandit problem is an MDP: the state space is given by the Cartesian product S = S1 × ⋯ × Sn; the actions are a1, …, an; the transition model updates the state of whichever arm Mi is selected, according to its specific transition model, leaving the other arms unchanged; and the discount factor is γ.","Bandit problems, Markov reward process"
AIMA,chapter3_2,"A standardized problem is intended to illustrate or exercise various problem-solving methods. It can be given a concise, exact description and hence is suitable as a benchmark for researchers to compare the performance of algorithms. A real-world problem, such as robot navigation, is one whose solutions people actually use, and whose formulation is idiosyncratic, not standardized, because, for example, each robot has different sensors that produce different data.","real-world problem, standardized problem"
AIMA,,"to make such choices, an agent must first have preferences among the different possible outcomes of the various plans. an outcome is a completely specified state, including such factors as whether the agent arrives on time and the length of the wait at the airport. we use utility theory to represent preferences and reason quantitatively with them. (the term utility is used here in the sense of “the quality of being useful, ” not in the sense of the electric company or water works.) utility theory says that every state (or state sequence) has a degree of usefulness, or utility, to an agent and that the agent will prefer states with highe1","outcome, utility theory, preference"
SLP,,Predicting the next word (rather than feeding the model its best case from the previous time step) is called teacher forcing.,teacher forcing
AIMA,chapter7_1,"The central component of a knowledge-based agent is its knowledge base, or KB. A knowledge base is a set of sentences. (Here “sentence” is used as a technical term. It is related but not identical to the sentences of English and other natural languages.) Each sentence is expressed in a language called a knowledge representation language and represents some assertion about the world. When the sentence is taken as being given without being derived from other sentences, we call it an axiom.","Knowledge-base, Sentence, Knowledge representation language, Axiom"
AIMA,chapter5_3,"Alpha–beta pruning prunes branches of the tree that can have no effect on the final evaluation, but forward pruning prunes moves that appear to be poor moves, but might possibly be good ones. Thus, the strategy saves computation time at the risk of making an error.",Forward pruning
SLP,,"Domain and bear structured relations with each other. For example, words might be related by being in the semantic field of hospitals (surgeon, scalpel, nurse, anesthetic, hospital), restaurants (waiter, menu, plate, food, chef), or houses (door, roof, kitchen, family, bed). Semantic fields are also related to topic models, like latent dirichlet allocation, lda, which apply unsupervised learning on large sets of texts to induce sets of associated words from text. Semantic fields and topic models are",topic models
SLP,slp-7,"Finally, the chain rule of derivatives. Suppose we are computing the derivative of a composite function f(x) = u(v(x)). The derivative of f(x) is the derivative of u(x) with respect to v(x) times the derivative of v(x) with respect to x.",chain rule
SLP,,"In supervised learning, the agent observes input-output pairs and learns a function that maps from input to output. For example, the inputs could be camera images, each one accompanied by an output saying “bus” or “pedestrian,” etc. An output like this is called a label. The agent learns a function that, when given a new image, predicts the appropriate label. In the case of braking actions (component 1 above), an input is the current state (speed and direction of the car, road condition), and an output is the distance it took to stop. In this case, a set of output values can be obtained by the agent from its own percepts (after the fact); the environment is the teacher, and the agent learns a function that maps states to stopping distance.","supervised learning, label"
AIMA,chapter2_2,"Our definition of rationality does not require omniscience, then, because the rational choice depends only on the percept sequence to date. We must also ensure that we haven’t inadvertently allowed the agent to engage in decidedly underintelligent activities. For example, if an agent does not look both ways before crossing a busy road, then its percept sequence will not tell it that there is a large truck approaching at high speed. Does our definition of rationality say that it’s now OK to cross the road? Far from it! First, it would not be rational to cross the road given this uninformative percept sequence: the risk of accident from crossing without looking is too great. Second, a rational agent should choose the “looking” action before stepping into the street, because looking helps maximize the expected performance. Doing actions in order to modify future percepts— sometimes called information gathering—is an important part of rationality and is covered in depth in Chapter 16 . A second example of information gathering is provided by the exploration that must be undertaken by a vacuum-cleaning agent in an initially unknown environment. Our definition requires a rational agent not only to gather information but also to learn as much as possible from what it perceives. The agent’s initial configuration could reflect some prior knowledge of the environment, but as the agent gains experience this may be modified and augmented. There are extreme cases in which the environment is completely known a priori and completely predictable. In such cases, the agent need not perceive or learn; it simply acts correctly.","Information gathering, Learning"
SLP,,"Tenets of semantics, called ""the principle of contrast"" (Glrard 1/16, Dreal 167/ Clark 1987), state that a difference in linguistic form is always associated with some difference in meaning. For example, the word ""H20"" is used in scientific contexts and would be inappropriate in a hiking guide—water would be more appropriate— and this genre difference is part of the meaning of the word. In practice, the word ""synonyn"" is therefore used to describe a relationship of approximate or rough synonymy.",principle of contrast
SLP,,"Stacked RNNs generally outperform single-layer networks. One reason for this success seems to be that the network induces representations at differing levels of abstraction across layers. Just as the early stages of the human visual system detect edges that are then used for finding larger regions and shapes, the initial layers of stacked networks can induce representations that serve as useful abstractions for further layers—representations that might prove difficult to induce in a single RNN. The optimal number of stacked RNNs is specific to each application and to each training set. However, as the number of stacks is increased the training costs rise quickly.",stacked rnns
AIMA,chapter22_2,"one solution to this problem, called experience replay, ensures that the car keeps relivingits youthful crashing behavior at regular intervals. the learning algorithm can retaintrajectories from the entire learning process and replay those trajectories to ensure that itsvalue function is still accurate for parts of the state space it no longer visits.",experience replay
AIMA,chapter3_5,"A* search is complete. Whether A* is cost-optimal depends on certain properties of the heuristic. A key property is admissibility: an admissible heuristic is one that never overestimates the cost to reach a goal. (An admissible heuristic is therefore optimistic.) With an admissible heuristic, A* is cost-optimal, which we can show with a proof by contradiction. Suppose the optimal path has cost but the algorithm returns a path with cost Then there must be some node which is on the optimal path and is unexpanded (because if all the nodes on the optimal path had been expanded, then we would have returned that optimal solution).",Admissible heuristic
SLP,slp_4,"By considering features in log space, eq. 4.10 computes the predicted class as a function of input features. Classifiers that use a linear combination of the input to make a classification decision — like naive bayes and also logistic regression — are called linear classifiers.",linear
SLP,,"However, z can’t be the output of the classifier, since it’s a vector of real-valued numbers, while what we need for classification is a vector of probabilities. There is one convenient function for normalizing a vector of real values, by which we mean converting it to a vector that encodes a probability distribution (all the numbers lie between 0 and 1 and sum to 1): the softmax function that we saw on page 126 of chapter 5. More generally for any vector z of dimensionality d, the softmax is defined as:",normalizing
AIMA,chapter22_1,"an alternative is reinforcement learning (rl), in which an agent interacts with the worldand periodically receives rewards (or, in the terminology of psychology, reinforcements)that reflect how well it is doing. for example, in chess the reward is 1 for winning, 0 forlosing, and 4 for a draw. we have already seen the concept of rewards in chapter 17 formarkov decision processes (mdps). indeed, the goal is the same in reinforcement learning:maximize the expected sum of rewards. reinforcement learning differs from “just solving anmdp” because the agent is not given the mdp as a problem to solve; the agent is in themdp. it may not know the transition model or the reward function, and it has to act in orderto learn more. imagine playing a new game whose rules you don’t know; after a hundred orso moves, the referee tells you “you lose.” that is reinforcement learning in a nutshell.",reinforcement learning
SLP,slp-7,"Lgistic regression solves this task by learning, from a training set, a vector of weights and a bias term. Each weight w; is a real number, and is associated with one of the input features x;. The weight w; represents how important that input feature is to the classification decision, and can be positive (providing evidence that the instance being classified belongs in the positive class) or negative (providing evidence that the instance being classified belongs in the negative class). Thus we might expect in a sentiment task the word awesome to have a high positive weight, and abysmal to have a very negative weight. The bias term, also called the intercept, is another real number that’s added to the weighted inputs.",intercept
AIMA,chapter6_1,"A domain, Di, consists of a set of allowable values, {v1, …, vk}, for variable Xi. For example, a Boolean variable would have the domain {true, false}. Different variables can have different domains of different sizes. Each constraint Cj consists of a pair ⟨scope, rel⟩, where scope is a tuple of variables that participate in the constraint and rel is a relation that defines the values that those variables can take on. A relation can be represented as an explicit set of all tuples of values that satisfy the constraint, or as a function that can compute whether a tuple is a member of the relation.",Relation
SLP,,"1. A better name would have been ""function approximation"" or ""numeric prediction."" But in 1886, Francis Galton wrote an influential article on the concept of regression to the mean (e.g., the children of tall parents are likely to be taller than average, but not as tall as the parents). Galton showed plots with what he called ""regression lines,"" and readers came to associate the word ""regression"" with the statistical technique of function approximation rather than with the topic of regression to the mean.",regression
AIMA,chapter17_1,"Each time a given policy is executed starting from the initial state, the stochastic nature of the environment may lead to a different environment history. The quality of a policy is therefore measured by the expected utility of the possible environment histories generated by that policy. An optimal policy is a policy that yields the highest expected utility. We use π∗ to denote an optimal policy. Given π∗ , the agent decides what to do by consulting its current percept, which tells it the current state s, and then executing the action π∗(s). A policy represents the agent function explicitly and is therefore a description of a simple reflex agent, computed from the information used for a utility-based agent.",Optimal policy
AIMA,chapter3_2,"Robot navigation is a generalization of the route-finding problem described earlier. Rather than following distinct paths (such as the roads in Romania), a robot can roam around, in effect making its own paths. For a circular robot moving on a flat surface, the space is essentially two-dimensional. When the robot has arms and legs that must also be controlled, the search space becomes many-dimensional—one dimension for each joint angle. Advanced techniques are required just to make the essentially continuous search space finite. In addition to the complexity of the problem, real robots must also deal with errors in their sensor readings and motor controls, with partial observability, and with other agents that might alter the environment.",Robot navigation
AIMA,chapter2_1,"An agent is anything that can be viewed as perceiving its environment through sensors and acting upon that environment through actuators. This simple idea is illustrated in Figure 2.1 . A human agent has eyes, ears, and other organs for sensors and hands, legs, vocal tract, and so on for actuators. A robotic agent might have cameras and infrared range finders for sensors and various motors for actuators. A software agent receives file contents, network packets, and human input (keyboard/mouse/touchscreen/voice) as sensory inputs and acts on the environment by writing files, sending network packets, and displaying information or generating sounds. The environment could be everything—the entire universe! In practice it is just that part of the universe whose state we care about when designing this agent—the part that affects what the agent perceives and that is affected by the agent’s actions.","Environment, Sensor, Actuator"
SLP,,"Decision trees have a lot going for them: ease of understanding, scalability to large data sets, and versatility in handling discrete and continuous inputs as well as classification and regression. However, they can have suboptimal accuracy (largely due to the greedy search), and if trees are very deep, then getting a prediction for a new example can be expensive in run time. Decision trees are also unstable in that adding just one new example can change the test at the root, which changes the entire tree. In section 19.8.2 we will see that the random Forest model can fix some of these issues.",unstable
SLP,,"Suppose a researcher generates a hypothesis for one setting of the x” pruning hyperparameter, measures the error rates on the test set, and then tries different hyperparameters. No individual hypothesis has peeked at the test set data, but the o process did, through the researcher.","hyperparameters, error rate"
SLP,,"Recent research focuses on ways to try to remove these kinds of biases, for example by developing a transformation of the embedding space that removes gender stereotypes but preserves definitional gender (Bolukbasi et al. 2016, Zhao et al. 2017) or changing the training procedure (Zhao et al., 2018). However, although these sorts of debiasing may reduce bias in in embeddings, they do not eliminate it.",debiasing
AIMA,chapter3_6,"We have presented several fixed search strategies—breadth-first, A*, and so on—that have been carefully designed and programmed by computer scientists. Could an agent learn how to search better? The answer is yes, and the method rests on an important concept called the metalevel state space. Each state in a metalevel state space captures the internal (computational) state of a program that is searching in an ordinary state space such as the map of Romania. (To keep the two concepts separate, we call the map of Romania an object-level state space.) For example, the internal state of the A* algorithm consists of the current search tree. Each action in the metalevel state space is a computation step that alters the internal state; for example, each computation step in A* expands a leaf node and adds its successors to the tree. For harder problems, there will be many such missteps, and a  metalevel learning algorithm can learn from these experiences to avoid exploring  unpromising subtrees. The techniques used for this kind of learning are described in  Chapter 22. The goal of learning is to minimize the total cost of problem solving, trading  off computational expense and path cost.","Metalevel state space, Metalevel learning, Object-level state space"
AIMA,chapter22_3,"we begin in section 22.218 with passive reinforcement learning, where the agent's policy isfixed and the task is to learn the utilities of states (or of state—action pairs); this could alsoinvolve learning a model of the environment. (an understanding of markov decisionprocesses, as described in chapter 1714, is essential for this section.) section 22.38 coversactive reinforcement learning, where the agent must also figure out what to do. theprincipal issue is exploration: an agent must experience as much as possible of itsenvironment in order to learn how to behave in it. section 22.410 discusses how an agentcan use inductive learning (including deep learning methods) to learn much faster from itsexperiences. we also discuss other approaches that can help scale up rl to solve realproblems, including providing intermediate pseudorewards to guide the learner andorganizing behavior into a hierarchy of actions. section 22.519 covers methods for policysearch. in section 22.6, we explore apprenticeship learning: training a learning agentusing demonstrations rather than reward signals. finally, section 22.718 reports on",passive reinforcement learning
AIMA,chapter16_3,"Currently several agencies of the U.S. government, including the Environmental Protection Agency, the Food and Drug Administration, and the Department of Transportation, use the value of a statistical life to determine the costs and benefits of regulations and interventions. Typical values in 2019 are roughly $10 million. Some attempts have been made to find out the value that people place on their own lives.  One common “currency” used in medical and safety analysis is the micromort, a one in a  million chance of death. If you ask people how much they would pay to avoid a risk—for  example, to avoid playing Russian roulette with a million-barreled revolver—they will  respond with very large numbers, perhaps tens of thousands of dollars, but their actual  behavior reflects a much lower monetary value for a micromort.","Value of a statistical life, Micromort"
SLP,,"Result from it being very close to linear. In the sigmoid or tanh functions, very high values of z result in values of y that are saturated, i.e., extremely close to 1, and have derivatives very close to 0. Zero derivatives cause problems for learning, because as we’ll see in section 7.6, we’ll train networks by propagating an error signal backwards, multiplying gradients (partial derivatives) from each layer of the network; gradients that are almost 0 cause the error signal to get smaller and smaller until it is too small to be used for training, a problem called the vanishing gradient problem. Rectifiers don’t have this problem, since the derivative of relu for high values of z is | rather than very close to 0.",vanishing gradient
AIMA,chapter6_2,"A single variable (corresponding to a node in the CSP graph) is node-consistent if all the values in the variable’s domain satisfy the variable’s unary constraints. For example, in the variant of the Australia map-coloring problem where South Australians dislike green, the variable SA starts with domain {red, green, blue}, and we can make it node consistent by eliminating green, leaving SA with the reduced domain {red, blue}. We say that a graph is node-consistent if every variable in the graph is node-consistent.",Node consistency
SLP,slp_4,"The bootstrap test (Efron and Tibshirani, 1993) can apply to any metric; from precision, recall, or F1 to the BLEU metric used in machine translation. The word ""bootstrapping"" refers to repeatedly drawing large numbers of smaller samples with replacement (called bootstrap samples) from an original larger sample. The intuition of the bootstrap test is that we can create many virtual test sets from an observed test set by repeatedly sampling from it. The method only makes the assumption that the sample is representative of the population.",bootstrapping
AIMA,chapter6_2,"Stronger forms of propagation can be defined with the notion of k-consistency. A CSP is k-consistent if, for any set of k − 1 variables and for any consistent assignment to those variables, a consistent value can always be assigned to any kth variable. 1-consistency says that, given the empty set, we can make any set of one variable consistent: this is what we called node consistency. 2-consistency is the same as arc consistency. For binary constraint graphs, 3-consistency is the same as path consistency.",k-consistency
AIMA,chapter17_3,"Examples of BSPs include daily life, where one can attend to one task at a time, even though several tasks may need attention; project management with multiple projects; teaching with multiple pupils needing individual guidance; and so on. The ordinary term for this is multitasking. It is so ubiquitous as to be barely noticeable: when formulating a real-world decision problem, decision analysts rarely ask if their client has other, unrelated problems.",Multitasking
SLP,,"Fig. 7.5 shows the possible logical inputs (00, 01, 10, and 11) and the line drawn by one possible set of parameters for an AND and an OR classifier. Notice that there is simply no way to draw a line that separates the positive cases of XOR (01 and 10) from the negative cases (00 and 11). We say that XOR is not a linearly separable function. Of course we could draw a boundary with a curve, or some other function, but not a single line.",linearly separable
SLP,,"Let's see how this happens. Like the hidden layer, the output layer has a weight matrix (let's call it u), but some models don't include a bias vector b in the output.",output layer
SLP,slp_4,"Rules can be fragile, however, as situations or data change over time, and for some tasks humans aren’t necessarily good at coming up with the rules. Most cases of classification in language processing are instead done via supervised machine learning, and this will be the subject of the remainder of this chapter. In supervised learning, we have a data set of input observations, each associated with some correct output (a ‘supervision signal’). The goal of the algorithm is to learn how to map from a new observation to a correct output.",supervised machine learning
SLP,slp-7,‘The sigmoid function 0(z) = y_ = takes a real value and maps it to the range [1]. It is nearly linear around 0 but outlier values get squashed toward 0 or 1.,the sigmoid function
SLP,slp_4,Spam detection is another important commercial application. The binary classification task of assigning an email to one of the two classes spam or not-spam many lexical and other features can be used to perform this classification. For example you might quite reasonably be suspicious of an email containing phrases like “online pharmaceutical” or “without any cost” or “dear winner’.,spam detection
AIMA,chapter2_2,"We need to be careful to distinguish between rationality and omniscience. An omniscient agent knows the actual outcome of its actions and can act accordingly; but omniscience is impossible in reality. Consider the following example: I am walking along the Champs Elysées one day and I see an old friend across the street. There is no traffic nearby and I’m not otherwise engaged, so, being rational, I start to cross the street. Meanwhile, at 33, 000 feet, a cargo door falls off a passing airliner, 3 and before I make it to the other side of the street I am flattened. Was I irrational to cross the street? It is unlikely that my obituary would read “Idiot attempts to cross street.” This example shows that rationality is not the same as perfection. Rationality maximizes expected performance, while perfection maximizes actual performance. Retreating from a requirement of perfection is not just a question of being fair to agents. The point is that if we expect an agent to do what turns out after the fact to be the best action, it will be impossible to design an agent to fulfill this specification—unless we improve the performance of crystal balls or time machines.",Omniscience
AIMA,chapter3_5,Greedy best-first search is a form of best-first search that expands first the node with the lowest h(n) value—the node that appears to be closest to the goal—on the grounds that this is likely to lead to a solution quickly. So the evaluation function f(n) = h(n).,Greedy best-first search
SLP,,"Mechanical indexing, now known as information retrieval, began in the early 1800s. However, it was not until the late 1900s that researchers truly began to understand how to define the meaning of words in terms of vectors (Switzer, 1965). They also refined methods for word similarity based on measures of statistical association between words, like mutual information (Giuliano, 1965) and idf (Sparck Jones, 1972). They showed that the meaning of documents could be represented in the same vector spaces used for words.",mechanical indexing
AIMA,chapter22_1,"it turns out, however, that a little bit of expertise can go a long way in reinforcementlearning. the two examples in the preceding paragraph—the win/loss rewards for chess anracing—are what we call sparse rewards, because in the vast majority of states the agent isgiven no informative reward signal at all. in games such as tennis and cricket, we can easil}supply additional rewards for each point won or for each run scored. in car racing, we coulreward the agent for making progress around the track in the right direction. when learninto crawl, any forward motion is an achievement. these intermediate rewards make learnin;",sparse
AIMA,chapter6_3,"This intuitive idea—choosing the variable with the fewest “legal” values—is called the minimum-remaining-values (MRV) heuristic. It also has been called the “most constrained variable” or “fail-first” heuristic, the latter because it picks a variable that is most likely to cause a failure soon, thereby pruning the search tree. If some variable X has no legal values left, the MRV heuristic will select X and failure will be detected immediately—avoiding pointless searches through other variables. The MRV heuristic usually performs better than a random or static ordering, sometimes by orders of magnitude, although the results vary depending on the problem.",minimum-remaining-values
AIMA,chapter22_3,"we start with the simple case of a fully observable environment with a small number ofactions and states, in which an agent already has a fixed policy 7(s) that determines itsactions. the agent is trying to learn the utility function u""(s)—the expected total discountedreward if policy 7 is executed beginning in state s. we call this a passive learning agent.",passive learning agent
SLP,slp-7,"Tension has just one minimum; there are no local minima to get stuck in, so gradient descent starting from any point is guaranteed to find the minimum. (By contrast, the loss for multi-layer neural networks is non-convex, and gradient descent may get stuck in local minima for neural network training and never find the global optimum.)",gradient
SLP,,‘Various forms of regularization are used to prevent overfitting. One of the most important is dropout: randomly dropping some units and their connections from the network during training (Hinton et al. 2012. Srivastava et al. 2014). Tuning,dropout
SLP,,"By variance we mean the amount of change in the hypotnesis due to fluctuation in the training data. The two rows of figure 19.1 represent data sets that were each sampled from the same f(x) function. The data sets turned out to be slightly different. For the first three columns, the small difference in the data set translates into a small difference in the hypothesis. We call that low variance. But the degree-12 polynomials in the fourth column have high variance: look how different the two functions are at both ends of the a-axis. Clearly, at least one of these polynomials must be a poor approximation to the true f(«). We say a function is overfitting the data when it pays too much attention to the particular data sp i an ela a om ca.","variance, overfitting"
AIMA,chapter22_3,"the first approach, bayesian reinforcement learning, assumes a prior probability p(h) overhypotheses h about what the true model is; the posterior probability p(h|e) is obtained inthe usual way by bayes’ rule given the observations to date. then, if the agent has decidedto stop learning, the optimal policy is the one that gives the highest expected utility. let u7be the expected utility, averaged over all possible start states, obtained by executing policy 2in model h. then we have",bayesian reinforcement learning
SLP,,"For some tasks, it’s OK to freeze the embedding layer E with initial Word2Vec values. Freezing means we use Word2Vec or some other pretraining algorithm to compute the initial embedding matrix E, and then hold it constant while we only modify W, U, and B, i.e., we don’t update E during language model training. However, often we'd like to learn the embeddings simultaneously with training the network. This is useful when the task the network is designed for (like sentiment classification, translation, or parsing) places strong constraints on what makes a good representation for words.",freeze
AIMA,chapter7_4,"The syntax of propositional logic defines the allowable sentences. The atomic sentences consist of a single proposition symbol. Each such symbol stands for a proposition that can be true or false. We use symbols that start with an uppercase letter and may contain other letters or subscripts, for example: P, Q, R, W1, 3, and FacingEast. The names are arbitrary but are often chosen to have some mnemonic value—we use W1, 3 to stand for the proposition that the wumpus is in [1, 3]. (Remember that symbols such as W1, 3 are atomic, i.e., W, 1, and 3 are not meaningful parts of the symbol.) There are two proposition symbols with fixed meanings: True is the always-true proposition and False is the always-false proposition. Complex sentences are constructed from simpler sentences, using parentheses and operators called logical connectives. There are five connectives in common use:","Atomic sentences, Proposition symbol, Complex sentences, Logical connectives"
SLP,,"First, we will make the assumption that the future examples will be like the past. We call this the ""stationarity assumption;"" without it, all bets are off. We assume that each example has the same prior probability distribution.",Stationarity
AIMA,chapter3_2,"A VLSI layout problem requires positioning millions of components and connections on a chip to minimize area, minimize circuit delays, minimize stray capacitances, and maximize manufacturing yield.",VLSI layout
AIMA,chapter7_5,"Forward chaining is an example of the general concept of data-driven reasoning—that is, reasoning in which the focus of attention starts with the known data. It can be used within an agent to derive conclusions from incoming percepts, often without a specific query in mind. For example, the wumpus agent might TELL its percepts to the knowledge base using an incremental forward-chaining algorithm in which new facts can be added to the agenda to initiate new inferences. In humans, a certain amount of data-driven reasoning occurs as new information arrives. For example, if I am indoors and hear rain starting to fall, it might occur to me that the picnic will be canceled. Yet it will probably not occur to me that the seventeenth petal on the largest rose in my neighbor’s garden will get wet; humans keep forward chaining under careful control, lest they be swamped with irrelevant consequences.",Data-driven
SLP,,"To visualize how perplexity can be computed as a function of the probabilities our LM will compute for each new word, we can use the chain rule to expand the computation of probability of the test set.",perplexity
AIMA,chapter2_3,"The code repository associated with this book (aima.cs.berkeley.edu) includes multiple environment implementations, together with a general-purpose environment simulator for evaluating an agent’s performance. Experiments are often carried out not for a single environment but for many environments drawn from an environment class. For example, to evaluate a taxi driver in simulated traffic, we would want to run many simulations with different traffic, lighting, and weather conditions. We are then interested in the agent’s average performance over the environment class.",Environment class
SLP,,Rated = part-of-speech tagging as sequence labeling with a simple RNN. Pre-trained word embeddings serve as inputs and a softmax layer provides a probability distribution over the part-of-speech tags as output at each time step.,embeddings
SLP,,"Term-document matrices were originally defined as a means of finding similar documents for the task of document information retrieval. Two documents that are similar will tend to have similar words, and if two documents have similar words their column vectors will tend to be similar. The vectors for the comedies ""As You Like It"" [1, 114, 36, 20] and ""Twelfth Night"" [0, 80, 58, 15] look a lot more like each other (more fools and wit than battles) than they look like ""Julius Caesar"" [7, 62, 1, 2] or ""Henry V"" [13, 89, 4, 3]. This is clear with the raw numbers; in the first dimension (battle) the comedies have low numbers and the others have high numbers, and we can see it visually in fig. 6.4; we'll see very shortly how to quantify this intuition more formally.",vector
SLP,,"We can answer this question by using a statistical significance test. Such a test begins by assuming that there is no underlying pattern (the so-called null hypothesis). Then the actual data are analyzed to calculate the extent to which they deviate from a perfect absence of pattern. If the degree of deviation is statistically unlikely (usually taken to mean a 5% probability or less), then that is considered to be good evidence for the presence of a significant pattern in the data. The probabilities are calculated from standard distributions of the amount of deviation one would expect to see in random sampling.","significance test, null hypothesis"
AIMA,chapter1_3,"The Greek philosopher Aristotle was one of the first to attempt to codify “right thinking”— that is, irrefutable reasoning processes. His syllogisms provided patterns for argument structures that always yielded correct conclusions when given correct premises. The canonical example starts with Socrates is a man and all men are mortal and concludes that Socrates is mortal. (This example is probably due to Sextus Empiricus rather than Aristotle.) These laws of thought were supposed to govern the operation of the mind; their study initiated the field called logic.",Syllogisms
AIMA,chapter6_1,"The simplest kind of CSP involves variables that have discrete, finite domains. Map-coloring problems and scheduling with time limits are both of this kind. The 8-queens problem can also be viewed as a finite-domain CSP, where the variables Q1, …, Q8 correspond to the queens in columns 1 to 8, and the domain of each variable specifies the possible row numbers for the queen in that column, Di = {1, 2, 3, 4, 5, 6, 7, 8}. The constraints say that no two queens can be in the same row or diagonal.","Discrete domain, Finite domain"
SLP,,"Most neural NLP applications do something different, however. Instead of using hand-built human-engineered features as the input to our classifier, we draw on deep learning’s ability to learn features from the data by representing words as embeddings, like the word2vec or glove embeddings we saw in chapter 6. There are various ways to represent an input for classification. One simple baseline is to apply some sort of pooling function to the embeddings of all the words in the input. For example, for a text with n input words/tokens w1, ..., wn, we can turn the n embeddings e(w1), ..., e(wn) (each of dimensionality d) into a single embedding also of dimensionality d by just summing the embeddings, or by taking their mean.",pooling
AIMA,chapter3_3,"It is important to understand the distinction between the state space and the search tree. The state space describes the (possibly infinite) set of states in the world, and the actions that allow transitions from one state to another. The search tree describes paths between these states, reaching towards the goal. The search tree may have multiple paths to (and thus multiple nodes for) any given state, but each node in the tree has a unique path back to the root (as in all trees).The rootnode of the search tree is at the initial state, Arad. We can expand the node, by considering the available ACTIONS for that state, using the RESULT function to see where those actions lead to, and generating a new node (called a child node or successor node) for each of the  resulting states. Each child node has Arad as its parent node.","Generating, Child node, Successor node, Parent node"
SLP,,"The second factor in TF-IDF is used to give a higher weight to words that occur only in a few documents. Terms that are limited to a few documents are useful for discriminating those documents from the rest of the collection; terms that occur frequently across the entire collection aren’t as helpful. The document frequency df, of a term f is the number of documents it occurs in. Document frequency is not the same as the collection frequency of a term, which is the total number of times the word appears in the whole collection in any document. Consider in the collection of shakespeare’s 37 plays the two words romeo and action. The words have identical collection frequencies (they both occur 113 times in all the plays) but very different document frequencies, since romeo only occurs in a single play. If our goal is to find documents about the romantic tribulations of romeo, the word romeo should be highly weighted, but not action.",document frequency
AIMA,chapter5_3,"Another technique, late move reduction, works under the assumption that move ordering has been done well, and therefore moves that appear later in the list of possible moves are less likely to be good moves. But rather than pruning them away completely, we just reduce the depth to which we search these moves, thereby saving time. If the reduced search comes back with a value above the current α value, we can re-run the search with the full depth.",Late move reduction
AIMA,chapter7_4,"⇔ (if and only if). The sentence W1, 3 ⇔ ¬W2, 2 is a biconditional.",Biconditional
AIMA,chapter3_2,"Touring problems describe a set of locations that must be visited, rather than a single goal destination. The traveling salesperson problem (TSP) is a touring problem in which every city on a map must be visited. The aim is to find a tour with cost < C (or in the optimization version, to find a tour with the lowest cost possible). An enormous amount of effort has been expended to improve the capabilities of TSP algorithms.","Touring problems, TSP"
AIMA,chapter3_5,"There are a variety of suboptimal search algorithms, which can be characterized by the criteria for what counts as “good enough.” In bounded suboptimal search, we look for a solution that is guaranteed to be within a constant factor of the optimal cost. Weighted A* provides this guarantee. In bounded-cost search, we look for a solution whose cost is less than some constant And in unbounded-cost search, we accept a solution of any cost, as long as we can find it quickly.","Bounded suboptimal search, Bounded-cost search, Unbounded-cost search"
SLP,,"The ordering of the numbers in a vector space indicates different meaningful dimensions on which documents vary. Thus the first dimension for both these vectors corresponds to the number of times the word battle occurs, and we can compare each dimension, noting for example that the vectors for As You Like It and Twelfth Night have similar values (1 and 0, respectively) for the first dimension.",vector space
SLP,,"To measure similarity between two target words, V and W, we need a metric that takes two vectors (of the same dimensionality, either both with words as dimensions, hence of length |V|, or both with documents as dimensions as documents, of length |D|) and gives a measure of their similarity. By far the most common similarity metric is the cosine of the angle between the vectors.",cosine
SLP,,"In sequence labeling, the network’s task is to assign a label chosen from a small fixed set of labels to each element of a sequence, like the part-of-speech tagging and named entity recognition tasks from chapter 8. In an RNN approach to sequence labeling, inputs are word embeddings and the outputs are tag probabilities generated by a softmax layer over the given tagset, as illustrated in fig. 9.7.",embeddings
AIMA,chapter3_4,"To keep depth-first search from wandering down an infinite path, we can use depth-limited search, a version of depth-first search in which we supply a depth limit, ℓ, and treat all nodes at depth ℓ as if they had no successors. Unfortunately, if we make a poor choice for ℓ the algorithm will fail to reach the solution, making it incomplete again.",depth-limited search
AIMA,chapter7_3,"An inference algorithm that derives only entailed sentences is called sound or truth-preserving. Soundness is a highly desirable property. An unsound inference procedure essentially makes things up as it goes along—it announces the discovery of nonexistent needles. It is easy to see that model checking, when it is applicable, is a sound procedure.","Sound, Truth-preserving"
AIMA,chapter2_3,"Now that we have a definition of rationality, we are almost ready to think about building rational agents. First, however, we must think about task environments, which are essentially the “problems” to which rational agents are the “solutions.” We begin by showing how to specify a task environment, illustrating the process with a number of examples. We then show that task environments come in a variety of flavors. The nature of the task environment directly affects the appropriate design for the agent program. In our discussion of the rationality of the simple vacuum-cleaner agent, we had to specify the performance measure, the environment, and the agent’s actuators and sensors. We group all these under the heading of the task environment. For the acronymically minded, we call this the PEAS (Performance, Environment, Actuators, Sensors) description. In designing an agent, the first step must always be to specify the task environment as fully as possible.","Task environment, PEAS"
AIMA,chapter6_3,"The MRV heuristic doesn’t help at all in choosing the first region to color in Australia, because initially every region has three legal colors. In this case, the degree heuristic comes in handy. It attempts to reduce the branching factor on future choices by selecting the variable that is involved in the largest number of constraints on other unassigned variables.The minimumremaining-values heuristic is usually a more powerful guide, but the degree heuristic can be useful as a tie-breaker. Once a variable has been selected, the algorithm must decide on the order in which to  examine its values. The least-constraining-value heuristic is effective for this. It prefers the  value that rules out the fewest choices for the neighboring variables in the constraint graph.","Degree heuristic, Least-constraining-value"
AIMA,chapter7_5,"The completeness of resolution makes it a very important inference method. In many practical situations, however, the full power of resolution is not needed. Some real-world knowledge bases satisfy certain restrictions on the form of sentences they contain, which enables them to use a more restricted and efficient inference algorithm. One such restricted form is the definite clause, which is a disjunction of literals of which exactly one is positive.",Definite clause
SLP,,"The true measure of a hypothesis is not how it does on the training set, but rather how it handles inputs it has not yet seen. We can evaluate that with a second sample of (x;, y;) pairs called a test set. We say that h generalizes well if it accurately predicts the outputs o:",test set
AIMA,chapter2_3,"For example, in chess, the opponent entity is trying to maximize its performance measure, which, by the rules of chess, minimizes agent ’s performance measure. Thus, chess is a competitive multiagent environment. On the other hand, in the taxi-driving environment, avoiding collisions maximizes the performance measure of all agents, so it is a partially cooperative multiagent environment. It is also partially competitive because, for example, only one car can occupy a parking space.","Competitive, Cooperative"
AIMA,chapter2_3,"The agent-design problems in multiagent environments are often quite different from those in single-agent environments; for example, communication often emerges as a rational behavior in multiagent environments; in some competitive environments, randomized behavior is rational because it avoids the pitfalls of predictability. Deterministic vs. nondeterministic. If the next state of the environment is completely determined by the current state and the action executed by the agent(s), then we say the environment is deterministic; otherwise, it is nondeterministic. In principle, an agent need not worry about uncertainty in a fully observable, deterministic environment. If the environment is partially observable, however, then it could appear to be nondeterministic.","Deterministic, Nondeterministic"
SLP,slp-7,"Of the threshold into a bias, a notation we still use (Widrow and Hoff, 1960). The field of neural networks declined after it was shown that a single perceptron unit was unable to model functions as simple as XOR (Minsky and Papert, 1969). While some small amount of work continued during the next two decades, a major revival for the field didn’t come until the 1980s, when practical tools for building deeper networks like error backpropagation became widespread (Rumelhart et al., 1986). During the 1980s a wide variety of neural network and related architectures were developed, particularly for applications in psychology and cognitive science (Rumelhart and McClelland 1986b, McClelland and Elman 1986, Rumelhart and McClelland 1986a, Elman 1990), for which the term connectionist or paral-",connectionist
AIMA,chapter3_4,"A variant of depth-first search called backtracking search uses even less memory. In backtracking, only one successor is generated at a time rather than all successors; each partially expanded node remembers which successor to generate next. In addition, successors are generated by modifying the current state  description directly rather than allocating memory for a brand-new state. This reduces the  memory requirements to just one state description and a path of O(m) actions; a significant  savings over O(bm) states for depth-first search. With backtracking we also have the option  of maintaining an efficient set data structure for the states on the current path, allowing us  to check for a cyclic path in O(1) time rather than O(m). For backtracking to work, we must  be able to undo each action when we backtrack. Backtracking is critical to the success of  many problems with large state descriptions, such as robotic assembly.",Backtracking search
AIMA,chapter1_1,"Turing viewed the physical simulation of a person as unnecessary to demonstrate intelligence. However, other researchers have proposed a total Turing test, which requires interaction with objects and people in the real world. To pass the total Turing test, a robot will need: computer vision and speech recognition to perceive the world; robotics to manipulate objects and move about.","Computer vision, Robotics"
SLP,,"This raw dot product, however, has a problem as a similarity metric: it favors long vectors. The vector length is defined as",vector length
AIMA,chapter3_3,"To be complete, a search algorithm must be systematic in the way it explores an infinite state space, making sure it can eventually reach any state that is connected to the initial state.",Systematic
SLP,slp-7,"Copyrights of a platooned machine car. Like have day logistic regression is a probabilistic classifier that makes use of supervised machine learning. Machine learning classifiers require a training corpus of m input/output pairs (x ‘ yl), (we’ il use superscripts in parentheses to refer to individual instances in the training set—for sentiment classification each instance might be an individual document to be classified.) A machine learning system for classification then has four components:",logistic regression
AIMA,chapter1_1,"We have claimed that AI is interesting, but we have not said what it is. Historically, researchers have pursued several different versions of AI. Some have defined intelligence in terms of fidelity to human performance, while others prefer an abstract, formal definition of intelligence called rationality—loosely speaking, doing the “right thing.” The subject matter itself also varies: some consider intelligence to be a property of internal thought processes and reasoning, while others focus on intelligent behavior, an external characterization.1",Rationality
AIMA,chapter6_2,"Arc consistency tightens down the domains (unary constraints) using the arcs (binary constraints). To make progress on problems like map coloring, we need a stronger notion of consistency. Path consistency tightens the binary constraints by using implicit constraints that are inferred by looking at triples of variables.",Path consistency
SLP,,"Word similarity: While words don't have many synonyms, most words do have lots of similar words. Cat is not a synonym of dog, but cats and dogs are certainly similar words. In moving from synonymy to similarity, it will be useful to shift from talking about relations between word senses (like synonymy) to relations between words (like similarity). Dealing with words avoids having to commit to a particular representation of word senses, which will turn out to simplify our task.",similarity
AIMA,chapter3_4,"When all actions have the same cost, an appropriate strategy is breadth-first search, in which the root node is expanded first, then all the successors of the root node are expanded next, then their successors, and so on. This is a systematic search strategy that is therefore complete even on infinite state spaces.",Breadth-first search
AIMA,chapter6_3,"A more intelligent approach is to backtrack to a variable that might fix the problem—a variable that was responsible for making one of the possible values of SA impossible. To do this, we will keep track of a set of assignments that are in conflict with some value for SA. The set (in this case {Q = red, NSW = green, V = blue}), is called the conflict set for SA. The backjumping method backtracks to the most recent assignment in the conflict set; in this case, backjumping would jump over Tasmania and try a new value for V . This method is easily implemented by a modification to BACKTRACK such that it accumulates the conflict set while checking for a legal value to assign. If no legal value is found, the algorithm should return the most recent element of the conflict set along with the failure indicator.","Conflict set, Backjumping"
SLP,,"The term-document matrix of Fig. 6.2 was first defined as part of the vector space model of information retrieval (Salton, 1971). In this model, a document is represented as a count vector, a column in Fig. 6.3. Vg to review some basic linear algebra, a vector is, at heart, just a list or array of 6 el.",vector space model
AIMA,chapter3_5,"Recursive best-first search (RBFS) attempts to mimic the operation of standard best-first search, but using only linear space. RBFS resembles a recursive depth-first search, but rather than continuing indefinitely down the current path, it uses the f_limit variable to keep track of the f-value of the best alternative path available from any ancestor of the current node. If the current node exceeds this limit, the recursion unwinds back to the alternative path. As the recursion unwinds, RBFS replaces the f-value of each node along the path with a backed-up value—the best f-value of its children. In this way, RBFS remembers the f-value of the best leaf in the forgotten subtree and can therefore decide whether it’s worth reexpanding the subtree at some later time.","Recursive best-first search, backed-up value"
SLP,,"We can evaluate the performance of a learning algorithm with a learning curve, as shown in figure 19.7. For this figure we have 100 examples at our disposal, which we split randomly into a training set and a test set. We learn a hypothesis h with the training set and measure its accuracy with the test set. We can do this starting with a training set of size 1 and increasing one at a time up to size 99. For each size, we actually repeat the process of randomly splitting into training and test sets 20 times, and average the results of the 20 trials. The curve shows that as the training set size grows, the accuracy increases. (For this reason, learning curves are also called happy graphs.) In this graph we reach 95% accuracy, and it looks as if the curve might continue to increase if we had more data.","learning curve, happy graphs"
AIMA,chapter3_6,"A problem with fewer restrictions on the actions is called a relaxed problem. The state-space graph of the relaxed problem is a supergraph of the original state space because the removal of restrictions creates added edges in the graph. Because the relaxed problem adds edges to the state-space graph, any optimal solution in  the original problem is, by definition, also a solution in the relaxed problem; but the relaxed  problem may have better solutions if the added edges provide shortcuts. Hence, the cost of an  optimal solution to a relaxed problem is an admissible heuristic for the original problem.  Furthermore, because the derived heuristic is an exact cost for the relaxed problem, it must  obey the triangle inequality and is therefore consistent",Relaxed problem
SLP,,"Thus, for example, we could compute ppmi(information, data), assuming we preended that fig. 6.6 encompassed all the relevant word contexts/dimensions, as fol-",ppmi
AIMA,chapter5_2,"Given a game tree, the optimal strategy can be determined by working out the minimax value of each state in the tree, which we write as MINIMAX(s). The minimax value is the utility (for MAX) of being in that state, assuming that both players play optimally from there to the end of the game. The minimax value of a terminal state is just its utility. In a non-terminal state, MAX prefers to move to a state of maximum value when it is MAX’s turn to move, and MIN prefers a state of minimum value (that is, minimum value for MAX and thus maximum value for MIN).",Minimax value
SLP,slp-7,"A generative model like Naive Bayes makes use of this likelihood term, which expresses how to generate the features of a document if we knew it was of class c. By contrast, a discriminative model in this text categorization scenario attempts to directly compute p(c|d). Perhaps it will learn to assign a high weight to document features that directly improve its ability to discriminate between possible classes, even if it couldn’t generate an example of one of the classes.",discriminative model
SLP,,One common kind of relatedness between words is if they belong to the same semantic field. A semantic field is a set of words which cover a particular semantic.,semantic field
AIMA,chapter16_3,"Notice that for small changes in wealth relative to the current wealth, almost any curve will be approximately linear. An agent that has a linear curve is said to be risk-neutral. For gambles with small sums, therefore, we expect risk neutrality. In a sense, this justifies the simplified procedure that proposed small gambles to assess probabilities and to justify the axioms of probability in Section 12.2.3.",Risk-neutral
AIMA,chapter7_4,"∧ (and). A sentence whose main connective is ∧, such as W1, 3 ∧ P3, 1, is called a conjunction; its parts are the conjuncts. (The ∧ looks like an “A” for “And.”)",Conjunction
SLP,slp-7,"The multinomial logistic classifier uses a generalization of the sigmoid, called the softmax function, to compute p(y; = 1|x). The softmax function takes a vector z= (z), 2, ..., zx]| of k arbitrary values and maps them to a probability distribution, with each value in the range (0, 1), and all the values summing to 1. Like the sigmoid, it is an exponential function.",softmax
AIMA,,"practical ignorance: even if we know all the rules, we might be uncertain about a particular patient because not all the necessary tests have been or can be run.",practical ignorance
SLP,,"Connotation: Finally, words have affective meanings or connotations. The word connotation has different meanings in different fields, but here we use it to mean the aspects of a word’s meaning that are related to a writer or reader’s emotions, sentiment, opinions, or evaluations. For example, some words have positive connotations (happy) while others have negative connotations (sad). Even words whose meanings are similar in other ways can vary in connotation; consider the difference in connotations between fake, knockoff, forgery, on the one hand, and copy, replica, reproduction on the other, or innocent (positive connotation) and naive (negative connotation). Some words describe positive evaluation (great, love) and others negative evaluation (terrible, hate). Positive or negative evaluation language is called sentiment, as we saw in chapter 4, and word sentiment plays a role in important tasks like sentiment analysis, stance detection, and applications of NLP to the language of politics and consumer reviews.","connotations, sentiment, connotation"
AIMA,chapter22_1,"model-based reinforcement learning: in these approaches the agent uses atransition model of the environment to help interpret the reward signals and to makedecisions about how to act. the model may be initially unknown, in which case theagent learns the model from observing the effects of its actions, or it may already beknown—for example, a chess program may know the rules of chess even if it does notknow how to choose good moves. in partially observable environments, the transitionmodel is also useful for state estimation (see chapter 14'2). model-based reinforcementlearning systems often learn a utility function u(s), defined (as in chapter 17) in termsof the sum of rewards from state s onward.2",model-based reinforcement learning
AIMA,chapter1,"We call ourselves Homo sapiens—man the wise—because our intelligence is so important to us. For thousands of years, we have tried to understand how we think and act—that is, how our brain, a mere handful of matter, can perceive, understand, predict, and manipulate a world far larger and more complicated than itself. The field of artificial intelligence, or AI, is concerned with not just understanding but also building intelligent entities—machines that can compute how to act effectively and safely in a wide variety of novel situations.","Intelligence, Artificial Intelligence"
AIMA,chapter17_2,"In the previous section, we observed that it is possible to get an optimal policy even when the utility function estimate is inaccurate. If one action is clearly better than all others, then the exact magnitude of the utilities on the states involved need not be precise. This insight suggests an alternative way to find optimal policies. The policy iteration algorithm alternates the following two steps, beginning from some initial policy π0: POLICY EVALUATION: given a policy πi, calculate Ui = Uπi , the utility of each state if πi were to be executed. POLICY IMPROVEMENT: Calculate a new MEU policy πi+1, using one-step look-ahead based on Ui","Policy iteration, Policy evaluation, Policy improvement"
AIMA,chapter3_3,Now we must choose which of these three child nodes to consider next. This is the essence of search—following up one option now and putting the others aside for later. Suppose we choose to expand Sibiu first. Figure shows the result: a set of 6 unexpanded nodes (outlined in bold). We call this the frontier of the search tree. We say that any state that has had a node generated for it has been reached (whether or not that node has been expanded),"Frontier, Reached"
SLP,,"Transformers map sequences of input vectors $(x_1, ..., x_n)$ to sequences of output vectors $(y_1, ..., y_n)$ of the same length. Transformers are made up of stacks of transformer blocks, which are multilayer networks made by combining simple linear layers, feedforward networks, and self-attention layers, the key innovation of transformers. Self-attention allows a network to directly extract and use information from arbitrarily large contexts without the need to pass it through intermediate recurrent connections as in RNNs. We’ll start by describing how self-attention works and then return to how it fits into larger transformer blocks.","self-attention, transformers"
SLP,,"The TF-IDF model of meaning is often used for document functions like deciding if two documents are similar. We represent a document by taking the vectors of all the words in the document, and computing the centroid of all those vectors. The centroid is the multidimensional version of the mean; the centroid of a set of vectors is a single vector that has the minimum sum of squared distances to each of the vectors in the set. Given k word vectors w1, w2, ..., wk, the centroid document vector d.","centroid, document vector"
AIMA,chapter22_3,"the second approach, derived from robust control theory, allows for a set of possiblemodels h without assigning probabilities to them, and defines an optimal robust policy «one that gives the best outcome in the worst case over h:",robust control theory
AIMA,chapter1_3,"Logicians in the 19th century developed a precise notation for statements about objects in the world and the relations among them. (Contrast this with ordinary arithmetic notation, which provides only for statements about numbers.) By 1965, programs could, in principle, solve any solvable problem described in logical notation. The so-called logicist tradition within artificial intelligence hopes to build on such programs to create intelligent systems.",Logicist
SLP,,"A better way to deal with continuous values is a split point test—an inequality test on the value of an attribute. For example, at a given node in the tree, it might be the case that testing on weight > 160 gives the most information. Efficient methods exist for finding good split points: start by sorting the values of the attribute, and then consider only split points that are between two examples in sorted order that have different classifications, while keeping track of the running totals of positive and negative examples on each side of the split point. Splitting is the most expensive part of real world decision tree learning applications.",split point
SLP,,"(Gonen and Goldberg, 2019), and this remains an open problem. Historical embeddings are also being used to measure biases in the past. Garg et al. (2018) used embeddings from historical texts to measure the association between embeddings for occupations and embeddings for names of various ethnicities or genders (for example the relative cosine similarity of women’s names versus men’s to occupation words like ‘librarian’ or ‘carpenter’) across the 20th century. They found that the cosines correlate with the empirical historical percentages of women or ethnic groups in those occupations. Historical embeddings also replicated old surveys of ethnic stereotypes; the tendency of experimental participants in 1933 to associate adjectives like ‘industrious’ or ‘superstitious’ with, e.g., Chinese ethnicity, correlates with the cosine between Chinese last names and those adjectives using embeddings trained on 1930s text. They also were able to document historical gender biases, such as the fact that embeddings for adjectives related to competence (‘smart’, ‘wise’, ‘thoughtful’, ‘resourceful’) had a higher cosine with male than female words, and showed that this bias has been slowly decreasing since 1960. We return in later chapters to this question about the role of bias in natural language processing.",garg
AIMA,chapter2_2,"Slightly more intelligent is the sphex wasp. The female sphex will dig a burrow, go out and sting a caterpillar and drag it to the burrow, enter the burrow again to check all is well, drag the caterpillar inside, and lay its eggs. The caterpillar serves as a food source when the eggs hatch. So far so good, but if an entomologist moves the caterpillar a few inches away while the sphex is doing the check, it will revert to the “drag the caterpillar” step of its plan and will continue the plan without modification, re-checking the burrow, even after dozens of caterpillar-moving interventions. The sphex is unable to learn that its innate plan is failing, and thus will not change it. To the extent that an agent relies on the prior knowledge of its designer rather than on its own percepts and learning processes, we say that the agent lacks autonomy. A rational agent should be autonomous—it should learn what it can to compensate for partial or incorrect prior knowledge. For example, a vacuum-cleaning agent that learns to predict where and when additional dirt will appear will do better than one that does not.",Autonomy
AIMA,chapter6_2,"Another important higher-order constraint is the resource constraint, sometimes called the Atmost constraint. For example, in a scheduling problem, let P1, …, P4 denote the numbers of personnel assigned to each of four tasks. The constraint that no more than 10 personnel are assigned in total is written as Atmost(10, P1, P2, P3, P4). We can detect an inconsistency simply by checking the sum of the minimum values of the current domains; for example, if each variable has the domain {3, 4, 5, 6}, the Atmost constraint cannot be satisfied. We can also enforce consistency by deleting the maximum value of any domain if it is not consistent with the minimum values of the other domains. Thus, if each variable in our example has the domain {2, 3, 4, 5, 6}, the values 5 and 6 can be deleted from each domain.",Resource constraint
AIMA,chapter22_2,"one common method, originally used in animal training, is called reward shaping. thisinvolves supplying the agent with additional rewards, called pseudorewards, for “makingprogress.” for example, we might give pseudorewards to the robot for making contact withthe ball or for advancing it toward the goal. such rewards can speed up learning enormoushand are simple to provide, but there is a risk that the agent will learn to maximize thepseudorewards rather than the true rewards; for example, standing next to the ball and“vibrating” causes many contacts with the ball.",reward shaping
SLP,slp_4,"Example: Due to biases in the human labelers), by the resources used (like lexicons. Or model components like pretrained embeddings), or even by model architecture (like what the model is trained to optimize). While the mitigation of these biases (for example by carefully considering the training data sources) is an important area of research, we currently don’t have general solutions. For this reason it’s important. When introducing any NLP model, to study these these kinds of factors and make them clear. One way to do this is by releasing a model card (Mitchell et al., 2019) for each version of a model. A model card documents a machine learning model",model card
AIMA,chapter12_1,"variables in probability theory are called random variables, and their names begin with anuppercase letter. thus, in the dice example, total and die; are random variables. everyrandom variable is a function that maps from the domain of possible worlds 2 to somerange—the set of possible values it can take on. the range of total for two dice is the set{2, ..., 12} and the range of die; is {1, ..., 6}. names for values are always lowercase, so wmight write >, p(x = x) to sum over the values of x. a boolean random variable has therange {true, false}. for example, the proposition that doubles are rolled can be written asdoubles = true. (an alternative range for boolean variables is the set {0, 1}, in which casethe variable is said to have a bernoulli distribution.) by convention, propositions of theform a = true are abbreviated simply as a, while a = false is abbreviated as —a. (the usesof doubles, cavity, and toothache in the preceding section are abbreviations of this kind.)","bernoulli, random variable, range"
AIMA,chapter16_3,"Decision theory is a normative theory: it describes how a rational agent should act. A descriptive theory, on the other hand, describes how actual agents—for example, humans— really do act. The application of economic theory would be greatly enhanced if the two coincided, but there appears to be some experimental evidence to the contrary. The evidence suggests that humans are “predictably irrational”","Normative theory, Descriptive theory"
SLP,,"Part of model selection is qualitative and subjective: we might select polynomials rather than decision trees based on something that we know about the problem. And part is quantitative and empirical: within the class of polynomials, we might select degree = 2 because that value performs best on the validation data set.",model selection
AIMA,chapter7_5,"Before we plunge into the details of theorem-proving algorithms, we will need some additional concepts related to entailment. The first concept is logical equivalence: two sentences and are logically equivalent if they are true in the same set of models. We write this as (Note that is used to make claims about sentences, while is used as part of a sentence.) For example, we can easily show (using truth tables) that and are logically equivalent; other equivalences are shown in Figure 7.11 . These equivalences play much the same role in logic as arithmetic identities do in ordinary mathematics. An alternative definition of equivalence is as follows: any two sentences and are equivalent if and only if each of them entails the other",Logical equivalence
AIMA,chapter6_4,"Another technique called constraint weighting aims to concentrate the search on the important constraints. Each constraint is given a numeric weight, initially all 1. At each step of the search, the algorithm chooses a variable/value pair to change that will result in the lowest total weight of all violated constraints. The weights are then adjusted by incrementing the weight of each constraint that is violated by the current assignment. This has two benefits: it adds topography to plateaus, making sure that it is possible to improve from the current state, and it also adds learning: over time the difficult constraints are assigned higher weights.",Constraint weighting
AIMA,chapter1_4,"An agent is just something that acts (agent comes from the Latin agere, to do). Of course, all computer programs do something, but computer agents are expected to do more: operate autonomously, perceive their environment, persist over a prolonged time period, adapt to change, and create and pursue goals. A rational agent is one that acts so as to achieve the best outcome or, when there is uncertainty, the best expected outcome.","Agent, Rational agent"
SLP,slp-7,Designing features: Features are generally designed by examining the training set with an eye to linguistic intuitions and the linguistic literature on the domain. A careful error analysis on the training set or devset of an early version of a system often provides insights into features.,designing features:
SLP,,"A validation set, also known as a development set or dev set, is used to evaluate the candidate models and choose the best one.",Validation set
AIMA,chapter7_1,"Humans, it seems, know things; and what they know helps them do things. In AI, knowledge-based agents use a process of reasoning over an internal representation of knowledge to decide what actions to take.","Knowledge-based agents, Reasoning, Representation"
AIMA,chapter17_3,"This equation defines a kind of “value” for M in terms of its ability to deliver a stream of timely rewards; the numerator of the fraction represents a utility while the denominator can be thought of as a “discounted time, ” so the value describes the maximum obtainable utility per unit of discounted time. (It’s important to remember that T in the equation is a stopping time, which is governed by a rule for stopping rather than being a simple integer; it reduces to a simple integer only when M is a deterministic reward sequence.) The value defined in Equation (17.15) is called the Gittins index of M.",Gittins index
SLP,,"In general, the input and output values can be discrete or continuous, but for now we will consider only inputs consisting of discrete values and outputs that are either true (a positive example) or false (a negative example). We call this boolean classification. We will use j to index the examples (x is the input vector for the jth example and yj is the output), and xj for the ith attribute of the 7th example.","negative, positive"
AIMA,chapter5_2,"The number of game states is exponential in the depth of the tree. No algorithm can completely eliminate the exponent, but we can sometimes cut it in half, computing the correct minimax decision without examining every state by pruning (see page 90) large parts of the tree that make no difference to the outcome. The particular technique we examine is called alpha–beta pruning",Alpha–beta pruning
AIMA,chapter7_3,"The preceding example not only illustrates entailment but also shows how the definition of entailment can be applied to derive conclusions—that is, to carry out logical inference. The inference algorithm illustrated in Figure 7.5 is called model checking, because it enumerates all possible models to check that is true in all models in which KB is true","Logical inference, Model checking"
SLP,slp_4,"In cross-validation, we choose a number k, and partition our data into k disjoint subsets called folds. Now we choose one of those k folds as a test set, train our classifier on the remaining k-1 folds, and then compute the error rate on the test set. Then we repeat with another fold as the test set, again training on the other k-1 folds. We do this sampling process k times and average the test set error rate from these k runs to get an average error rate. If we choose k = 10, we would train 10 different models (each on 90% of our data), test the model 10 times, and average these 10 values. This is called 10-fold cross-validation.",folds
AIMA,chapter17_3,"A second method, Thompson sampling (Thompson, 1933), chooses chooses an arm randomly according to the probability that the arm is in fact optimal, given the samples so far. Suppose that Pi(μi) is the current probability distribution for the true value of arm Mi. Then a simple way to implement Thompson sampling is to generate one sample from each Pi and then pick the best sample. This algorithm also has a regret that grows as O(logN).",Thompson sampling
SLP,,"When the output is one of a finite set of values (such as sunny/cloudy/rainy or true/false), the learning problem is called classification. When it is a number (such as tomorrow’s temperature, measured either as an integer or a real number), the learning problem has an admittedly obscure name: regression.",classification
AIMA,chapter12_1,laziness: it is too much work to list the complete set of antecedents or consequentsneeded to ensure an exceptionless rule and too hard to use such rules.,laziness
SLP,slp-7,"In this section we give the derivation of the gradient of the cross-entropy loss function lcg for logistic regression. Let’s start with some quick calculus refreshers. First, the derivative of in(x):",logistic regression
SLP,,"Transformers can also be used for sequence labeling tasks (like part-of-speech tagging or named entity tagging) and sequence classification tasks (like sentiment classification), as we’ll see in detail in chapter 11. Just to give a preview, however, we don’t directly train a raw transformer on these tasks. Instead, we use a technique called pretraining, in which we first train a transformer language model on a large corpus of text, in a normal self-supervised way, and only afterwards add a linear feedforward layer on top that we finetune on a smaller dataset hand-labeled with part-of-speech or sentiment labels. Pretraining on large amounts of data via the","finetune, pretraining"
SLP,,"Another option, instead of using just the last token h, to represent the whole sequence, is to use some sort of pooling function of all the hidden states h; for each word i in the sequence. For example, we can create a representation that pools all the n hidden states by taking their element-wise mean:",pooling
AIMA,chapter22_3,"(according to my estimates)?” sarsa is an on-policy algorithm: it learns q-values thatanswer the question “what would this action be worth in this state, assuming i stick with mpolicy?” q-learning is more flexible than sarsa, in the sense that a q-learning agent canlearn how to behave well when under the control of a wide variety of exploration policies.on the other hand, sarsa is appropriate if the overall policy is even partly controlled byother agents or programs, in which case it is better to learn a q-function for what willactually happen rather than what would happen if the agent got to pick estimated bestactions. both q-learning and sarsa learn the optimal policy for the 4 x 3 world, but theydo so at a much slower rate than the adp agent. this is because the local updates do notenforce consistency among all the o-values via the model.",on-policy
SLP,slp_4,"We do this by creating a random variable x ranging over all test sets. Now we ask how likely is it, if the null hypothesis ho was correct, that among these test sets we would encounter the value of 5(x) that we found. We formalize this likelihood as the p-value: the probability, assuming the null hypothesis hp is true, of seeing the 5(x) that we saw or one even greater.",null hypothesis
AIMA,chapter2_3,"DISCRETE VS. CONTINUOUS: The discrete/continuous distinction applies to the state of the environment, to the way time is handled, and to the percepts and actions of the agent. For example, the chess environment has a finite number of distinct states (excluding the clock). Chess also has a discrete set of percepts and actions. Taxi driving is a continuous-state and continuous-time problem: the speed and location of the taxi and of the other vehicles sweep through a range of continuous values and do so smoothly over time. Taxi-driving actions are also continuous (steering angles, etc.). Input from digital cameras is discrete, strictly speaking, but is typically treated as representing continuously varying intensities and locations.","Discrete, Continuous"
AIMA,chapter2_3,"SINGLE-AGENT VS. MULTIAGENT: The distinction between single-agent and multiagent environments may seem simple enough. For example, an agent solving a crossword puzzle by itself is clearly in a single-agent environment, whereas an agent playing chess is in a twoagent environment. However, there are some subtle issues. First, we have described how an entity may be viewed as an agent, but we have not explained which entities must be viewed as agents. Does an agent (the taxi driver for example) have to treat an object (another vehicle) as an agent, or can it be treated merely as an object behaving according to the laws of physics, analogous to waves at the beach or leaves blowing in the wind? The key distinction is whether ’s behavior is best described as maximizing a performance measure whose value depends on agent ’s behavior.","Single-agent, Multiagent"
AIMA,chapter17_2,"The algorithms we have described so far require updating the utility or policy for all states at once. It turns out that this is not strictly necessary. In fact, on each iteration, we can pick any subset of states and apply either kind of updating (policy improvement or simplified value iteration) to that subset. This very general algorithm is called asynchronous policy iteration. Given certain conditions on the initial policy and initial utility function, asynchronous policy iteration is guaranteed to converge to an optimal policy. The freedom to choose any states to work on means that we can design much more efficient heuristic algorithms—for example, algorithms that concentrate on updating the values of states that are likely to be reached by a good policy. There’s no sense planning for the results of an action you will never do.",Asynchronous policy iteration
SLP,,"In a confusion of terminology, Fano used the phrase ""mutual information"" to refer to what we now call ""pointwise mutual information"" and the phrase ""expectation of the mutual information"" for what we now call ""pase ei a pe.""",pointwise mutual information
SLP,slp-7,"How do we make a decision about which class to apply to a test instance x? For a given x, we say yes if the probability p(y = 1|x) is more than .5, and no otherwise. We call .5 the decision boundary.",decision boundary
AIMA,chapter3_3,"A priority queue first pops the node with the minimum cost according to some evaluation function, f. It is used in best-first search.",priority queue
SLP,slp_4,"A very small p-value means that the difference we observed is very unlikely under the null hypothesis, and we can reject the null hypothesis. What counts as very small? It is common to use values like .05 or .01 as the thresholds. A value of .01 means that if the p-value (the probability of observing the 6 we saw assuming hp is true) is less than .01, we reject the null hypothesis and assume that a is indeed better than b. We say that a result (e.g., “a is better than b”) is statistically significant if the 5 we saw has a probability that is below the threshold and we therefore reject this null hypothesis.",statistically significant
AIMA,chapter17_1,"where the discount factor γ is a number between 0 and 1. The discount factor describes the preference of an agent for current rewards over future rewards. When γ is close to 0, rewards in the distant future are viewed as insignificant. When γ is close to 1, an agent is more willing to wait for long-term rewards. When γ is exactly 1, discounted rewards reduce to the special case of purely additive rewards. Notice that additivity was used implicitly in our use of path cost functions in heuristic search algorithms (Chapter 3 ).","Discount factor, Additive reward"
AIMA,chapter6_3,"The algorithm called MAC (for Maintaining Arc Consistency) detects inconsistencies like this. After a variable Xi is assigned a value, the INFERENCE procedure calls AC-3, but instead of a queue of all arcs in the CSP, we start with only the arcs (Xj, Xi) for all Xj that are unassigned variables that are neighbors of Xi. From there, AC-3 does constraint propagation in the usual way, and if any variable has its domain reduced to the empty set, the call to AC-3 fails and we know to backtrack immediately. We can see that MAC is strictly more powerful than forward checking because forward checking does the same thing as MAC on the initial arcs in MAC’s queue; but unlike MAC, forward checking does not recursively propagate constraints when changes are made to the domains of variables.",Maintaining Arc Consistency
SLP,,"Let’s now walk through a slightly more formal presentation of the simplest kind of neural network, the feedforward network. A feedforward network is a multilayer network in which the units are connected with no cycles; the outputs from units in each layer are passed to units in the next higher layer, and no outputs are passed back to lower layers. (In chapter 9 we’ll introduce networks with cycles, called recurrent neural networks.)",feedforward network
SLP,slp_4,"In this section we introduce the multinomial Naive Bayes classifier, so called because it is a Bayesian classifier that makes a simplifying (naive) assumption about.",naive bayes classifier
SLP,slp-7,"The new regularization term, r(@), is used to penalize large weights. Thus a setting of the weights that matches the training data perfectly—but uses many weights with high values to do so—will be penalized more than a setting that matches the data little less well, but does so using smaller weights. There are two common ways to compute this regularization term r(@). L2 regularization is a quadratic function of the weight values, named because it uses the (square of the) L2 norm of the weight values. The L2 norm, ||6||2, is the same as the Euclidean distance of the vector 6 from the origin. If @ consists of n weights, then:",regularization
AIMA,chapter1_1,"The Turing test, proposed by Alan Turing (1950), was designed as a thought experiment that would sidestep the philosophical vagueness of the question “Can a machine think?” A computer passes the test if a human interrogator, after posing some written questions, cannot tell whether the written responses come from a person or from a computer. Chapter 27 discusses the details of the test and whether a computer would really be intelligent if it passed. For now, we note that programming a computer to pass a rigorously applied test provides plenty to work on. The computer would need the following capabilities: natural language processing to communicate successfully in a human language; knowledge representation to store what it knows or hears; automated reasoning to answer questions and to draw new conclusions; machine learning to adapt to new circumstances and to detect and extrapolate patterns.","Turing test, Natural language processing, Knowledge representation, Automated reasoning, Machine learning, Total Turing test"
AIMA,chapter7_3,"Now that we have a notion of truth, we are ready to talk about logical reasoning. This involves the relation of logical entailment between sentences—the idea that a sentence follows logically from another sentence.",Entailment
AIMA,chapter2_4,So far we have talked about agents by describing behavior—the action that is performed after any given sequence of percepts. Now we must bite the bullet and talk about how the insides work. The job of AI is to design an agent program that implements the agent function—the mapping from percepts to actions. We assume this program will run on some sort of computing device with physical sensors and actuators—we call this the agent architecture: agent = architecture + program,"Agent program, Agent architecture"
SLP,,One way to analyze hypothesis spaces is by the bias they impose (regardless of the training data) and the variance they exhibit (from one training set to another).,bias
SLP,,"Tuning of hyperparameters is also important. The parameters of a neural network are the weights w and biases b; those are learned by gradient descent. The hyperparameters are things that are chosen by the algorithm designer; optimal values are tuned on a devset rather than by gradient descent learning on the training set. Hyperparameters include the learning rate, the mini-batch size, the model architecture (the number of layers, the number of hidden nodes per layer, the choice of activation functions), how to regularize, and so on. Gradient descent itself also has many architectural anal ti.",hyperparameter
AIMA,chapter22_1,"here, f is the exploration function. the function f(u, n) determines how greed (preferenc:for high values of the utility u) is traded off against curiosity (preference for actions thathave not been tried often and have a low count n). the function should be increasing in uand decreasing in n. obviously, there are many possible functions that fit these conditions.one particularly simple definition is",exploration function
AIMA,chapter6_2,"An atomic state-space search algorithm makes progress in only one way: by expanding a node to visit the successors. A CSP algorithm has choices. It can generate successors by choosing a new variable assignment, or it can do a specific type of inference called constraint propagation: using the constraints to reduce the number of legal values for a variable, which in turn can reduce the legal values for another variable, and so on. The idea is that this will leave fewer choices to consider when we make the next choice of a variable assignment. Constraint propagation may be intertwined with search, or it may be done as a preprocessing step, before search starts. Sometimes this preprocessing can solve the whole problem, so no search is required at all.",Constraint propagation
SLP,,"Here the form ""mouse"" is the lemma, also called the citation form. The form ""mouse"" would also be the lemma for the word ""mice""; dictionaries don’t have separate definitions for inflected forms like ""mice."" Similarly ""sing"" is the lemma for ""sing,"" ""sang,"" ""sung."" In many languages the infinitive form is used as the lemma for the verb, so Spanish ""dormir"" “to sleep” is the lemma for ""duermes"" “you sleep.” The specific forms ""sung"" or ""carpets"" or ""sing"" or ""duermes"" are called wordforms.","wordform, citation form, lemma"
AIMA,chapter2_3,"KNOWN VS. UNKNOWN: Strictly speaking, this distinction refers not to the environment itself but to the agent’s (or designer’s) state of knowledge about the “laws of physics” of the environment. In a known environment, the outcomes (or outcome probabilities if the environment is nondeterministic) for all actions are given. Obviously, if the environment is unknown, the agent will have to learn how it works in order to make good decisions.","Known, Unknown"
AIMA,chapter3_4,"The algorithms we have covered so far start at an initial state and can reach any one of multiple possible goal states. An alternative approach called bidirectional search simultaneously searches forward from the initial state and backwards from the goal state(s), hoping that the two searches will meet.",Bidirectional search
SLP,slp-7,"For some tasks it is especially helpful to build complex features that are combinations of more primitive features. We saw such a feature for period disambiguation above, where a period on the word ""st."" was less likely to be the end of the sentence if the previous word was capitalized. For logistic regression and naive bayes these combination features or feature interactions have to be designed by hand.","logistic regression, feature interactions"
AIMA,chapter1_4,"We need to make one important refinement to the standard model to account for the fact that perfect rationality—always taking the exactly optimal action—is not feasible in complex environments. The computational demands are just too high. Chapters 5 and 17 deal with the issue of limited rationality—acting appropriately when there is not enough time to do all the computations one might like. However, perfect rationality often remains a good starting point for theoretical analysis.",Limited rationality
AIMA,chapter3_4,"Iterative deepening search solves the problem of picking a good value for ℓ by trying all values: first 0, then 1, then 2, and so on—until either a solution is found, or the depth-limited search returns the failure value rather than the cutoff value. Iterative deepening combines many of the benefits of depth-first and breadth-first search. Like depth-first search, its memory requirements are modest: O(bd) when there is a solution, or O(bm) on finite state spaces with no solution. Like breadth-first search, iterative deepening is optimal for problems where all actions have the same cost, and is complete on finite acyclic state spaces, or on any finite state space when we check nodes for cycles all the way up the path.",Iterative deepening search
AIMA,chapter3_5,"Iterative-deepening A* search (IDA*) is to A* what iterative-deepening search is to depth-first: IDA* gives us the benefits of A* without the requirement to keep all reached states in memory, at a cost of visiting some states multiple times. It is a very important and commonly used algorithm for problems that do not fit in memory.",Iterative-deepening A* search
SLP,,"In our examples thus far, the inputs to our RNNs have consisted of sequences of word or character embeddings (vectors) and the outputs have been vectors useful for: predicting words, tags or sequence labels. However, nothing prevents us from using; the entire sequence of outputs from one RNN as an input sequence to another one. Stacked RNNs consist of multiple networks where the output of one layer serves as: the input to a subsequent layer, as shown in Fig. 9.10.",stacked rnns
SLP,,"We’ve seen that documents can be represented as vectors in a vector space. But vector semantics can also be used to represent the meaning of words. We do this by associating each word with a word vector— a row vector rather than a column vector, hence with different dimensions, as shown in Fig. 6.5. The four dimensions of the vector for fool, [36, 58, 1, 4], correspond to the four Shakespeare plays. Word counts in the same four dimensions are used to form the vectors for the other words: wit, [20, 15, 2, 3]; battle, [1, 0, 7, 13]; and good [114, 80, 62, 89].",row vector
AIMA,chapter6_2,"We say that a CSP is bounds-consistent if for every variable X, and for both the lower--ound and upper-bound values of X, there exists some value of Y that satisfies the constraint between X and Y for every variable Y . This kind of bounds propagation is widely used in practical constraint problems.",Bounds-consistent
AIMA,chapter5_3,"To make use of our limited computation time, we can cut off the search early and apply a heuristic evaluation function to states, effectively treating nonterminal nodes as if they were terminal. In other words, we replace the UTILITY function with EVAL, which estimates a state’s utility. We also replace the terminal test by a cutoff test, which must return true for terminal states, but is otherwise free to decide when to cut off the search, based on the search depth and any property of the state that it chooses to consider.",cutoff test
SLP,,How do we choose a good hypothesis from within the hypothesis space? We could hope for a consistent hypothesis: an h such that each h(x;) in the training set has h(x;) = y;. With continuous-valued outputs we can't expect an exact match to the ground truth; instead we look for a best-fit function for which each h(x;) is close to y; (in a way that we will formalize in section 19.4.2!b).,"ground truth, consistent hypothesis"
SLP,,"The weights in the network are adjusted to minimize the average ch loss over the training sequence via gradient descent. Fig. 9.6 illustrates this training regimen. Careful readers may have noticed that the input embedding matrix e and the final layer matrix v, which feeds the output softmax, are quite similar. The columns of e represent the word embeddings for each word in the vocabulary learned during the training process with the goal that words that have similar meaning and function will have similar embeddings. And, since the length of these embeddings corresponds to the size of the hidden layer dj, the shape of the embedding matrix e is d, x |v].","words, embeddings"
AIMA,,"nonetheless, in some sense ago is in fact the right thing to do. what do we mean by this? as we discussed in chapter 212, we mean that out of all the plans that could be executed, ago is expected to maximize the agent's performance measure (where the expectation is relative to the agent’s knowledge about the environment). the performance measure includes getting to the airport in time for the flight, avoiding a long, unproductive wait at the airport, and avoiding speeding tickets along the way. the agent’s knowledge cannot guarantee any of these outcomes for ago, but it can provide some degree of belief that they will be achieved. other plans, such as ajo, might increase the agent's belief that it will get to the airport on time, but also increase the likelihood of a long, boring wait. the right thing to do—the rational decision—therefore depends on both the relative importance of various goals and the likelihood that, and degree to which, they will be achieved. the remainder of this section hones these ideas, in preparation for the development of the general theories of uncertain reasoning and rational decisions that we present in this and subsequent chapters.","degree of belief, outcome"
AIMA,chapter7_5,"This section covers inference rules that can be applied to derive a proof—a chain of conclusions that leads to the desired goal. Another useful inference rule is And-Elimination, which says that, from a conjunction, any of the conjuncts can be inferred","Inference rules, Proof, Modus Ponens, And-Elimination "
SLP,,The need for non-linear activation functions is one of the reasons we use nonlinear activation functions for each layer in a neural network. Let’s see why this is true. Imagine the first two layers of such a network of purely linear layers:,the need for non-linear activation functions
AIMA,chapter22_3,"unfortunately, the real world is less forgiving. if you are a baby sunfish, your probabilitysurviving to adulthood is about 0.00000001. many actions are irreversible, in the sensedefined for online search agents in section 4.5'8: no subsequent sequence of actions carrestore the state to what it was before the irreversible action was taken. in the worst casthe agent enters an absorbing state where no actions have any effect and no rewards at",absorbing state
SLP,,"The tf-idf weighting is the way for weighting co-occurrence matrices in information retrieval, but also plays a role in many other aspects of natural language processing. It’s also a great baseline, the simple thing to try first. We'll look at other weightings like ppmi (positive pointwise mutual information) in section 6.6.",tf-idf
AIMA,chapter3_3,"A search algorithm takes a search problem as input and returns a solution, or an indication of failure. In this chapter we consider algorithms that superimpose a search tree over the state-space graph, forming various paths from the initial state, trying to find a path that reaches a goal state. Each node in the search tree corresponds to a state in the state space and the edges in the search tree correspond to actions. The root of the tree corresponds to the initial state of the problem.","search algorithm, node"
SLP,slp-7,"Stochastic gradient descent is called stochastic because it chooses a single random example at a time, moving the weights so as to improve performance on that single example. That can result in very choppy movements, so it’s common to compute the gradient over batches of training instances rather than a single instance.",gradient descent
AIMA,chapter7_5,"Proving β from α by checking the unsatisfiability of (α ∧ ¬β) corresponds exactly to the standard mathematical proof technique of reductio ad absurdum (literally, “reduction to an absurd thing”). It is also called proof by refutation or proof by contradiction. One assumes a sentence β to be false and shows that this leads to a contradiction with known axioms α. This contradiction is exactly what is meant by saying that the sentence (α ∧ ¬β) is unsatisfiable.","Reductio ad absurdum, Refutation, Contradiction"
SLP,,"Synonymy is one important component of word meaning. For example, when one word has a sense whose meaning is similar to that of another word, we say that the two words are synonymous.",synonymy
AIMA,chapter5_1,"There are at least three stances we can take towards multi-agent environments. The first stance, appropriate when there are a very large number of agents, is to consider them in the aggregate as an economy, allowing us to do things like predict that increasing demand will cause prices to rise, without having to predict the action of any individual agent.",Economy
SLP,slp_4,"The intuition of the classifier is shown in Fig. 4.1. We represent a text document as if it were a bag-of-words, that is, an unordered set of words with their position ignored, keeping only their frequency in the document. In the example in the figure instead of representing the word order in all the phrases like “I love this movie” and “I would recommend it”, we simply note that the word “I” occurred 5 times in the entire excerpt, the word “it” 6 times, the words “love,” “recommend,” and “movie” once, and so on.",bag-of-words
AIMA,chapter12_1,"practical ignorance: even if we know all the rules, we might be uncertain abouta particular patient because not all the necessary tests have been or can be run.",practical ignorance
AIMA,chapter17_3,"The task of choosing the best option under these conditions is called a selection problem. Selection problems are ubiquitous in industrial and personnel contexts. One often must decide which supplier to use for a process; or which candidate employees to hire. Selection problems are superficially similar to the bandit problem but have different mathematical properties. In particular, no index function exists for selection problems. The proof of this fact requires showing any scenario where the optimal policy switches its preferences for two arms M1 and M2 when a third arm M3 is added (see Exercise 17.SELC).",Selection problem
SLP,slp_4,"Many language processing tasks involve classification, although luckily our classes are much easier to define than those of Borges. In this chapter we introduce the Naive Bayes algorithm and apply it to text categorization, the task of assigning a label or category to an entire text or document.",categorization
SLP,,"These activation functions have different properties that make them useful for different language applications or network architectures. For example, the tanh function has the nice properties of being smoothly differentiable and mapping outlier values toward the mean. The rectifier function, on the other hand, has nice properties that",tanh
AIMA,chapter5_3,"One strategy to mitigate the horizon effect is to allow singular extensions, moves that are “clearly better” than all other moves in a given position, even when the search would normally be cut off at that point.",Singular extension
SLP,,"The decision tree learning algorithm chooses the attribute with the highest importance. We will now show how to measure importance, using the notion of information gain, which is defined in terms of entropy, which is the fundamental quantity in information theory (Shannon and Weaver 1646).","entropy, information gain"
AIMA,chapter7_1,"A knowledge-based agent can be built simply by TELLing it what it needs to know. Starting with an empty knowledge base, the agent designer can TELL sentences one by one until the agent knows how to operate in its environment. This is called the declarative approach to system building. In contrast, the procedural approach encodes desired behaviors directly as program code. In the 1970s and 1980s, advocates of the two approaches engaged in heated debates. We now understand that a successful agent often combines both declarative and procedural elements in its design, and that declarative knowledge can often be compiled into more efficient procedural code.","Declarative, Procedural"
SLP,slp-7,"In this chapter, we introduce an algorithm that is admirably suited for discovering the link between features or cues and some particular outcome: logistic regression. Indeed, logistic regression is one of the most important analytic tools in the social and natural sciences. In natural language processing, logistic regression is the baseline supervised machine learning algorithm for classification, and also has a very close relationship with neural networks. As we will see in chapter 7, a neural network can be viewed as a series of logistic regression classifiers stacked on top of each other. Thus the classification and machine learning techniques introduced here will play an important role throughout the book.",logistic regression
AIMA,chapter6_1,"A discrete domain can be infinite, such as the set of integers or strings. (If we didn’t put a deadline on the job-scheduling problem, there would be an infinite number of start times for each variable.) With infinite domains, we must use implicit constraints like T1 + d1 ≤ T2 rather than explicit tuples of values. Special solution algorithms (which we do not discuss here) exist for linear constraints on integer variables—that is, constraints, such as the one just given, in which each variable appears only in linear form. It can be shown that no algorithm exists for solving general nonlinear constraints on integer variables—the problem is undecidable.","Infinite, Linear constraints, Nonlinear constraints"
SLP,,"‘Let’s introduce some constants to represent the dimensionalities of these vectors and matrices. We’ll refer to the input layer as layer 0 of the network, and have n0 represent the number of inputs, so x is a vector of real numbers of dimension n0, or more formally x ∈ Rn0 , a column vector of dimensionality [n0 , 1]. Let’s call the hidden layer layer 1 and the output layer layer 2. The hidden layer has dimensionality n1, so h ∈ Rn1 and also b ∈ Rn1 (since each hidden unit can take a different bias value). And the weight matrix w has dimensionality w ∈ Rn1×n0 , i.e. [n1 , n0]. Take a moment to convince yourself that the matrix multiplication in eq. 7.8 will","7.8 will, input layer"
SLP,slp_4,"In the previous section, we pointed out that Naive Bayes doesn't require that our classifier use all the words in the training data as features. In fact, features in Naive Bayes can express any property of the input text we want. Consider the task of spam detection, deciding if a particular piece of email is",spam detection
SLP,,"Layer normalization (or layer norm) is one of many forms of normalization that can be used to improve training performance in deep neural networks by keeping the values of a hidden layer in a range that facilitates gradient-based training. Layer norm is a variation of the standard score, or z-score, from statistics applied to a single hidden layer. The first step in layer normalization is to calculate the mean, μ, and standard deviation, σ, over the elements of the vector to be normalized. Given a hidden layer with dimensionality d;, these values are calculated as follows.",layer norm
AIMA,chapter16_3,"That is, agents with curves of this shape are risk-averse: they prefer a sure thing with a payoff that is less than the expected monetary value of a gamble. On the other hand, in the “desperate” region at large negative wealth in Figure 16.2(b), the behavior is risk-seeking. The value an agent will accept in lieu of a lottery is called the certainty equivalent of the lottery. Studies have shown that most people will accept about $400 in lieu of a gamble that gives $1000 half the time and $0 the other half—that is, the certainty equivalent of the lottery is $400, while the EMV is $500. The difference between the EMV of a lottery and its certainty equivalent is called the insurance premium. Risk aversion is the basis for the insurance industry, because it means that insurance premiums are positive. People would rather pay a small insurance premium than gamble the price of their house against the chance of a fire. From the insurance company’s point of view, the price of the house is very small compared with the firm’s total reserves. This means that the insurer’s utility curve is approximately linear over such a small region, and the gamble costs the company almost nothing.","Risk-averse, Risk-seeking, Certainty equivalent, Insurance premium"
AIMA,,laziness: it is too much work to list the complete set of antecedents or consequents needed to ensure an exceptionless rule and too hard to use such rules.,laziness
AIMA,chapter7_5,"In Horn form, the premise is called the body and the conclusion is called the head. A sentence consisting of a single positive literal, such as L1, 1, is called a fact.","Body, Head, Fact"
AIMA,chapter6_2,"A variable in a CSP is arc-consistent1 if every value in its domain satisfies the variable’s binary constraints. More formally, Xi is arc-consistent with respect to another variable Xj if for every value in the current domain Di there is some value in the domain Dj that satisfies the binary constraint on the arc (Xi, Xj). A graph is arc-consistent if every variable is arc-consistent with every other variable. For example, consider the constraint Y = X2 where the domain of both X and Y is the set of decimal digits.",Arc consistency
AIMA,chapter17_1,"We have defined the utility of a state, , as the expected sum of discounted rewards from that point onwards. From this, it follows that there is a direct relationship between the utility of a state and the utility of its neighbors: the utility of a state is the expected reward for the next transition plus the discounted utility of the next state, assuming that the agent chooses the optimal action. This is called the Bellman equation.",Bellman equation
SLP,,"By bias, we mean (loosely) the tendency of a predictive hypothesis to deviate from the expected value when averaged over different training sets. Bias often results from restrictions imposed by the hypothesis space. For example, the hypothesis space of linear functions induces a strong bias: it only allows functions consisting of straight lines. If there are any patterns in the data other than the overall slope of a line, a linear function will not be able to represent those patterns. We say that a hypothesis is underfitting when it fails to find a pattern in the data. On the other hand, the piecewise linear function has low bias; the shape of the function is driven by the data.",underfitting
SLP,slp-7,"In such cases we use multinomial logistic regression, also called softmax regression (in older NLP literature you will sometimes see the name maxent classifier). In multinomial logistic regression we want to label each observation with a class k from a set of k classes, under the stipulation that only one of these classes is the correct one (sometimes called hard classification; an observation can not be in multiple classes). Let’s use the following representation: the output y for each input x will be a vector of length k. If class c is the correct class, we’ll set y. = 1, and set all the other elements of y to be 0, i.e., y. = 1 and y; = 0 vj #c. A vector like this y, with one value = 1 and the rest 0, is called a one-hot vector. The job of the classifier is produce an estimate vector $. For each class k, the value j, will be the classifier’s estimate of the probability p(y, = 1|x).","logistic regression, multinomial logistic regression"
SLP,slp_4,"In this chapter, we'll introduce feedforward networks as classifiers, and also apply them to the simple task of language modeling: assigning probabilities to word sequences and predicting upcoming words. In subsequent chapters, we'll introduce many other aspects of neural models, such as recurrent neural networks and the transformer (chapter 9), contextual embeddings like bert (chapter 11), and encoder-decoder models and attention (chapter 10).",feedforward
AIMA,chapter5_2,"MAX wants to find a sequence of actions leading to a win, but MIN has something to say about it. This means that MAX’s strategy must be a conditional plan—a contingent strategy specifying a response to each of MIN’s possible moves. In games that have a binary outcome (win or lose), we could use AND–OR search (page 125) to generate the conditional plan. In fact, for such games, the definition of a winning strategy for the game is identical to the definition of a solution for a nondeterministic planning problem: in both cases the desirable outcome must be guaranteed no matter what the “other side” does. For games with multiple outcome scores, we need a slightly more general algorithm called minimax search.",Minimax search
AIMA,chapter7_5,"The second concept we will need is validity. A sentence is valid if it is true in all models. For example, the sentence P ∨ ¬P is valid. Valid sentences are also known as tautologies—they are necessarily true. Because the sentence True is true in all models, every valid sentence is logically equivalent to True. What good are valid sentences? From our definition of entailment, we can derive the deduction theorem, which was known to the ancient Greeks.","Validity, Tautology, Deduction theorem"
AIMA,chapter3_6,"One might ask whether h2 is always better than h1. The answer is “Essentially, yes.” It is easy to see from the definitions of the two heuristics that for any node n, h2(n) ≥ h1(n). We thus say that h2 dominates h1. Domination translates directly into efficiency: A* using h2 will never expand more nodes than A* using h1 (except in the case of breaking ties unluckily). The argument is simple. Recall the observation on page 90 that every node with f(n) < C∗ will surely be expanded. This is the same as saying that every node with h(n) < C∗ − g(n) is surely expanded when h is consistent. But because h2 is at least as big as h1 for all nodes, every node that is surely expanded by A* search with h2 is also surely expanded with h1, and h1 might cause other nodes to be expanded as well. Hence, it is generally better to use a heuristic function with higher values, provided it is consistent and that the computation time for the heuristic is not too long.",Domination
AIMA,chapter7_3,"When we need to be precise, we use the term model in place of “possible world.” Whereas possible worlds might be thought of as (potentially) real environments that the agent might or might not be in, models are mathematical abstractions, each of which has a fixed truth value (true or false) for every relevant sentence. Informally, we may think of a possible world as, for example, having x men and y women sitting at a table playing bridge, and the sentence x + y = 4 is true when there are four people in total. Formally, the possible models are just all possible assignments of nonnegative integers to the variables x and y. Each such assignment determines the truth of any sentence of arithmetic whose variables are x and y. If a sentence α is true in model m, we say that m satisfies α or sometimes m is a model of α. We use the notation M(α) to mean the set of all models of α.","Model, Satisfaction"
AIMA,chapter7_5,"Backward chaining is a form of goal-directed reasoning. It is useful for answering specific questions such as “What shall I do now?” and “Where are my keys?” Often, the cost of backward chaining is much less than linear in the size of the knowledge base, because the process touches only relevant facts.",Goal-directed reasoning
SLP,slp-7,"Binary versus multinomial logistic regression. Binary logistic regression uses a single weight vector w, and has a scalar output y. In multinomial logistic regression we have k separate weight vectors corresponding to the k classes, all packed into a single weight matrix w and a vector of y.","weight vector, output"
AIMA,chapter17_2,"In the previous section, we observed that it is possible to get an optimal policy even when the utility function estimate is inaccurate. If one action is clearly better than all others, then the exact magnitude of the utilities on the states involved need not be precise. This insight suggests an alternative way to find optimal policies. The policy iteration algorithm alternates the following two steps, beginning from some initial policy π0: POLICY EVALUATION: given a policy πi, calculate Ui = Uπi , the utility of each state if πi were to be executed. POLICY IMPROVEMENT: Calculate a new MEU policy πi+1, using one-step look-ahead based on Ui","Policy iteration, Policy evaluation, Policy improvement"
SLP,slp_4,What do we do about words that occur in our test data but are not in our vocabulary at all because they did not occur in any training document in any class? The solution for such unknown words is to ignore them—remove them from the test document and not include any probability for them at all.,unknown word
AIMA,chapter16_2,"As in game-playing, in a deterministic environment an agent needs only a preference ranking on states—the numbers don’t matter. This is called a value function or ordinal utility function.","Value function, Ordinal utility function"
AIMA,chapter22_2,"by ignoring the connections between states, direct utility estimation misses opportunities forlearning. for example, the second of the three trials given earlier reaches the state (3, 2), which has not previously been visited. the next transition reaches (3, 3), which is knownfrom the first trial to have a high utility. the bellman equation suggests immediately that(3, 2) is also likely to have a high utility, because it leads to (3, 3), but direct utility estimationlearns nothing until the end of the trial. more broadly, we can view direct utility estimationas searching for u in a hypothesis space that is much larger than it needs to be, in that itincludes many functions that violate the bellman equations. for this reason, the algorithm 4",direct utility estimation
AIMA,chapter22_2,"each sequence, the algorithm calculates the observed reward-to-go for each state andupdates the estimated utility for that state accordingly, just by keeping a running average feach state in a table. in the limit of infinitely many trials, the sample average will convergeto the true expectation in equation (22.1)4",reward-to-go
AIMA,chapter3_5,"This section shows how an informed search strategy—one that uses domain-specific hints about the location of goals—can find solutions more efficiently than an uninformed strategy. The hints come in the form of a heuristic function, denoted h(n): h(n) = estimated cost of the cheapest path from the state at node n to a goal state.","Informed search, Heuristic function"
AIMA,chapter5_2,"In Section 3.3.3, we noted that redundant paths to repeated states can cause an exponential increase in search cost, and that keeping a table of previously reached states can address this problem. In game tree search, repeated states can occur because of transpositions—different permutations of the move sequence that end up in the same position, and the problem can be addressed with a transposition table that caches the heuristic value of states.","Transposition, Transposition table"
AIMA,chapter17_3,"Suppose we augment M so that at each state in M, the agent has two choices: either continue with M as before, or quit and receive an infinite sequence of λ-rewards (see Figure 17.13(a) ). This turns M into an MDP, whose optimal policy is just the optimal stopping rule for M. Hence the value of an optimal policy in this new MDP is equal to the value of an infinite sequence of λ-rewards, that is, λ/(1 − γ). So we can just solve this MDP ... but, unfortunately, we don’t know the value of λ to put into the MDP, as this is precisely what we are trying to calculate. But we do know that, at the tipping point, an optimal policy is indifferent between M and Mλ, so we could replace the choice to get an infinite sequence of λ-rewards with the choice to go back and restart M from its initial state s. (More precisely, we add a new action in every state that has the same rewards and outcomes as the action available in s; see Exercise 17.KATV.) This new MDP Ms , called a restart MDP",restart MDP
AIMA,chapter22_2,"all temporal-difference methods work by adjusting the utility estimates toward the idealequilibrium that holds locally when the utility estimates are correct. in the case of passivelearning, the equilibrium is given by equation (22.2). now equation (22.3)© does in factcause the agent to reach the equilibrium given by equation (22.2), but there is somesubtlety involved. first, notice that the update involves only the observed successor s', whereas the actual equilibrium conditions involve all possible next states. one might thinkthat this causes an improperly large change in u™(s) when a very rare transition occurs: bu",temporal-difference
SLP,,"If there are no attributes left, but both positive and negative examples, it means that these examples have exactly the same description, but different classifications. This can happen because there is an error or noise in the data; because the domain is nondeterministic; or because we can’t observe an attribute that would distinguish the examples. The best we can do is return the most common output value of the remaining examples.",noise
AIMA,chapter1_3,"Logic as conventionally understood requires knowledge of the world that is certain—a condition that, in reality, is seldom achieved. We simply don’t know the rules of, say, politics or warfare in the same way that we know the rules of chess or arithmetic. The theory of probability fills this gap, allowing rigorous reasoning with uncertain information. In principle, it allows the construction of a comprehensive model of rational thought, leading from raw perceptual information to an understanding of how the world works to predictions about the future. What it does not do, is generate intelligent behavior. For that, we need a theory of rational action. Rational thought, by itself, is not enough.",Probability
AIMA,chapter3_5,"There are a variety of suboptimal search algorithms, which can be characterized by the criteria for what counts as “good enough.” In bounded suboptimal search, we look for a solution that is guaranteed to be within a constant factor of the optimal cost. Weighted A* provides this guarantee. In bounded-cost search, we look for a solution whose cost is less than some constant And in unbounded-cost search, we accept a solution of any cost, as long as we can find it quickly.","Bounded suboptimal search, Bounded-cost search, Unbounded-cost search"
SLP,,"Fig. 9.15 illustrates the flow of information in a single causal, or backward-looking, self-attention layer. As with the overall transformer, a self-attention layer maps input sequences (x1, ..., xn) to output sequences of the same length (y1, ..., yn). When processing each item in the input, the model has access to all of the inputs up to and including the one under consideration, but no access to information about inputs beyond the current one. In addition, the computation performed for each item i is:",self-attention
SLP,19_1,"This chapter assumes little prior knowledge on the part of the agent: it starts from scratch and learns from the data. In section 21.7.2'5 we consider transfer learning, in which knowledge from one domain is transferred to a new domain, so that learning can proceed faster with less data. We do assume, however, that the designer of the system chooses a model framework that can lead to effective learning.",prior knowledge
AIMA,chapter17_3,"Perhaps the simplest and best-known instance of a bandit problem is the Bernoulli bandit, where each arm Mi produces a reward of 0 or 1 with a fixed but unknown probability μi.",Bernoulli bandit
AIMA,chapter6_4,"Local search algorithms (see Section 4.1) turn out to be very effective in solving many CSPs. They use a complete-state formulation (as introduced in Section 4.1.1) where each state assigns a value to every variable, and the search changes the value of one variable at a time. As an example, we’ll use the 8-queens problem, as defined as a CSP on page 183. In Figure 6.8 we start on the left with a complete assignment to the 8 variables; typically this will violate several constraints. We then randomly choose a conflicted variable, which turns out to be Q8, the rightmost column. We’d like to change the value to something that brings us closer to a solution; the most obvious approach is to select the value that results in the minimum number of conflicts with other variables—the min-conflicts heuristic.",Min-conflicts
AIMA,,"the connection between toothaches and cavities is not a strict logical consequence in eithe direction. this is typical of the medical domain, as well as most other judgmental domains: law, business, design, automobile repair, gardening, dating, and so on. the agent's knowledge can at best provide only a degree of belief in the relevant sentences. our main tool for dealing with degrees of belief is probability theory. in the terminology of section 8.14 the ontological commitments of logic and probability theory are the same—that the world is composed of facts that do or do not hold in any particular case—but the epistemological commitments are different: a logical agent believes each sentence to be true or false or has no opinion, whereas a probabilistic agent may have a numerical degree at haliaf hatwaan (fae cantancbe that are gartainie tales) and 1 (eartainle tee).","degree of belief, probability theory"
SLP,slp_4,"And negative word features from sentiment lexicons, lists of words that are pre annotated with positive or negative sentiment. Four popular lexicons are the Genera Inquirer (Stone et al., 1966), LIWC (Pennebaker et al., 2007), the Opinion Lexicon (Hu and Liu, 2004), and AFINN (Nielsen, 2011).",sentiment lexicons
SLP,slp-7,The loss function for multinomial logistic regression generalizes the loss function for binary logistic regression from 2 to k classes. Recall that the cross-entropy loss for binary logistic regression (repeated from eq. 5.22) is:,logistic regression
SLP,slp-7,"There is a problem with learning weights that make the model perfectly match the training data. If a feature is perfectly predictive of the outcome because it happens to only occur in one class, it will be assigned a very high weight. The weights for features will attempt to perfectly fit details of the training set, in fact too perfectly, modeling noisy factors that just accidentally correlate with the class. This problem is called overfitting. A good model should be able to generalize well from the training data to the unseen test set, but a model that overfits will have poor generalization. To avoid overfitting, a new regularization term r(@) is added to the objective.","regularization, regularization"
AIMA,chapter17_2,"This general approach is called real-time dynamic programming (RTDP) and is quite analogous to LRTA* in Chapter 4. Algorithms of this kind can be quite effective in moderate-sized domains such as grid worlds; in larger domains such as Tetris, there are two issues. First, the state space is such that any manageable set of explored states contains very few repeated states, so one might as well use a simple expectimax tree. Second, a simple heuristic for frontier nodes may not be enough to guide the agent, particularly if rewards are sparse.",Real-time dynamic programming (RTDP)
AIMA,chapter6_1,"Preference constraints can often be encoded as costs on individual variable assignments—for example, assigning an afternoon slot for Prof. R costs 2 points against the overall objective function, whereas a morning slot costs 1. With this formulation, CSPs with preferences can be solved with optimization search methods, either path-based or local. We call such a problem a constrained optimization problem, or COP. Linear programs are one class of COPs.",Constrained optimization problem
AIMA,chapter17_1,"To complete the definition of the task environment, we must specify the utility function for the agent. Because the decision problem is sequential, the utility function will depend on a sequence of states and actions—an environment history—rather than on a single state. Later in this section, we investigate the nature of utility functions on histories; for now, we simply stipulate that for every transition from s to s′ via action a, the agent receives a reward R(s, a, s′).",Reward
AIMA,chapter17_1,"The next question is, what does a solution to the problem look like? No fixed action sequence can solve the problem, because the agent might end up in a state other than the goal. Therefore, a solution must specify what the agent should do for any state that the agent might reach. A solution of this kind is called a policy. It is traditional to denote a policy by π, and π(s) is the action recommended by the policy π for state s. No matter what the outcome of the action, the resulting state will be in the policy, and the agent will know what to do next.",Policy
SLP,,"Continuous-valued output attribute: if we are trying to predict a numerical output value, such as the price of an apartment, then we need a regression tree rather than a classification tree. A regression tree has at each leaf a linear function of some subset of numerical attributes, rather than a single output value. For example, the branch for two-bedroom apartments might end with a linear function of square footage and number of bathrooms. The learning algorithm must decide when to stop splitting and begin applying linear regression (see section 19.612) over the attributes. The name CART, standing for classification and regression trees, is used to cover both classes.","cart, regression tree"
SLP,,"If we are only going to create one hypothesis, then this approach is sufficient. But often we will end up creating multiple hypotheses: we might want to compare two completely different machine learning models, or we might want to adjust the various ""knobs"" within one model. For example, we could try different thresholds for χ2 pruning of decision trees, or different degrees for polynomials. We call these ""knobs"" hyperparameters—parameters of the model class, not of the individual model.",hyperparameters
AIMA,chapter7_3,"we said that knowledge bases consist of sentences. These sentences are expressed according to the syntax of the representation language, which specifies all the sentences that are well formed. A logic must also define the semantics, or meaning, of sentences. The semantics defines the truth of each sentence with respect to each possible world.","Semantics,Truth, Possible world"
AIMA,chapter16_1,"Decision theory, in its simplest form, deals with choosing among actions based on the desirability of their immediate outcomes; that is, the environment is assumed to be episodic in the sense defined on page 45. (This assumption is relaxed in Chapter 17.) The agent’s preferences are captured by a utility function, U(s), which assigns a single number to express the desirability of a state. The expected utility of an action given the evidence, EU(a) , is just the average utility value of the outcomes, weighted by the probability that the outcome occurs","Utility function, Expected utility"
SLP,,"The final gate we'll use is the output gate, which is used to decide what information is required for the current hidden state (as opposed to what information needs to be preserved for future decisions).",output gate
SLP,slp_4,"We would like to know if 6(x) > 0, meaning that our logistic regression classifier has a higher f, than our naive bayes classifier on x. 5(x) is called the effect size; a bigger 6 means that a seems to be way better than b; a small 5 means a seems to be only a little better.",effect size
SLP,slp_4,"There are two common non-parametric tests used in NLP: approximate randomization (Noreen, 1989) and the bootstrap test. We will describe bootstrap below, showing the paired version of the test, which again is most common in NLP. Paired tests are those in which we compare two sets of observations that are aligned: each observation in one set can be paired with an observation in another. This happens naturally when we are comparing the performance of two systems on the same test set; we can pair the performance of system A on an individual observation xj with the performance of system B on the same xj.","paired, bootstrap test"
AIMA,chapter5_3,"The evaluation function should be applied only to positions that are quiescent—that is, positions in which there is no pending move (such as a capturing the queen) that would wildly swing the evaluation. For nonquiescent positions the IS-CUTOFF returns false, and the search continues until quiescent positions are reached. This extra quiescence search is sometimes restricted to consider only certain types of moves, such as capture moves, that will quickly resolve the uncertainties in the position.","Quiescence, Quiescence search"
SLP,slp_4,"Another thing we might want to know about a text is the language it’s written in. Texts on social media, for example, can be in any number of languages and we'll need to apply different processing. The task of language ID is thus the first step in most language processing pipelines. Related text classification tasks like authorship attribution— determining a text’s author— are also relevant to the digital humanities and francophone linguistics.",language id
AIMA,chapter3_1,"Control theorists call this an open-loop system: ignoring the percepts breaks the loop between agent and environment. If there is a chance that the model is incorrect, or the environment is nondeterministic, then the agent would be safer using a closed-loop approach that monitors the percepts","open-loop, closed-loop"
AIMA,chapter1_5,"The problem of achieving agreement between our true preferences and the objective we put into the machine is called the value alignment problem: the values or objectives put into the machine must be aligned with those of the human. If we are developing an AI system in the lab or in a simulator—as has been the case for most of the field’s history—there is an easy fix for an incorrectly specified objective: reset the system, fix the objective, and try again. As the field progresses towards increasingly capable intelligent systems that are deployed in the real world, this approach is no longer viable. A system deployed with an incorrect objective will have negative consequences. Moreover, the more intelligent the system, the more negative the consequences.",Value alignment problem
AIMA,chapter7_4,"⇒ (implies). A sentence such as (W1, 3 ∧ P3, 1) ⇒ ¬W2, 2 is called an implication (or conditional). Its premise or antecedent is (W1, 3 ∧ P3, 1), and its conclusion or consequent is ¬W2, 2. Implications are also known as rules or if–then statements. The implication symbol is sometimes written in other books as ⊃ or →.","Implication, Premise, Conclusion, Rules"
AIMA,chapter6_5,"Completely independent subproblems are delicious, then, but rare. Fortunately, some other  graph structures are also easy to solve. For example, a constraint graph is a tree when any  two variables are connected by only one path. We will show that any tree-structured CSP can  be solved in time linear in the number of variables.4 The key is a new notion of consistency,   called directional arc consistency or DAC. A CSP is defined to be directional arc-consistent  under an ordering of variables X1, X2, …, Xn if and only if every Xi is arc-consistent with  each Xj  for j > i. To solve a tree-structured CSP, first pick any variable to be the root of the tree, and choose an ordering of the variables such that each variable appears after its parent in the tree. Such an ordering is called a topological sort.","Directional arc consistency, Topological sort"
SLP,,"The chain rule extends to more than two functions. If computing the derivative of a composite function f(x) = u(v(w(x))), the derivative of f(x) is:",chain rule
SLP,,"Recall that we evaluate language models by examining how well they predict unseen text. Intuitively, good models are those that assign higher probabilities to unseen data (are less surprised when encountering the new words).",words
SLP,slp-7,"Features in multinomial logistic regression function similarly to binary logistic regression, with the difference mentioned above that we'll need separate weight vectors and biases for each of the k classes. Recall our binary exclamation point feature.",logistic regression
AIMA,,"the utility of a state is relative to an agent. for example, the utility of a state in which white has checkmated black in a game of chess is obviously high for the agent playing white, but low for the agent playing black. but we can’t go strictly by the scores of 1, 1/2, and 0 that are dictated by the rules of tournament chess—some players (including the authors) might be thrilled with a draw against the world champion, whereas other players (including the former world champion) might not. there is no accounting for taste or preferences: you might think that an agent who prefers jalapefio bubble-gum ice cream to chocolate chip is odd, but you could not say the agent is irrational. a utility function can account for any set of preferences—quirky or typical, noble or perverse. note that utilities can account for altruism, simply by including the welfare of others as one of the factors.",preference
SLP,,"Father' is to 'doctor' as 'mother' is to 'nurse'. This could result in what Crawford (2017) and Blodgett et al. (2020) call an allocation harm, when a system allocates resources (jobs or credit) unfairly to different groups. for example algorithms hat use embeddings as part of a search for hiring potential programmers or doctors night thus incorrectly down weight documents with women's names. it turns out that embeddings don't just reflect the statistics of their input, but also",allocational harm
SLP,slp_4,"We'll discuss three popular non-linear functions, f(), below (the sigmoid, the tanh, and the rectified linear unit, or relu), but it's pedagogically convenient to start with the sigmoid function since we saw it in chapter 5.",sigmoid
AIMA,chapter17_1,"Another important quantity is the action-utility function, or Q-function: is the expected utility of taking a given action in a given state.",Q-function
AIMA,chapter3_6,"Some route-finding algorithms save even more time by adding shortcuts—artificial edges in the graph that define an optimal multi-action path. For example, if there were shortcuts predefined between all the 100 biggest cities in the U.S., and we were trying to navigate from the Berkeley campus in California to NYU in New York, we could take the shortcut between Sacramento and Manhattan and cover 90% of the path in one action.",Shortcuts
AIMA,chapter12_1,theoretical ignorance: medical science has no complete theory for the domain.,theoretical ignorance
SLP,,"The revolutionary intuition here is that we can just use running text as implicitly supervised training data for such a classifier; a word c that occurs near the target word apricot acts as gold ‘correct answer’ to the question “is word c likely to show up near apricot?” This method, often called self-supervision, avoids the need for any sort of hand-labeled supervision signal. This idea was first proposed in the task of neural language modeling, when Bengio et al. (2003) and Collobert et al. (2011) showed that a neural language model (a neural network that learned to predict the",self-supervision
SLP,,"The simplest activation function, and perhaps the most commonly used, is the rectified linear unit, also called the relu, shown in Fig. 7.3b. It’s just the same as z when z is positive, and 0 otherwise:",relu
SLP,,"The function h is called a hypothesis about the world. It is drawn from a hypothesis space h of possible functions. For example, the hypothesis space might be the set of polynomials of degree 3: or the set of javascript functions: or the set of 3-sat boolean logic formulas.",hypothesis space
SLP,,"One final warning: You might think that X” pruning and information gain look similar, so why not combine them using an approach called early stopping—have the decision tree algorithm stop generating nodes when there is no good attribute to split on, rather than going to all the trouble of generating nodes and then pruning them away? The problem with early stopping is that it stops us from recognizing situations where there is no one good attribute, but there are combinations of attributes that are informative. For example, consider the XOR function of two binary attributes. If there are roughly equal numbers of examples for all four combinations of input values, then neither attribute will be informative, yet the correct thing to do is to split on one of the attributes (it doesn’t matter which one), and then at the second level we will get splits that are very informative. Early stopping would miss this, but generate-and-then-prune handles it correctly.",early stopping
AIMA,chapter7_5,"Inference with Horn clauses can be done through the forward-chaining and backward-chaining algorithms, which we explain next. Both of these algorithms are natural, in that the inference steps are obvious and easy for humans to follow. This type of inference is the basis for logic programming","Forward-chaining, Backward-chaining"
AIMA,,"the fundamental idea of decision theory is that an agent is rational if and only if it chooses th action that yields the highest expected utility, averaged over all the possible outcomes of the action this is called the principle of maximum expected utility (meu). here, “expected” means the “average, ” or “statistical mean” of the outcome utilities, weighted by the probability of the outcome. we saw this principle in action in chapter 5'8 when we touched briefly on optimal decisions in backgammon; it is in fact a completely general principle for singleagent decision making.","decision theory, maximum expected utility (meu)"
AIMA,chapter3_4,Depth-first search always expands the deepest node in the frontier first. It could be implemented as a call to BEST-FIRST-SEARCH where the evaluation function f is the negative of the depth.,Depth-first search
AIMA,chapter12_1,"ve nave seen problem-solving and logical agents handle uncertainty dy keeping track of a belief state—a representation of the set of all possible world states that it might be in—and generating a contingency plan that handles every possible eventuality that its sensors may report during execution. this approach works on simple problems, but it has drawbacks:",uncertainty
AIMA,chapter17_3,"Instead, we can take advantage of the loose nature of the interaction between the arms. Thisinteraction arises only from the agent’s limited ability to attend to the arms simultaneously.To some extent, the interaction can be modeled by the notion of opportunity cost: howmuch utility is given up per time step by not devoting that time step to another arm. Thehigher the opportunity cost, the more necessary it is to generate early rewards in a givenarm. In some cases, an optimal policy in a given arm is unaffected by the opportunity cost.(Trivially, this is true in a Markov reward process because there is only one policy.) In thatcase, an optimal policy can be applied, converting that arm into a Markov reward process.",Opportunity cost
SLP,,"Session to depend on information from hundreds of words in the past. The transformer offers new mechanisms (self-attention and positional encodings) that help represent time and help focus on how words relate to each other over long distances. We'll see how to apply both models to the task of language modeling, to sequence labeling tasks like part-of-speech tagging, and to text classification tasks like sentiment analysis.","words, self-attention"
AIMA,chapter6_1,"In addition to examining the types of variables that can appear in CSPs, it is useful to look at the types of constraints. The simplest type is the unary constraint, which restricts the value of a single variable. For example, in the map-coloring problem it could be the case that South Australians won’t tolerate the color green; we can express that with the unary constraint ⟨(SA), SA ≠ green⟩. (The initial specification of the domain of a variable can also be seen as a unary constraint.) A binary constraint relates two variables. For example, SA ≠ NSW is a binary constraint. A binary CSP is one with only unary and binary constraints. A constraint involving an arbitrary number of variables is called a global constraint. (The name is traditional but confusing because a global constraint need not involve all the variables in a problem). One of the most common global constraints is Alldiff, which says that all of the variables involved in the constraint must have different values. In Sudoku problems (see Section 6.2.6), all variables in a row, column, or 3 × 3 box must satisfy an Alldiff constraint.",Unary constraint
SLP,,"From this, one can infer that syllable structure is highly important in the German language (Schmidt and Pedersen, 1993). Two words have first-order co-occurrence (sometimes called syntagmatic association) if they are typically nearby each other. Thus, ""wrote"" is a first-order associate of ""book"" or ""poem."" Two words have second-order co-occurrence (sometimes called paradigmatic association) if they have similar neighbors. Thus, ""wrote"" is a second-order associate of words like ""said"" or ""remarked.""",first-order co-occurrence
SLP,,"This example was first shown for the perceptron, which is a very simple neural net that has a binary output and does not have a non-linear activation function. The output y of a perceptron is 0 or 1, and is computed as follows (using the same weight v, input x, and bias b as in Eq. 7.2):",perceptron
SLP,,"Selecting the embedding vector for word ""vs"" by multiplying the embedding .e with a one-hot vector with a ""|"" in index 5.",one-hot vector
AIMA,chapter2_1,"We can imagine tabulating the agent function that describes any given agent; for most agents, this would be a very large table—infinite, in fact, unless we place a bound on the length of percept sequences we want to consider. Given an agent to experiment with, we can, in principle, construct this table by trying out all possible percept sequences and recording which actions the agent does in response.1 The table is, of course, an external characterization of the agent. Internally, the agent function for an artificial agent will be implemented by an agent program. It is important to keep these two ideas distinct. The agent function is an abstract mathematical description; the agent program is a concrete implementation, running within some physical system.",Agent program
AIMA,chapter7_4,"The rules can also be expressed with truth tables that specify the truth value of a complex sentence for each possible assignment of truth values to its components. Truth tables for the five connectives are given in Figure 7.8 . From these tables, the truth value of any sentence s can be computed with respect to any model m by a simple recursive evaluation.",Truth table
SLP,,"A decision tree is a representation of a function that maps a vector of attribute values to a single output value—a “decision.” A decision tree reaches its decision by performing a sequence of tests, starting at the root and following the appropriate branch until a leaf is reached. Each internal node in the tree corresponds to a test of the value of one of the input attributes, the branches from the node are labeled with the possible values of the attribute, and the leaf nodes specify what value is to be returned by the function.",decision tree
AIMA,chapter2_1,"We use the term percept to refer to the content an agent’s sensors are perceiving. An agent’s percept sequence is the complete history of everything the agent has ever perceived. In general, an agent’s choice of action at any given instant can depend on its built-in knowledge and on the entire percept sequence observed to date, but not on anything it hasn’t perceived. By specifying the agent’s choice of action for every possible percept sequence, we have said more or less everything there is to say about the agent. Mathematically speaking, we say that an agent’s behavior is described by the agent function that maps any given percept sequence to an action.","Percept, Percept sequence, Agent function"
AIMA,,"Let us make this idea more concrete. Most evaluation functions work by calculating various features of the state—for example, in chess, we would have features for the number of white pawns, black pawns, white queens, black queens, and so on. The features, taken together, define various categories or equivalence classes of states: the states in each category have the same values for all the features. For example, one category might contain all two-pawn versus one-pawn endgames. Any given category will contain some states that lead (with perfect play) to wins, some that lead to draws, and some that lead to losses.",Features
SLP,slp_4,"In both cases, we need a metric for knowing how well our spam detector (or pie-tweet-detector) is doing. To evaluate any system for detecting things, we start by building a confusion matrix like the one shown in Fig. 4.4. A confusion matrix is a table for visualizing how an algorithm performs with respect to the human gold labels, using two dimensions (system output and gold labels), and each cell labeling a set of possible outcomes. In the spam detection case, for example, true positives are documents that are indeed spam (indicated by human-created gold labels) that our system correctly said were spam. False negatives are documents that are indeed spam but our system incorrectly labeled as non-spam.",confusion matrix
AIMA,chapter5_2,"The possible moves for MAX at the root node are labeled a1, a2, and a3. The possible replies to a1 for MIN are b1, b2, b3, and so on. This particular game ends after one move each by MAX and MIN. (NOTE: In some games, the word “move” means that both players have taken an action; therefore the word ply is used to unambiguously mean one move by one player, bringing us one level deeper in the game tree.) The utilities of the terminal states in this game range from 2 to 14.",ply
AIMA,chapter3_5,"The most common informed search algorithm is A* search (pronounced “A-star search”), a best-first search that uses the evaluation function f(n) = g(n) + h(n); where g(n) is the path cost from the initial state to node n, and h(n) is the estimated cost of  the shortest path from n to a goal state, so we have  f(n) = estimated cost of the best path that continues from n to a goal.",A* search
AIMA,chapter6_3,"This leads to a different–and deeper–notion of the conflict set for a variable such as NT: it is that set of preceding variables that caused NT, together with any subsequent variables, to have no consistent solution. In this case, the set is WA and NSW, so the algorithm should backtrack to NSW and skip over Tasmania. A backjumping algorithm that uses conflict sets defined in this way is called conflict-directed backjumping.",Conflict-directed backjumping
AIMA,chapter7_5,A sentence expressed as a conjunction of clauses is said to be in conjunctive normal form or CNF,Conjunctive normal form
SLP,,"""St"" is the term frequency (Luhn, 1957): the frequency of the word ""f"" in the 1. We can just use the raw count as the term frequency:",term frequency
AIMA,chapter7_5,"Slightly more general is the Horn clause, which is a disjunction of literals of which at most one is positive. So all definite clauses are Horn clauses, as are clauses with no positive literals. these are called goal clauses. Horn clauses are closed under resolution: if you resolve two. Horn clauses, you get back a Horn clause. One more class is the k-CNF sentence, which is a CNF sentence where each clause has at most k literals.","Horn clause, Goal clauses"
AIMA,chapter7_4,"¬ (not). A sentence such as ¬W1, 3 is called the negation of W1, 3. A literal is either an atomic sentence (a positive literal) or a negated atomic sentence (a negative literal)","Negation, Literal"
AIMA,chapter17_1,"So, with a finite horizon, an optimal action in a given state may depend on how much time is left. A policy that depends on the time is called nonstationary. With no fixed time limit, on the other hand, there is no reason to behave differently in the same state at different times. Hence, an optimal action depends only on the current state, and the optimal policy is stationary. Policies for the infinite-horizon case are therefore simpler than those for the finite-horizon case, and we deal mainly with the infinite-horizon case in this chapter. (We will see later that for partially observable environments, the infinite-horizon case is not so simple.) Note that “infinite horizon” does not necessarily mean that all state sequences are infinite; it just means that there is no fixed deadline. There can be finite state sequences in an infinite-horizon MDP that contains a terminal state.","Nonstationary policy, Stationary policy"
SLP,,"Visualizing embeddings is an important goal in helping understand, apply, and improve these models of word meaning. But how can we visualize a (for example) 100-dimensional vector?",visualizing embeddings
AIMA,chapter7_4,"∨ (or). A sentence whose main connective is ∨, such as (W1, 3 ∧ P3, 1) ∨ W2, 2, is a disjunction; its parts are disjuncts—in this example, (W1, 3 ∧ P3, 1) and W2, 2.",Disjunction
AIMA,chapter5_1,"We show that pruning makes the search more efficient by ignoring portions of the search tree that make no difference to the optimal move. For nontrivial games, we will usually not have enough time to be sure of finding the optimal move (even with pruning); we will have to cut off the search at some point.",Pruning
AIMA,,"like logical assertions, probabilistic assertions are about possible worlds. whereas logical assertions say which possible worlds are strictly ruled out (all those in which the assertion is false), probabilistic assertions talk about how probable the various worlds are. in probability theory, the set of all possible worlds is called the sample space. the possible worlds are mutually exclusive and exhaustive—two possible worlds cannot both be the case, and one possible world must be the case. for example, if we are about to roll two (distinguishable) dice, there are 36 possible worlds to consider: (1, 1), (1, 2), ..., (6, 6). the greek letter 0 (uppercase omega) is used to refer to the sample space, and w (lowercase omega) refers to elements of the space, that is, particular possible worlds.",sample space
SLP,,"Vectors semantics is the standard way to represent word meaning in NLP, helping us model many of the aspects of word meaning we saw in the previous section. The roots of the model lie in the 1950s when two big ideas converged: Osgood’s 1957 idea mentioned above to use a point in three-dimensional space to represent the connotation of a word, and the proposal by linguists like Joos (1950), Harris (1954) and Firth (1957) to define the meaning of a word by its distribution in language.",semantics
SLP,slp_4,F-measure comes from a weighted harmonic mean of precision and recall. The harmonic mean of a set of numbers is the reciprocal of the arithmetic mean of reciprocal values:,"precision, f-measure"
SLP,slp_4,"Often it’s more convenient to express this weighted sum using vector notation; recall from linear algebra that a vector is, at heart, just a list or array of numbers. Thus we’ll talk about z in terms of a weight vector w, a scalar bias b, and an input vector x, and we’ll replace the sum with the convenient dot product.",vector
AIMA,chapter17_2,"The Bellman equation (Equation (17.5)) is the basis of the value iteration algorithm for solving MDPs. If there are n possible states, then there are n Bellman equations, one for each state. The n equations contain n unknowns—the utilities of the states. So we would like to solve these simultaneous equations to find the utilities. There is one problem: the equations are nonlinear, because the “max” operator is not a linear operator. Whereas systems of linear equations can be solved quickly using linear algebra techniques, systems of nonlinear equations are more problematic. One thing to try is an iterative approach. We start with arbitrary initial values for the utilities, calculate the right-hand side of the equation, and plug it into the left-hand side—thereby updating the utility of each state from the utilities of its neighbors. We repeat this until we reach an equilibrium. Let Ui(s) be the utility value for state s at the ith iteration. The iteration step, called a  Bellman update","Value iteration, Bellman update"
AIMA,chapter1_2,"Once we have a sufficiently precise theory of the mind, it becomes possible to express the theory as a computer program. If the program’s input–output behavior matches corresponding human behavior, that is evidence that some of the program’s mechanisms could also be operating in humans. For example, Allen Newell and Herbert Simon, who developed GPS, the “General Problem Solver” (Newell and Simon 1961), were not content merely to have their program solve problems correctly. They were more concerned with comparing the sequence and timing of its reasoning steps to those of human subjects solving the same problems. The interdisciplinary field of cognitive science brings together computer models from AI and experimental techniques from psychology to construct precise and testable theories of the human mind.",Cognitive science
SLP,,"The hidden layer forming a new representation of the input. (B) shows the representation of the hidden layer, h, compared to the original input representation x in (A). Notice that the input point [0, 1] has been collapsed with the input point [1, 0], making it possible to linearly separate the positive and negative cases of xor. After Goodfellow et al.",hidden layer
AIMA,chapter17_2,"The algorithms we have described so far require updating the utility or policy for all states at once. It turns out that this is not strictly necessary. In fact, on each iteration, we can pick any subset of states and apply either kind of updating (policy improvement or simplified value iteration) to that subset. This very general algorithm is called asynchronous policy iteration.",Asynchronous policy iteration
AIMA,chapter7_5,"The final concept we will need is satisfiability. A sentence is satisfiable if it is true in, or satisfied by, some model. For example, the knowledge base given earlier, ( R1 ∧ R2 ∧ R3 ∧ R4 ∧ R5), is satisfiable because there are three models in which it is true, as shown in Figure 7.9 . Satisfiability can be checked by enumerating the possible models until one is found that satisfies the sentence. The problem of determining the satisfiability of sentences in propositional logic—the SAT problem—was the first problem proved to be NP-complete. Many problems in computer science are really satisfiability problems. For example, all the constraint satisfaction problems in Chapter 6 ask whether the constraints are satisfiable by some assignment.","Satisfiability, SAT"
AIMA,chapter17_1,"a sequential decision problem for a fully observable, stochastic environment with a Markovian transition model and additive rewards is called a Markov decision process, or MDP, and consists of a set of states (with an initial state s0); a set ACTIONS(s) of actions in each state; a transition model P(s′|s, a); and a reward function R(s, a, s′). Methods for solving MDPs usually involve dynamic programming: simplifying a problem by recursively breaking it into smaller pieces and remembering the optimal solutions to the pieces.","Markov decision process, Dynamic programming"
AIMA,chapter2_3,"FULLY OBSERVABLE VS. PARTIALLY OBSERVABLE: If an agent’s sensors give it access to the complete state of the environment at each point in time, then we say that the task environment is fully observable. A task environment is effectively fully observable if the sensors detect all aspects that are relevant to the choice of action; relevance, in turn, depends on the performance measure. Fully observable environments are convenient because the agent need not maintain any internal state to keep track of the world. An environment might be partially observable because of noisy and inaccurate sensors or because parts of the state are simply missing from the sensor data—for example, a vacuum agent with only a local dirt sensor cannot tell whether there is dirt in other squares, and an automated taxi cannot see what other drivers are thinking. If the agent has no sensors at all then the environment is unobservable. One might think that in such cases the agent’s plight is hopeless, but, as we discuss in Chapter 4 , the agent’s goals may still be achievable, sometimes with certainty.","Fully observable, Partially observable, Unobservable"
AIMA,chapter3_3,"A very general approach is called best-first search, in which we choose a node, n, with minimum value of some evaluation function, f(n). On each iteration we choose a node on the frontier with minimum f(n) value, return it if its state is a goal state, and otherwise apply EXPAND to generate child nodes. Each child node is added to the frontier if it has not been reached before, or is re-added if it is now being reached with a path that has a lower path cost than any previous path. The algorithm returns either an indication of failure, or a node that represents a path to a goal. By employing different f(n) functions, we get different specific algorithms, which this chapter will cover.","Best-first search, Evaluation function"
SLP,slp-7,"The learning rate 7 is a hyperparameter that must be adjusted. If it’s too high, the learner will take steps that are too large, overshooting the minimum of the loss function. If it’s too low, the learner will take steps that are too small, and take too long to get to the minimum. It is common to start with a higher learning rate and then slowly decrease it, so that it is a function of the iteration k of training; the notation mn, can be used to mean the value of the learning rate at iteration k.",hyperparameter
SLP,,"In unsupervised learning, the agent learns patterns in the input without any explicit feedback. The most common unsupervised learning task is clustering: detecting potentially useful clusters of input examples. For example, when shown millions of images taken from the Internet, a computer vision system can identify a large cluster of similar images which an English speaker would call ""cats.""","unsupervised learning, feedback"
AIMA,chapter12_1,"it is important to understand that p(cavity) = 0.2 is still valid after toothache is observed;just isn’t especially useful. when making decisions, an agent needs to condition on all theevidence it has observed. it is also important to understand the difference betweenconditioning and logical implication. the assertion that p(cavity | toothache) = 0.6 does nmean “whenever toothache is true, conclude that cavity is true with probability 0.6” rathe",evidence
SLP,slp_4,"To introduce the methods for evaluating text classification, let’s first consider some simple binary detection tasks. For example, in spam detection, our goal is to label every text as being in the spam category (“positive”) or not in the spam category (“negative”). For each item (email document) we therefore need to know whether our system called it spam or not. We also need to know whether the email is actually spam or not, i.e. the human-defined labels for each document that we are trying to match. We will refer to these human labels as the gold labels.",gold labels
SLP,slp_4,"We call naive bayes a generative model because we can read eq. 4.4 as stating a kind of implicit assumption about how a document is generated: first a class is sampled from p(c), and then the words are generated by sampling from p(d|c). (In fact we could imagine generating artificial documents, or at least their word counts, by following this process). We’ll say more about this intuition of generative models in chapter 5.",assumption
AIMA,chapter6_5,"The second way to reduce a constraint graph to a tree is based on constructing a tree decomposition of the constraint graph: a transformation of the original graph into a tree where each node in the tree consists of a set of variables, as in Figure 6.13. A tree decomposition must satisfy these three requirements: Every variable in the original problem appears in at least one of the tree nodes. If two variables are connected by a constraint in the original problem, they must appear together (along with the constraint) in at least one of the tree nodes. If a variable appears in two nodes in the tree, it must appear in every node along the path connecting those nodes.",Tree decomposition
AIMA,chapter16_3,"It will usually be the case that an agent prefers more money to less, all other things being equal. We say that the agent exhibits a monotonic preference for more money. This does not mean that money behaves as a utility function, because it says nothing about preferences between lotteries involving money.",Monotonic preference
AIMA,chapter12_1,"like logical assertions, probabilistic assertions are about possible worlds. whereas logicalassertions say which possible worlds are strictly ruled out (all those in which the assertion isfalse), probabilistic assertions talk about how probable the various worlds are. in probabilitytheory, the set of all possible worlds is called the sample space. the possible worlds aremutually exclusive and exhaustive—two possible worlds cannot both be the case, and onepossible world must be the case. for example, if we are about to roll two (distinguishable)dice, there are 36 possible worlds to consider: (1, 1), (1, 2), ..., (6, 6). the greek letter 0(uppercase omega) is used to refer to the sample space, and w (lowercase omega) refers toelements of the space, that is, particular possible worlds.",sample space
SLP,slp-7,"Scaling input features: When different input features have extremely different ranges of values, it’s common to rescale them so they have comparable ranges. We standardize input values by centering them to result in a zero mean and a standard deviation of one (this transformation is sometimes called the z-score). That is, if u_w is the mean of the values of feature x across the m observations in the input dataset and \sigma is the standard deviation of the values of feature x across the input dataset, we can replace each feature x by a new feature x’ computed as follows:",standardize
AIMA,chapter1_5,"It is impossible to anticipate all the ways in which a machine pursuing a fixed objective might misbehave. There is good reason, then, to think that the standard model is inadequate. We don’t want machines that are intelligent in the sense of pursuing their objectives; we want them to pursue our objectives. If we cannot transfer those objectives perfectly to the machine, then we need a new formulation—one in which the machine is pursuing our objectives, but is necessarily uncertain as to what they are. When a machine knows that it doesn’t know the complete objective, it has an incentive to act cautiously, to ask permission, to learn more about our preferences through observation, and to defer to human control. Ultimately, we want agents that are provably beneficial to humans. We will return to this topic in Section 1.5 .",Provably beneficial
AIMA,chapter6_5,"So far, we have looked at the structure of the constraint graph. There can also be important structure in the values of variables, or in the structure of the constraint relations themselves. Consider the map-coloring problem with d colors. For every consistent solution, there is actually a set of d! solutions formed by permuting the color names. For example, on the Australia map we know that WA, NT, and SA must all have different colors, but there are 3! = 6 ways to assign three colors to three regions. This is called value symmetry. We would like to reduce the search space by a factor of d! by breaking the symmetry in assignments. We do this by introducing a symmetry-breaking constraint. For our example, we might impose an arbitrary ordering constraint, NT < SA < WA, that requires the three values to be in alphabetical order. This constraint ensures that only one of the d! solutions is possible: {NT = blue, SA = green, WA = red}.","Value, symmetry, Symmetry-breaking, constraint"
SLP,,"Replacing the bias unit in describing networks, we will often use a slightly simplified notation that represents exactly the same function without referring to an explicit bias node b. Instead, we add a dummy node ao to each layer whose value will always be 1. Thus layer 0, the input layer, will have a dummy node al! = 1, layer 1 will have al}! = 1, and so on. This dummy node still has an associated weight, and that weight represents the bias value b. For example instead of an equation like",replacing the bias unit
SLP,,"The first gate we'll consider is the forget gate. The purpose of this gate is to delete information from the context that is no longer needed. The forget gate computes a weighted sum of the previous state's hidden layer and the current input and passes that through a sigmoid. This mask is then multiplied element-wise by the context vector to remove the information from context that is no longer required. Element-wise multiplication of two vectors (represented by the operator ⊗, and sometimes called the Hadamard product) is the vector of the same dimension as the two input vectors, where each element i is the product of element i in the two input vectors.",forget gate
AIMA,chapter6_5,"The only way we can possibly hope to deal with the vast real world is to decompose it into subproblems. Looking again at the constraint graph for Australia (Figure 6.1(b), repeated as Figure 6.12(a)), one fact stands out: Tasmania is not connected to the mainland.3 Intuitively, it is obvious that coloring Tasmania and coloring the mainland are independent subproblems—any solution for the mainland combined with any solution for Tasmania yields a solution for the whole map. Independence can be ascertained simply by finding connected components of the  constraint graph.","Independent subproblems, Connected component"
AIMA,chapter2_2,"Moral philosophy has developed several different notions of the “right thing, ” but AI has generally stuck to one notion called consequentialism: we evaluate an agent’s behavior by its consequences. When an agent is plunked down in an environment, it generates a sequence of actions according to the percepts it receives. This sequence of actions causes the environment to go through a sequence of states. If the sequence is desirable, then the agent has performed well. This notion of desirability is captured by a performance measure that evaluates any given sequence of environment states.","Consequentialism, Performance measure"
SLP,,"Number of documents can be enormous (think about all the pages on the web). Information retrieval (IR) is the task of finding the document d from the d documents in some collection that best matches a query g. For IR we’ll therefore also represent a query by a vector, also of length |v|, and we’ll need a way to compare two vectors to find how similar they are. (Doing IR will also require efficient ways to store and manipulate these vectors by making use of the convenient fact that these vectors are sparse. i.e.. mostly zeros).",information retrieval
SLP,slp_4,This idea of Bayesian inference has been known since the work of Bayes (1763) and was first applied to text classification by Mosteller and Wallace (1964). The intuition of Bayesian classification is to use Bayes’ rule to transform Eq. 4.1 into other probabilities that have some useful properties. Bayes’ rule is presented in Eg. 4.2; it gives us a way to break down any conditional probability p(x|y) into three other probabilities:,bayesian inference
AIMA,chapter6_2,"For large resource-limited problems with integer values—such as logistical problems involving moving thousands of people in hundreds of vehicles—it is usually not possible to represent the domain of each variable as a large set of integers and gradually reduce that set by consistency-checking methods. Instead, domains are represented by upper and lower bounds and are managed by bounds propagation.",bounds propagation
AIMA,chapter6_3,"When we reach a contradiction, backjumping can tell us how far to back up, so we don’t waste time changing variables that won’t fix the problem. But we would also like to avoid running into the same problem again. When the search arrives at a contradiction, we know that some subset of the conflict set is responsible for the problem. Constraint learning is the idea of finding a minimum set of variables from the conflict set that causes the problem. This set of variables, along with their corresponding values, is called a no-good. We then record the no-good, either by adding a new constraint to the CSP to forbid this combination of assignments or by keeping a separate cache of no-goods.","Constraint learning, No-good"
SLP,,"Semantic frames and roles closely related to semantic fields is the idea of a semantic frame. A semantic frame is a set of words that denote perspectives of participants in a particular type of event. A commercial transaction, for example, is a kind of event in which one entity trades money to another entity in return for some good or service, after which the good changes hands or perhaps the service is performed. This event can be encoded lexically by using verbs like buy (the event from the perspective of the buyer), sell (from the perspective of the seller), pay (focusing on the monetary aspect), or nouns like buyer. Frames have semantic roles (like buyer, seller, goods, money), and words in a sentence can take on these roles.","semantic frame, semantic frames and roles"
AIMA,chapter16_2,"ORDERABILITY: Given any two lotteries, a rational agent must either prefer one or else rate them as equally preferable. That is, the agent cannot avoid deciding. As noted on page 394, refusing to bet is like refusing to allow time to pass. TRANSITIVITY: Given any three lotteries, if an agent prefers A to B and prefers B to C, then the agent must prefer A to C. CONTINUITY: If some lottery is between and in preference, then there is some probability for which the rational agent will be indifferent between getting for sure and the lottery that yields with probability and with probability 1-p. SUBSTITUTABILITY: If an agent is indifferent between two lotteries A and B, then the agent is indifferent between two more complex lotteries that are the same except that B is substituted for A in one of them. This holds regardless of the probabilities and the other outcome(s) in the lotteries. MONOTONICITY: Suppose two lotteries have the same two possible outcomes, A and B. If an agent prefers A to B, then the agent must prefer the lottery that has a higher probability for A(and vice versa). DECOMPOSABILITY: Compound lotteries can be reduced to simpler ones using the laws of probability. This has been called the “no fun in gambling” rule: as Figure 16.1(b) shows, it compresses two consecutive lotteries into a single equivalent lottery.","Orderability, Transitivity, Continuity, Substitutability, Monotonicity, Decomposability"
AIMA,chapter22_2,"as noted in the introduction to this chapter, real-world environments may have very sparserewards: many primitive actions are required to achieve any nonzero reward. for example, soccer-playing robot might send a hundred thousand motor control commands to its varioujoints before conceding a goal. now it has to work out what it did wrong. the technical tertfor this is the credit assignment problem. other than playing trillions of soccer games sothat the negative reward eventually propagates back to the actions responsible for it, is ther",credit assignment
SLP,,"While the addition of gates allows LSTMs to handle more distant information than RNNs, they don’t completely solve the underlying problem: passing information through an extended series of recurrent connections leads to information loss and difficulties in training. Moreover, the inherently sequential nature of recurrent networks makes it hard to do computation in parallel. These considerations led to the development of transformers — an approach to sequence processing that eliminates recurrent connections and returns to architectures reminiscent of the fully connected networks described earlier in chapter 7.",transformers
AIMA,chapter2_4,"The simplest kind of agent is the simple reflex agent. These agents select actions on the basis of the current percept, ignoring the rest of the percept history. For example, the vacuum agent whose agent function is tabulated in Figure 2.3 is a simple reflex agent, because its decision is based only on the current location and on whether that location contains dirt. An agent program for this agent is shown in Figure 2.8 .",Simple reflex agent
SLP,,"An embedding representation for our input words—is called pretraining. Using pretrained embedding representations, whether simple static word embeddings like word2vec or the much more powerful contextual embeddings we’ll introduce in chapter 11, is one of the central ideas of deep learning. (It’s also possible, however, to train the word embeddings as part of an NLP task; we’ll talk about how to do this in section 7.7 in the context of the neural language modeling task.)",pretraining
AIMA,chapter7_3,"The property of completeness is also desirable: an inference algorithm is complete if it can derive any sentence that is entailed. For real haystacks, which are finite in extent, it seems obvious that a systematic examination can always decide whether the needle is in the haystack. For many knowledge bases, however, the haystack of consequences is infinite, and completeness becomes an important issue.6 Fortunately, there are complete inference procedures for logics that are sufficiently expressive to handle many knowledge bases.",Completeness
AIMA,chapter7_5,"So far, we have shown how to determine entailment by model checking: enumerating models and showing that the sentence must hold in all models. In this section, we show how entailment can be done by theorem proving—applying rules of inference directly to the sentences in our knowledge base to construct a proof of the desired sentence without consulting models. If the number of models is large but the length of the proof is short, then theorem proving can be more efficient than model checking.",Theorem proving
SLP,,"In this figure, the inputs at each time step are pre-trained word embeddings corresponding to the input tokens. The RNN block is an abstraction that represents an unrolled simple recurrent network consisting of an input layer, hidden layer, and output layer at each time step, as well as the shared u, v and w weight matrices that comprise the network. The outputs of the network at each time step represent the distribution over the POS tagset generated by a softmax layer.",embeddings
SLP,,"As the input layer, x, is a vector of simple scalar values just as we saw in Fig. 7.2, the core of the neural network is the hidden layer, h, formed of hidden units, h,, each of which is a neural unit as described in section 7.1, taking a weighted sum of its inputs and then applying a non-linearity. In the standard architecture, each layer is fully-connected, meaning that each unit in each layer takes as input the outputs from all the units in the previous layer, and there is a link between every pair of units from two adjacent layers. Thus each hidden unit sums over all the input units. Recall that a cinole hidden unit has a parameter, a weight vector, and a bias, w,a.",fully-connected
SLP,,"The idea of vector semantics is to represent a word as a point in a multidimensional semantic space that is derived (in ways we’ll see) from the distributions of word neighbors. Vectors for representing words are called embeddings (although the term is sometimes more strictly applied only to dense vectors like word2vec (section 6.8), rather than sparse tf-idf or ppmi vectors (section 6.3-section 6.6)). The word “embedding” derives from its mathematical sense as a mapping from one space or structure to another, although the meaning has shifted; see the end of the chapter.","vector semantics, embeddings, vector semantics"
SLP,,"In this section, we summarize some of these desiderata, drawing on results in the linguistic study of word meaning, which is called lexical semantics. We'll return to and expand on this list in chapter 18 and chapter 10.",lexical semantics
AIMA,chapter6_1,"It can be helpful to visualize a CSP as a constraint graph. The nodes of the graph correspond to variables of the problem, and an edge connects any two variables that participate in a constraint.",Constraint graph
SLP,,"Analogy/relational similarity: Another semantic property of embeddings is their ability to capture relational meanings. In an important early vector space model of cognition, Rumelhart and Abrahamson (1973) proposed the parallelogram model for solving simple analogy problems of the form A is to B as A* is to what?. In such problems, a system given a problem like apple:tree::grape:? , i.e., apple is to tree as grape is to, and must fill in the word vine. In the parallelogram model, illustrated in Fig. 6.15, the vector from the word apple to the word tree (= tree — apple) is added to the vector for grape (grapé); the nearest word to that point is returned. In early work with sparse embeddings, scholars showed that sparse vector mod-",parallelogram model
AIMA,chapter17_3,"An important generalization of the bandit process is the bandit superprocess or BSP, in which each arm is a full Markov decision process in its own right, rather than being a Markov reward process with only one possible action. All other properties remain the same: the arms are independent, only one (or a bounded number) can be worked on at a time, and there is a single discount factor.","Bandit superprocess, BSP"
AIMA,,"figure 12.1 sketches the structure of an agent that uses decision theory to select actions. the agent is identical, at an abstract level, to the agents described in chapters 419 and 70 that maintain a belief state reflecting the history of percepts to date. the primary difference is that the decision-theoretic agent's belief state represents not just the possibilities for world states but also their probabilities. given the belief state and some knowledge of the effects of actions, the agent can make probabilistic predictions of action outcomes and hence select the action with the highest expected utility.",decision theory
SLP,,"The dimensions used by vector models of meaning to define words, however, are only abstractly related to this idea of a small fixed number of hand-built dimensions. Nonetheless, there has been some attempt to show that certain dimensions of embedding models do contribute some specific compositional aspect of meaning like these early semantic features.",semantic feature
SLP,slp-7,"In machine learning, the amount to move in gradient descent is the value of the slope (alf(x;w), y) weighted by a learning rate n. A higher (faster) learning rate means that we should move w more on each step. The change we make in our parameter is the learning rate times the gradient (or the slope, in our single-variable example):",learning rate
SLP,,"A simplified sketch of a feedforward neural language model moving through a text is shown below. At each time step, the network converts n context words, each to a d-dimensional embedding, and concatenates the n embeddings together to get the nd x 1 unit input vector x for the network. The output of the network is a probability distribution over the vocabulary representing the model’s belief with respect to each word being the next possible word.","words, embeddings"
AIMA,chapter16_3,"If we want to build a decision-theoretic system that helps a human make decisions or acts on his or her behalf, we must first work out what the human’s utility function is. This process, often called preference elicitation, involves presenting choices to the human and using the observed preferences to pin down the underlying utility function.",Preference elicitation
SLP,,"First, we'll need a loss function that models the distance between the system output and the gold output. It's common to use the loss function used for logistic regression, the cross-entropy loss.",cross-entropy
AIMA,chapter17_3,"In Las Vegas, a one-armed bandit is a slot machine. A gambler can insert a coin, pull the lever, and collect the winnings (if any). An n-armed bandit has n levers. Behind each lever is a fixed but unknown probability distribution of winnings; each pull samples from that unknown distribution.",N-armed bandit
AIMA,chapter3_5,"A* search has many good qualities, but it expands a lot of nodes. We can explore fewer nodes (taking less time and space) if we are willing to accept solutions that are suboptimal, but are “good enough”—what we call satisficing solutions. If we allow A* search to use an inadmissible heuristic—one that may overestimate—then we risk missing the optimal solution, but the heuristic can potentially be more accurate, thereby reducing the number of nodes expanded. For example, road engineers know the concept of a detour index, which is a multiplier applied to the straight-line distance to account for the typical curvature of roads.","Inadmissible heuristic, "
AIMA,chapter5_1,"S0: The initial state, which specifies how the game is set up at the start. TO-MOVE(s): The player whose turn it is to move in state s. ACTIONS(s): The set of legal moves in state s. RESULT(s, a): The transition model, which defines the state resulting from taking action a in state s. IS-TERMINAL(s): A terminal test, which is true when the game is over and false otherwise. States where the game has ended are called terminal states. UTILITY(s, p): A utility function (also called an objective function or payoff function), which defines the final numeric value to player p when the game ends in terminal state s. In chess, the outcome is a win, loss, or draw, with values 1, 0, or 1/2. Some games have a wider range of possible outcomes—for example, the payoffs in backgammon range from 0 to 192.","Transition model, Terminal test, Terminal state, Terminal test, Terminal state"
SLP,,"What if we don't have enough data to make all three of these data sets? We can squeeze more out of the data using a technique called k-fold cross-validation. The idea is that each example serves double duty—as training data and validation data—but not at the same time.

First we split the data into k equal subsets. We then perform k rounds of learning; on each round 1/k of the data are held out as a validation set and the remaining examples are used as the training set. The average test set score of the k rounds should then be a better estimate than a single score. Popular values for k are 5 and 10—enough to give an estimate that is statistically likely to be accurate, at a cost of 5 to 10 times longer computation time. The extreme is k = n, also known as leave-one-out cross-validation or LOOCV. Even with cross-validation, we still need a separate test set.","K-fold cross-validation, LOOCV"
SLP,,"For decision trees, a technique called decision tree pruning combats overfitting. Pruning works by eliminating nodes that are not clearly relevant. We start with a full tree, as generated by learn-decision-tree. We then look at a test node that has only leaf nodes as descendants. If the test appears to be irrelevant—detecting only noise in the data—then we eliminate the test, replacing it with a leaf node. We repeat this process, considering each test with only leaf descendants, until each one has either been pruned or accepted as is.",decision tree pruning