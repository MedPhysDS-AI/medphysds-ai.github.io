Chapter,Text,Concepts
chapter1,"We call ourselves Homo sapiens—man the wise—because our intelligence is so important to us. For thousands of years, we have tried to understand how we think and act—that is, how our brain, a mere handful of matter, can perceive, understand, predict, and manipulate a world far larger and more complicated than itself. The field of artificial intelligence, or AI, is concerned with not just understanding but also building intelligent entities—machines that can compute how to act effectively and safely in a wide variety of novel situations.","Intelligence, Artificial Intelligence"
chapter1_1,"We have claimed that AI is interesting, but we have not said what it is. Historically, researchers have pursued several different versions of AI. Some have defined intelligence in terms of fidelity to human performance, while others prefer an abstract, formal definition of intelligence called rationality—loosely speaking, doing the “right thing.” The subject matter itself also varies: some consider intelligence to be a property of internal thought processes and reasoning, while others focus on intelligent behavior, an external characterization.1",Rationality
chapter1_1,"The Turing test, proposed by Alan Turing (1950), was designed as a thought experiment that would sidestep the philosophical vagueness of the question “Can a machine think?” A computer passes the test if a human interrogator, after posing some written questions, cannot tell whether the written responses come from a person or from a computer. Chapter 27 discusses the details of the test and whether a computer would really be intelligent if it passed. For now, we note that programming a computer to pass a rigorously applied test provides plenty to work on. The computer would need the following capabilities: natural language processing to communicate successfully in a human language; knowledge representation to store what it knows or hears; automated reasoning to answer questions and to draw new conclusions; machine learning to adapt to new circumstances and to detect and extrapolate patterns.","Turing test, Natural language processing, Knowledge representation, Automated reasoning, Machine learning, Total Turing test"
chapter1_1,"Turing viewed the physical simulation of a person as unnecessary to demonstrate intelligence. However, other researchers have proposed a total Turing test, which requires interaction with objects and people in the real world. To pass the total Turing test, a robot will need: computer vision and speech recognition to perceive the world; robotics to manipulate objects and move about.","Computer vision, Robotics"
chapter1_2,"To say that a program thinks like a human, we must know how humans think. We can learn about human thought in three ways: introspection—trying to catch our own thoughts as they go by; psychological experiments—observing a person in action; brain imaging—observing the brain in action.","Introspection, Psychological experiments, Brain imaging"
chapter1_2,"Once we have a sufficiently precise theory of the mind, it becomes possible to express the theory as a computer program. If the program’s input–output behavior matches corresponding human behavior, that is evidence that some of the program’s mechanisms could also be operating in humans. For example, Allen Newell and Herbert Simon, who developed GPS, the “General Problem Solver” (Newell and Simon 1961), were not content merely to have their program solve problems correctly. They were more concerned with comparing the sequence and timing of its reasoning steps to those of human subjects solving the same problems. The interdisciplinary field of cognitive science brings together computer models from AI and experimental techniques from psychology to construct precise and testable theories of the human mind.",Cognitive science
chapter1_3,"The Greek philosopher Aristotle was one of the first to attempt to codify “right thinking”— that is, irrefutable reasoning processes. His syllogisms provided patterns for argument structures that always yielded correct conclusions when given correct premises. The canonical example starts with Socrates is a man and all men are mortal and concludes that Socrates is mortal. (This example is probably due to Sextus Empiricus rather than Aristotle.) These laws of thought were supposed to govern the operation of the mind; their study initiated the field called logic.",Syllogisms
chapter1_3,"Logicians in the 19th century developed a precise notation for statements about objects in the world and the relations among them. (Contrast this with ordinary arithmetic notation, which provides only for statements about numbers.) By 1965, programs could, in principle, solve any solvable problem described in logical notation. The so-called logicist tradition within artificial intelligence hopes to build on such programs to create intelligent systems.",Logicist
chapter1_3,"Logic as conventionally understood requires knowledge of the world that is certain—a condition that, in reality, is seldom achieved. We simply don’t know the rules of, say, politics or warfare in the same way that we know the rules of chess or arithmetic. The theory of probability fills this gap, allowing rigorous reasoning with uncertain information. In principle, it allows the construction of a comprehensive model of rational thought, leading from raw perceptual information to an understanding of how the world works to predictions about the future. What it does not do, is generate intelligent behavior. For that, we need a theory of rational action. Rational thought, by itself, is not enough.",Probability
chapter1_4,"An agent is just something that acts (agent comes from the Latin agere, to do). Of course, all computer programs do something, but computer agents are expected to do more: operate autonomously, perceive their environment, persist over a prolonged time period, adapt to change, and create and pursue goals. A rational agent is one that acts so as to achieve the best outcome or, when there is uncertainty, the best expected outcome.","Agent, Rational agent"
chapter1_4,"In the “laws of thought” approach to AI, the emphasis was on correct inferences. Making correct inferences is sometimes part of being a rational agent, because one way to act rationally is to deduce that a given action is best and then to act on that conclusion. On the other hand, there are ways of acting rationally that cannot be said to involve inference. For example, recoiling from a hot stove is a reflex action that is usually more successful than a slower action taken after careful deliberation. All the skills needed for the Turing test also allow an agent to act rationally. Knowledge representation and reasoning enable agents to reach good decisions. We need to be able to generate comprehensible sentences in natural language to get by in a complex society. We need learning not only for erudition, but also because it improves our ability to generate effective behavior, especially in circumstances that are new. The rational-agent approach to AI has two advantages over the other approaches. First, it is more general than the “laws of thought” approach because correct inference is just one of several possible mechanisms for achieving rationality. Second, it is more amenable to scientific development. The standard of rationality is mathematically well defined and completely general. We can often work back from this specification to derive agent designs that provably achieve it—something that is largely impossible if the goal is to imitate human behavior or thought processes. For these reasons, the rational-agent approach to AI has prevailed throughout most of the field’s history. In the early decades, rational agents were built on logical foundations and formed definite plans to achieve specific goals. Later, methods based on probability theory and machine learning allowed the creation of agents that could make decisions under uncertainty to attain the best expected outcome. In a nutshell, AI has focused on the study and construction of agents that do the right thing. What counts as the right thing is defined by the objective that we provide to the agent. This general paradigm is so pervasive that we might call it the standard model. It prevails not only in AI, but also in control theory, where a controller minimizes a cost function; in operations research, where a policy maximizes a sum of rewards; in statistics, where a decision rule minimizes a loss function; and in economics, where a decision maker maximizes utility or some measure of social welfare.","Do the right thing, Standard model"
chapter1_4,"We need to make one important refinement to the standard model to account for the fact that perfect rationality—always taking the exactly optimal action—is not feasible in complex environments. The computational demands are just too high. Chapters 5 and 17 deal with the issue of limited rationality—acting appropriately when there is not enough time to do all the computations one might like. However, perfect rationality often remains a good starting point for theoretical analysis.",Limited rationality
chapter1_5,"The problem of achieving agreement between our true preferences and the objective we put into the machine is called the value alignment problem: the values or objectives put into the machine must be aligned with those of the human. If we are developing an AI system in the lab or in a simulator—as has been the case for most of the field’s history—there is an easy fix for an incorrectly specified objective: reset the system, fix the objective, and try again. As the field progresses towards increasingly capable intelligent systems that are deployed in the real world, this approach is no longer viable. A system deployed with an incorrect objective will have negative consequences. Moreover, the more intelligent the system, the more negative the consequences.",Value alignment problem
chapter1_5,"It is impossible to anticipate all the ways in which a machine pursuing a fixed objective might misbehave. There is good reason, then, to think that the standard model is inadequate. We don’t want machines that are intelligent in the sense of pursuing their objectives; we want them to pursue our objectives. If we cannot transfer those objectives perfectly to the machine, then we need a new formulation—one in which the machine is pursuing our objectives, but is necessarily uncertain as to what they are. When a machine knows that it doesn’t know the complete objective, it has an incentive to act cautiously, to ask permission, to learn more about our preferences through observation, and to defer to human control. Ultimately, we want agents that are provably beneficial to humans. We will return to this topic in Section 1.5 .",Provably beneficial
chapter2_1,"An agent is anything that can be viewed as perceiving its environment through sensors and acting upon that environment through actuators. This simple idea is illustrated in Figure 2.1 . A human agent has eyes, ears, and other organs for sensors and hands, legs, vocal tract, and so on for actuators. A robotic agent might have cameras and infrared range finders for sensors and various motors for actuators. A software agent receives file contents, network packets, and human input (keyboard/mouse/touchscreen/voice) as sensory inputs and acts on the environment by writing files, sending network packets, and displaying information or generating sounds. The environment could be everything—the entire universe! In practice it is just that part of the universe whose state we care about when designing this agent—the part that affects what the agent perceives and that is affected by the agent’s actions.","Environment, Sensor, Actuator"
chapter2_1,"We use the term percept to refer to the content an agent’s sensors are perceiving. An agent’s percept sequence is the complete history of everything the agent has ever perceived. In general, an agent’s choice of action at any given instant can depend on its built-in knowledge and on the entire percept sequence observed to date, but not on anything it hasn’t perceived. By specifying the agent’s choice of action for every possible percept sequence, we have said more or less everything there is to say about the agent. Mathematically speaking, we say that an agent’s behavior is described by the agent function that maps any given percept sequence to an action.","Percept, Percept sequence, Agent function"
chapter2_1,"We can imagine tabulating the agent function that describes any given agent; for most agents, this would be a very large table—infinite, in fact, unless we place a bound on the length of percept sequences we want to consider. Given an agent to experiment with, we can, in principle, construct this table by trying out all possible percept sequences and recording which actions the agent does in response.1 The table is, of course, an external characterization of the agent. Internally, the agent function for an artificial agent will be implemented by an agent program. It is important to keep these two ideas distinct. The agent function is an abstract mathematical description; the agent program is a concrete implementation, running within some physical system.",Agent program
chapter2_2,"A rational agent is one that does the right thing. Obviously, doing the right thing is better than doing the wrong thing, but what does it mean to do the right thing?",Rational agent
chapter2_2,"Moral philosophy has developed several different notions of the “right thing, ” but AI has generally stuck to one notion called consequentialism: we evaluate an agent’s behavior by its consequences. When an agent is plunked down in an environment, it generates a sequence of actions according to the percepts it receives. This sequence of actions causes the environment to go through a sequence of states. If the sequence is desirable, then the agent has performed well. This notion of desirability is captured by a performance measure that evaluates any given sequence of environment states.","Consequentialism, Performance measure"
chapter2_2,"What is rational at any given time depends on four things: The performance measure that defines the criterion of success. The agent’s prior knowledge of the environment. The actions that the agent can perform. The agent’s percept sequence to date. This leads to a definition of a rational agent: For each possible percept sequence, a rational agent should select an action that is expected to maximize its performance measure, given the evidence provided by the percept sequence and whatever built-in knowledge the agent has.",Definition of a rational agent
chapter2_2,"We need to be careful to distinguish between rationality and omniscience. An omniscient agent knows the actual outcome of its actions and can act accordingly; but omniscience is impossible in reality. Consider the following example: I am walking along the Champs Elysées one day and I see an old friend across the street. There is no traffic nearby and I’m not otherwise engaged, so, being rational, I start to cross the street. Meanwhile, at 33, 000 feet, a cargo door falls off a passing airliner, 3 and before I make it to the other side of the street I am flattened. Was I irrational to cross the street? It is unlikely that my obituary would read “Idiot attempts to cross street.” This example shows that rationality is not the same as perfection. Rationality maximizes expected performance, while perfection maximizes actual performance. Retreating from a requirement of perfection is not just a question of being fair to agents. The point is that if we expect an agent to do what turns out after the fact to be the best action, it will be impossible to design an agent to fulfill this specification—unless we improve the performance of crystal balls or time machines.",Omniscience
chapter2_2,"Our definition of rationality does not require omniscience, then, because the rational choice depends only on the percept sequence to date. We must also ensure that we haven’t inadvertently allowed the agent to engage in decidedly underintelligent activities. For example, if an agent does not look both ways before crossing a busy road, then its percept sequence will not tell it that there is a large truck approaching at high speed. Does our definition of rationality say that it’s now OK to cross the road? Far from it! First, it would not be rational to cross the road given this uninformative percept sequence: the risk of accident from crossing without looking is too great. Second, a rational agent should choose the “looking” action before stepping into the street, because looking helps maximize the expected performance. Doing actions in order to modify future percepts— sometimes called information gathering—is an important part of rationality and is covered in depth in Chapter 16 . A second example of information gathering is provided by the exploration that must be undertaken by a vacuum-cleaning agent in an initially unknown environment. Our definition requires a rational agent not only to gather information but also to learn as much as possible from what it perceives. The agent’s initial configuration could reflect some prior knowledge of the environment, but as the agent gains experience this may be modified and augmented. There are extreme cases in which the environment is completely known a priori and completely predictable. In such cases, the agent need not perceive or learn; it simply acts correctly.","Information gathering, Learning"
chapter2_2,"Slightly more intelligent is the sphex wasp. The female sphex will dig a burrow, go out and sting a caterpillar and drag it to the burrow, enter the burrow again to check all is well, drag the caterpillar inside, and lay its eggs. The caterpillar serves as a food source when the eggs hatch. So far so good, but if an entomologist moves the caterpillar a few inches away while the sphex is doing the check, it will revert to the “drag the caterpillar” step of its plan and will continue the plan without modification, re-checking the burrow, even after dozens of caterpillar-moving interventions. The sphex is unable to learn that its innate plan is failing, and thus will not change it. To the extent that an agent relies on the prior knowledge of its designer rather than on its own percepts and learning processes, we say that the agent lacks autonomy. A rational agent should be autonomous—it should learn what it can to compensate for partial or incorrect prior knowledge. For example, a vacuum-cleaning agent that learns to predict where and when additional dirt will appear will do better than one that does not.",Autonomy
chapter2_3,"Now that we have a definition of rationality, we are almost ready to think about building rational agents. First, however, we must think about task environments, which are essentially the “problems” to which rational agents are the “solutions.” We begin by showing how to specify a task environment, illustrating the process with a number of examples. We then show that task environments come in a variety of flavors. The nature of the task environment directly affects the appropriate design for the agent program. In our discussion of the rationality of the simple vacuum-cleaner agent, we had to specify the performance measure, the environment, and the agent’s actuators and sensors. We group all these under the heading of the task environment. For the acronymically minded, we call this the PEAS (Performance, Environment, Actuators, Sensors) description. In designing an agent, the first step must always be to specify the task environment as fully as possible.","Task environment, PEAS"
chapter2_3,"FULLY OBSERVABLE VS. PARTIALLY OBSERVABLE: If an agent’s sensors give it access to the complete state of the environment at each point in time, then we say that the task environment is fully observable. A task environment is effectively fully observable if the sensors detect all aspects that are relevant to the choice of action; relevance, in turn, depends on the performance measure. Fully observable environments are convenient because the agent need not maintain any internal state to keep track of the world. An environment might be partially observable because of noisy and inaccurate sensors or because parts of the state are simply missing from the sensor data—for example, a vacuum agent with only a local dirt sensor cannot tell whether there is dirt in other squares, and an automated taxi cannot see what other drivers are thinking. If the agent has no sensors at all then the environment is unobservable. One might think that in such cases the agent’s plight is hopeless, but, as we discuss in Chapter 4 , the agent’s goals may still be achievable, sometimes with certainty.","Fully observable, Partially observable, Unobservable"
chapter2_3,"SINGLE-AGENT VS. MULTIAGENT: The distinction between single-agent and multiagent environments may seem simple enough. For example, an agent solving a crossword puzzle by itself is clearly in a single-agent environment, whereas an agent playing chess is in a two- agent environment. However, there are some subtle issues. First, we have described how an entity may be viewed as an agent, but we have not explained which entities must be viewed as agents. Does an agent (the taxi driver for example) have to treat an object (another vehicle) as an agent, or can it be treated merely as an object behaving according to the laws of physics, analogous to waves at the beach or leaves blowing in the wind? The key distinction is whether ’s behavior is best described as maximizing a performance measure whose value depends on agent ’s behavior.","Single-agent, Multiagent"
chapter2_3,"For example, in chess, the opponent entity is trying to maximize its performance measure, which, by the rules of chess, minimizes agent ’s performance measure. Thus, chess is a competitive multiagent environment. On the other hand, in the taxi-driving environment, avoiding collisions maximizes the performance measure of all agents, so it is a partially cooperative multiagent environment. It is also partially competitive because, for example, only one car can occupy a parking space.","Competitive, Cooperative"
chapter2_3,"The agent-design problems in multiagent environments are often quite different from those in single-agent environments; for example, communication often emerges as a rational behavior in multiagent environments; in some competitive environments, randomized behavior is rational because it avoids the pitfalls of predictability. Deterministic vs. nondeterministic. If the next state of the environment is completely determined by the current state and the action executed by the agent(s), then we say the environment is deterministic; otherwise, it is nondeterministic. In principle, an agent need not worry about uncertainty in a fully observable, deterministic environment. If the environment is partially observable, however, then it could appear to be nondeterministic.","Deterministic, Nondeterministic"
chapter2_3,"One final note: the word stochastic is used by some as a synonym for “nondeterministic, ” but we make a distinction between the two terms; we say that a model of the environment is stochastic if it explicitly deals with probabilities (e.g., “there’s a 25% chance of rain tomorrow”) and “nondeterministic” if the possibilities are listed without being quantified (e.g., “there’s a chance of rain tomorrow”).",Stochastic
chapter2_3,"EPISODIC VS. SEQUENTIAL: In an episodic task environment, the agent’s experience is divided into atomic episodes. In each episode the agent receives a percept and then performs a single action. Crucially, the next episode does not depend on the actions taken in previous episodes. Many classification tasks are episodic. For example, an agent that has to spot defective parts on an assembly line bases each decision on the current part, regardless of previous decisions; moreover, the current decision doesn’t affect whether the next part is defective. In sequential environments, on the other hand, the current decision could affect all future decisions.4 Chess and taxi driving are sequential: in both cases, short-term actions can have long-term consequences. Episodic environments are much simpler than sequential environments because the agent does not need to think ahead.","Episodic, Sequential, Static, Dynamic"
chapter2_3,"STATIC VS. DYNAMIC: If the environment can change while an agent is deliberating, then we say the environment is dynamic for that agent; otherwise, it is static. Static environments are easy to deal with because the agent need not keep looking at the world while it is deciding on an action, nor need it worry about the passage of time. Dynamic environments, on the other hand, are continuously asking the agent what it wants to do; if it hasn’t decided yet, that counts as deciding to do nothing. If the environment itself does not change with the passage of time but the agent’s performance score does, then we say the environment is semidynamic. Taxi driving is clearly dynamic: the other cars and the taxi itself keep moving while the driving algorithm dithers about what to do next. Chess, when played with a clock, is semidynamic. Crossword puzzles are static.",Semidynamic
chapter2_3,"DISCRETE VS. CONTINUOUS: The discrete/continuous distinction applies to the state of the environment, to the way time is handled, and to the percepts and actions of the agent. For example, the chess environment has a finite number of distinct states (excluding the clock). Chess also has a discrete set of percepts and actions. Taxi driving is a continuous-state and continuous-time problem: the speed and location of the taxi and of the other vehicles sweep through a range of continuous values and do so smoothly over time. Taxi-driving actions are also continuous (steering angles, etc.). Input from digital cameras is discrete, strictly speaking, but is typically treated as representing continuously varying intensities and locations.","Discrete, Continuous"
chapter2_3,"KNOWN VS. UNKNOWN: Strictly speaking, this distinction refers not to the environment itself but to the agent’s (or designer’s) state of knowledge about the “laws of physics” of the environment. In a known environment, the outcomes (or outcome probabilities if the environment is nondeterministic) for all actions are given. Obviously, if the environment is unknown, the agent will have to learn how it works in order to make good decisions.","Known, Unknown"
chapter2_3,"The code repository associated with this book (aima.cs.berkeley.edu) includes multiple environment implementations, together with a general-purpose environment simulator for evaluating an agent’s performance. Experiments are often carried out not for a single environment but for many environments drawn from an environment class. For example, to evaluate a taxi driver in simulated traffic, we would want to run many simulations with different traffic, lighting, and weather conditions. We are then interested in the agent’s average performance over the environment class.",Environment class
chapter2_4,So far we have talked about agents by describing behavior—the action that is performed after any given sequence of percepts. Now we must bite the bullet and talk about how the insides work. The job of AI is to design an agent program that implements the agent function—the mapping from percepts to actions. We assume this program will run on some sort of computing device with physical sensors and actuators—we call this the agent architecture: agent = architecture + program,"Agent program, Agent architecture"
chapter2_4,"The simplest kind of agent is the simple reflex agent. These agents select actions on the basis of the current percept, ignoring the rest of the percept history. For example, the vacuum agent whose agent function is tabulated in Figure 2.3 is a simple reflex agent, because its decision is based only on the current location and on whether that location contains dirt. An agent program for this agent is shown in Figure 2.8 .",Simple reflex agent
chapter3_1,"Control theorists call this an open-loop system: ignoring the percepts breaks the loop between agent and environment. If there is a chance that the model is incorrect, or the environment is nondeterministic, then the agent would be safer using a closed-loop approach that monitors the percepts","open-loop, closed-loop"
chapter3_2,"A standardized problem is intended to illustrate or exercise various problem-solving methods. It can be given a concise, exact description and hence is suitable as a benchmark for researchers to compare the performance of algorithms. A real-world problem, such as robot navigation, is one whose solutions people actually use, and whose formulation is idiosyncratic, not standardized, because, for example, each robot has different sensors that produce different data.","real-world problem, standardized problem"
chapter3_2,"Touring problems describe a set of locations that must be visited, rather than a single goal destination. The traveling salesperson problem (TSP) is a touring problem in which every city on a map must be visited. The aim is to find a tour with cost < C (or in the optimization version, to find a tour with the lowest cost possible). An enormous amount of effort has been expended to improve the capabilities of TSP algorithms.","Touring problems, TSP"
chapter3_2,"A VLSI layout problem requires positioning millions of components and connections on a chip to minimize area, minimize circuit delays, minimize stray capacitances, and maximize manufacturing yield.",VLSI layout
chapter3_2,"Robot navigation is a generalization of the route-finding problem described earlier. Rather than following distinct paths (such as the roads in Romania), a robot can roam around, in effect making its own paths. For a circular robot moving on a flat surface, the space is essentially two-dimensional. When the robot has arms and legs that must also be controlled, the search space becomes many-dimensional—one dimension for each joint angle. Advanced techniques are required just to make the essentially continuous search space finite. In addition to the complexity of the problem, real robots must also deal with errors in their sensor readings and motor controls, with partial observability, and with other agents that might alter the environment.",Robot navigation
chapter3_2,"Automatic assembly sequencing of complex objects (such as electric motors) by a robot has been standard industry practice since the 1970s. Algorithms first find a feasible assembly sequence and then work to optimize the process. Minimizing the amount of manual human labor on the assembly line can produce significant savings in time and cost. In assembly problems, the aim is to find an order in which to assemble the parts of some object. If the wrong order is chosen, there will be no way to add some part later in the sequence without undoing some of the work already done. Checking an action in the sequence for feasibility is a difficult geometrical search problem closely related to robot navigation. Thus, the generation of legal actions is the expensive part of assembly sequencing. Any practical algorithm must avoid exploring all but a tiny fraction of the state space. One important assembly problem is protein design, in which the goal is to find a sequence of amino acids that will fold into a three-dimensional protein with the right properties to cure some disease.","Automatic assembly sequencing, Protein design"
chapter3_3,"A search algorithm takes a search problem as input and returns a solution, or an indication of failure. In this chapter we consider algorithms that superimpose a search tree over the state-space graph, forming various paths from the initial state, trying to find a path that reaches a goal state. Each node in the search tree corresponds to a state in the state space and the edges in the search tree correspond to actions. The root of the tree corresponds to the initial state of the problem.","search algorithm, node"
chapter3_3,"It is important to understand the distinction between the state space and the search tree. The state space describes the (possibly infinite) set of states in the world, and the actions that allow transitions from one state to another. The search tree describes paths between these states, reaching towards the goal. The search tree may have multiple paths to (and thus multiple nodes for) any given state, but each node in the tree has a unique path back to the root (as in all trees).The rootnode of the search tree is at the initial state, Arad. We can expand the node, by considering the available ACTIONS for that state, using the RESULT function to see where those actions lead to, and generating a new node (called a child node or successor node) for each of the resulting states. Each child node has Arad as its parent node.","Generating, Child node, Successor node, Parent node"
chapter3_3,Now we must choose which of these three child nodes to consider next. This is the essence of search—following up one option now and putting the others aside for later. Suppose we choose to expand Sibiu first. Figure shows the result: a set of 6 unexpanded nodes (outlined in bold). We call this the frontier of the search tree. We say that any state that has had a node generated for it has been reached (whether or not that node has been expanded),"Frontier, Reached"
chapter3_3,"A very general approach is called best-first search, in which we choose a node, n, with minimum value of some evaluation function, f(n). On each iteration we choose a node on the frontier with minimum f(n) value, return it if its state is a goal state, and otherwise apply EXPAND to generate child nodes. Each child node is added to the frontier if it has not been reached before, or is re-added if it is now being reached with a path that has a lower path cost than any previous path. The algorithm returns either an indication of failure, or a node that represents a path to a goal. By employing different f(n) functions, we get different specific algorithms, which this chapter will cover.","Best-first search, Evaluation function"
chapter3_3,"A priority queue first pops the node with the minimum cost according to some evaluation function, f. It is used in best-first search.",priority queue
chapter3_3,"To be complete, a search algorithm must be systematic in the way it explores an infinite state space, making sure it can eventually reach any state that is connected to the initial state.",Systematic
chapter3_4,"When all actions have the same cost, an appropriate strategy is breadth-first search, in which the root node is expanded first, then all the successors of the root node are expanded next, then their successors, and so on. This is a systematic search strategy that is therefore complete even on infinite state spaces.",Breadth-first search
chapter3_4,Depth-first search always expands the deepest node in the frontier first. It could be implemented as a call to BEST-FIRST-SEARCH where the evaluation function f is the negative of the depth.,Depth-first search
chapter3_4,"A variant of depth-first search called backtracking search uses even less memory. In backtracking, only one successor is generated at a time rather than all successors; each partially expanded node remembers which successor to generate next. In addition, successors are generated by modifying the current state description directly rather than allocating memory for a brand-new state. This reduces the memory requirements to just one state description and a path of O(m) actions; a significant savings over O(bm) states for depth-first search. With backtracking we also have the option of maintaining an efficient set data structure for the states on the current path, allowing us to check for a cyclic path in O(1) time rather than O(m). For backtracking to work, we must be able to undo each action when we backtrack. Backtracking is critical to the success of many problems with large state descriptions, such as robotic assembly.",Backtracking search
chapter3_4,"Iterative deepening search solves the problem of picking a good value for ℓ by trying all values: first 0, then 1, then 2, and so on—until either a solution is found, or the depth-limited search returns the failure value rather than the cutoff value. Iterative deepening combines many of the benefits of depth-first and breadth-first search. Like depth-first search, its memory requirements are modest: O(bd) when there is a solution, or O(bm) on finite state spaces with no solution. Like breadth-first search, iterative deepening is optimal for problems where all actions have the same cost, and is complete on finite acyclic state spaces, or on any finite state space when we check nodes for cycles all the way up the path.",Iterative deepening search
chapter3_4,"The algorithms we have covered so far start at an initial state and can reach any one of multiple possible goal states. An alternative approach called bidirectional search simultaneously searches forward from the initial state and backwards from the goal state(s), hoping that the two searches will meet.",Bidirectional search
chapter3_4,"To keep depth-first search from wandering down an infinite path, we can use depth-limited search, a version of depth-first search in which we supply a depth limit, ℓ, and treat all nodes at depth ℓ as if they had no successors. Unfortunately, if we make a poor choice for ℓ the algorithm will fail to reach the solution, making it incomplete again.",depth-limited search
chapter3_5,"This section shows how an informed search strategy—one that uses domain-specific hints about the location of goals—can find solutions more efficiently than an uninformed strategy. The hints come in the form of a heuristic function, denoted h(n): h(n) = estimated cost of the cheapest path from the state at node n to a goal state.","Informed search, Heuristic function"
chapter3_5,Greedy best-first search is a form of best-first search that expands first the node with the lowest h(n) value—the node that appears to be closest to the goal—on the grounds that this is likely to lead to a solution quickly. So the evaluation function f(n) = h(n).,Greedy best-first search
chapter3_5,"The most common informed search algorithm is A* search (pronounced “A-star search”), a best-first search that uses the evaluation function f(n) = g(n) + h(n); where g(n) is the path cost from the initial state to node n, and h(n) is the estimated cost of the shortest path from n to a goal state, so we have f(n) = estimated cost of the best path that continues from n to a goal.",A* search
chapter3_5,"A* search is complete. Whether A* is cost-optimal depends on certain properties of the heuristic. A key property is admissibility: an admissible heuristic is one that never overestimates the cost to reach a goal. (An admissible heuristic is therefore optimistic.) With an admissible heuristic, A* is cost-optimal, which we can show with a proof by contradiction. Suppose the optimal path has cost but the algorithm returns a path with cost Then there must be some node which is on the optimal path and is unexpanded (because if all the nodes on the optimal path had been expanded, then we would have returned that optimal solution).",Admissible heuristic
chapter3_5,"There are a variety of suboptimal search algorithms, which can be characterized by the criteria for what counts as “good enough.” In bounded suboptimal search, we look for a solution that is guaranteed to be within a constant factor of the optimal cost. Weighted A* provides this guarantee. In bounded-cost search, we look for a solution whose cost is less than some constant And in unbounded-cost search, we accept a solution of any cost, as long as we can find it quickly.","Bounded suboptimal search, Bounded-cost search, Unbounded-cost search"
chapter3_5,"A* search has many good qualities, but it expands a lot of nodes. We can explore fewer nodes (taking less time and space) if we are willing to accept solutions that are suboptimal, but are “good enough”—what we call satisficing solutions. If we allow A* search to use an inadmissible heuristic—one that may overestimate—then we risk missing the optimal solution, but the heuristic can potentially be more accurate, thereby reducing the number of nodes expanded. For example, road engineers know the concept of a detour index, which is a multiplier applied to the straight-line distance to account for the typical curvature of roads.","Inadmissible heuristic, "
chapter3_5,"There are a variety of suboptimal search algorithms, which can be characterized by the criteria for what counts as “good enough.” In bounded suboptimal search, we look for a solution that is guaranteed to be within a constant factor of the optimal cost. Weighted A* provides this guarantee. In bounded-cost search, we look for a solution whose cost is less than some constant And in unbounded-cost search, we accept a solution of any cost, as long as we can find it quickly.","Bounded suboptimal search, Bounded-cost search, Unbounded-cost search"
chapter3_5,"Iterative-deepening A* search (IDA*) is to A* what iterative-deepening search is to depth-first: IDA* gives us the benefits of A* without the requirement to keep all reached states in memory, at a cost of visiting some states multiple times. It is a very important and commonly used algorithm for problems that do not fit in memory.",Iterative-deepening A* search
chapter3_5,"Recursive best-first search (RBFS) attempts to mimic the operation of standard best-first search, but using only linear space. RBFS resembles a recursive depth-first search, but rather than continuing indefinitely down the current path, it uses the f_limit variable to keep track of the f-value of the best alternative path available from any ancestor of the current node. If the current node exceeds this limit, the recursion unwinds back to the alternative path. As the recursion unwinds, RBFS replaces the f-value of each node along the path with a backed-up value—the best f-value of its children. In this way, RBFS remembers the f-value of the best leaf in the forgotten subtree and can therefore decide whether it’s worth reexpanding the subtree at some later time.","Recursive best-first search, backed-up value"
chapter3_6,"One might ask whether h2 is always better than h1. The answer is “Essentially, yes.” It is easy to see from the definitions of the two heuristics that for any node n, h2(n) ≥ h1(n). We thus say that h2 dominates h1. Domination translates directly into efficiency: A* using h2 will never expand more nodes than A* using h1 (except in the case of breaking ties unluckily). The argument is simple. Recall the observation on page 90 that every node with f(n) < C∗ will surely be expanded. This is the same as saying that every node with h(n) < C∗ − g(n) is surely expanded when h is consistent. But because h2 is at least as big as h1 for all nodes, every node that is surely expanded by A* search with h2 is also surely expanded with h1, and h1 might cause other nodes to be expanded as well. Hence, it is generally better to use a heuristic function with higher values, provided it is consistent and that the computation time for the heuristic is not too long.",Domination
chapter3_6,"A problem with fewer restrictions on the actions is called a relaxed problem. The state-space graph of the relaxed problem is a supergraph of the original state space because the removal of restrictions creates added edges in the graph. Because the relaxed problem adds edges to the state-space graph, any optimal solution in the original problem is, by definition, also a solution in the relaxed problem; but the relaxed problem may have better solutions if the added edges provide shortcuts. Hence, the cost of an optimal solution to a relaxed problem is an admissible heuristic for the original problem. Furthermore, because the derived heuristic is an exact cost for the relaxed problem, it must obey the triangle inequality and is therefore consistent",Relaxed problem
chapter3_6,"Some route-finding algorithms save even more time by adding shortcuts—artificial edges in the graph that define an optimal multi-action path. For example, if there were shortcuts predefined between all the 100 biggest cities in the U.S., and we were trying to navigate from the Berkeley campus in California to NYU in New York, we could take the shortcut between Sacramento and Manhattan and cover 90% of the path in one action.",Shortcuts
chapter3_6,"We have presented several fixed search strategies—breadth-first, A*, and so on—that have been carefully designed and programmed by computer scientists. Could an agent learn how to search better? The answer is yes, and the method rests on an important concept called the metalevel state space. Each state in a metalevel state space captures the internal (computational) state of a program that is searching in an ordinary state space such as the map of Romania. (To keep the two concepts separate, we call the map of Romania an object-level state space.) For example, the internal state of the A* algorithm consists of the current search tree. Each action in the metalevel state space is a computation step that alters the internal state; for example, each computation step in A* expands a leaf node and adds its successors to the tree. For harder problems, there will be many such missteps, and a metalevel learning algorithm can learn from these experiences to avoid exploring unpromising subtrees. The techniques used for this kind of learning are described in Chapter 22. The goal of learning is to minimize the total cost of problem solving, trading off computational expense and path cost.","Metalevel state space, Metalevel learning, Object-level state space"
chapter5_1,"There are at least three stances we can take towards multi-agent environments. The first stance, appropriate when there are a very large number of agents, is to consider them in the aggregate as an economy, allowing us to do things like predict that increasing demand will cause prices to rise, without having to predict the action of any individual agent.",Economy
chapter5_1,"We show that pruning makes the search more efficient by ignoring portions of the search tree that make no difference to the optimal move. For nontrivial games, we will usually not have enough time to be sure of finding the optimal move (even with pruning); we will have to cut off the search at some point.",Pruning
chapter5_1,"The games most commonly studied within AI (such as chess and Go) are what game theorists call deterministic, two-player, turn-taking, perfect information, zero-sum games. “Perfect information” is a synonym for “fully observable, ”1 and “zero-sum” means that what is good for one player is just as bad for the other: there is no “win-win” outcome. For games we often use the term move as a synonym for “action” and position as a synonym for “state.”","Perfect information, Zero-sum games, Position, Move"
chapter5_1,"S0: The initial state, which specifies how the game is set up at the start. TO-MOVE(s): The player whose turn it is to move in state s. ACTIONS(s): The set of legal moves in state s. RESULT(s, a): The transition model, which defines the state resulting from taking action a in state s. IS-TERMINAL(s): A terminal test, which is true when the game is over and false otherwise. States where the game has ended are called terminal states. UTILITY(s, p): A utility function (also called an objective function or payoff function), which defines the final numeric value to player p when the game ends in terminal state s. In chess, the outcome is a win, loss, or draw, with values 1, 0, or 1/2. Some games have a wider range of possible outcomes—for example, the payoffs in backgammon range from 0 to 192.","Transition model, Terminal test, Terminal state, Terminal test, Terminal state"
chapter5_2,"MAX wants to find a sequence of actions leading to a win, but MIN has something to say about it. This means that MAX’s strategy must be a conditional plan—a contingent strategy specifying a response to each of MIN’s possible moves. In games that have a binary outcome (win or lose), we could use AND–OR search (page 125) to generate the conditional plan. In fact, for such games, the definition of a winning strategy for the game is identical to the definition of a solution for a nondeterministic planning problem: in both cases the desirable outcome must be guaranteed no matter what the “other side” does. For games with multiple outcome scores, we need a slightly more general algorithm called minimax search.",Minimax search
chapter5_2,"The possible moves for MAX at the root node are labeled a1, a2, and a3. The possible replies to a1 for MIN are b1, b2, b3, and so on. This particular game ends after one move each by MAX and MIN. (NOTE: In some games, the word “move” means that both players have taken an action; therefore the word ply is used to unambiguously mean one move by one player, bringing us one level deeper in the game tree.) The utilities of the terminal states in this game range from 2 to 14.",ply
chapter5_2,"Given a game tree, the optimal strategy can be determined by working out the minimax value of each state in the tree, which we write as MINIMAX(s). The minimax value is the utility (for MAX) of being in that state, assuming that both players play optimally from there to the end of the game. The minimax value of a terminal state is just its utility. In a non-terminal state, MAX prefers to move to a state of maximum value when it is MAX’s turn to move, and MIN prefers a state of minimum value (that is, minimum value for MAX and thus maximum value for MIN).",Minimax value
chapter5_2,"The number of game states is exponential in the depth of the tree. No algorithm can completely eliminate the exponent, but we can sometimes cut it in half, computing the correct minimax decision without examining every state by pruning (see page 90) large parts of the tree that make no difference to the outcome. The particular technique we examine is called alpha–beta pruning",Alpha–beta pruning
chapter5_2,"In Section 3.3.3, we noted that redundant paths to repeated states can cause an exponential increase in search cost, and that keeping a table of previously reached states can address this problem. In game tree search, repeated states can occur because of transpositions—different permutations of the move sequence that end up in the same position, and the problem can be addressed with a transposition table that caches the heuristic value of states.","Transposition, Transposition table"
chapter5_3,"To make use of our limited computation time, we can cut off the search early and apply a heuristic evaluation function to states, effectively treating nonterminal nodes as if they were terminal. In other words, we replace the UTILITY function with EVAL, which estimates a state’s utility. We also replace the terminal test by a cutoff test, which must return true for terminal states, but is otherwise free to decide when to cut off the search, based on the search depth and any property of the state that it chooses to consider.",cutoff test
,"Let us make this idea more concrete. Most evaluation functions work by calculating various features of the state—for example, in chess, we would have features for the number of white pawns, black pawns, white queens, black queens, and so on. The features, taken together, define various categories or equivalence classes of states: the states in each category have the same values for all the features. For example, one category might contain all two-pawn versus one-pawn endgames. Any given category will contain some states that lead (with perfect play) to wins, some that lead to draws, and some that lead to losses.",Features
chapter5_3,"The evaluation function should be applied only to positions that are quiescent—that is, positions in which there is no pending move (such as a capturing the queen) that would wildly swing the evaluation. For nonquiescent positions the IS-CUTOFF returns false, and the search continues until quiescent positions are reached. This extra quiescence search is sometimes restricted to consider only certain types of moves, such as capture moves, that will quickly resolve the uncertainties in the position.","Quiescence, Quiescence search"
chapter5_3,"One strategy to mitigate the horizon effect is to allow singular extensions, moves that are “clearly better” than all other moves in a given position, even when the search would normally be cut off at that point.",Singular extension
chapter5_3,"Alpha–beta pruning prunes branches of the tree that can have no effect on the final evaluation, but forward pruning prunes moves that appear to be poor moves, but might possibly be good ones. Thus, the strategy saves computation time at the risk of making an error.",Forward pruning
chapter5_3,"Another technique, late move reduction, works under the assumption that move ordering has been done well, and therefore moves that appear later in the list of possible moves are less likely to be good moves. But rather than pruning them away completely, we just reduce the depth to which we search these moves, thereby saving time. If the reduced search comes back with a value above the current α value, we can re-run the search with the full depth.",Late move reduction
chapter16_1,"Decision theory, in its simplest form, deals with choosing among actions based on the desirability of their immediate outcomes; that is, the environment is assumed to be episodic in the sense defined on page 45. (This assumption is relaxed in Chapter 17.) The agent’s preferences are captured by a utility function, U(s), which assigns a single number to express the desirability of a state. The expected utility of an action given the evidence, EU(a) , is just the average utility value of the outcomes, weighted by the probability that the outcome occurs","Utility function, Expected utility"
chapter16_2,"ORDERABILITY: Given any two lotteries, a rational agent must either prefer one or else rate them as equally preferable. That is, the agent cannot avoid deciding. As noted on page 394, refusing to bet is like refusing to allow time to pass. TRANSITIVITY: Given any three lotteries, if an agent prefers A to B and prefers B to C, then the agent must prefer A to C. CONTINUITY: If some lottery is between and in preference, then there is some probability for which the rational agent will be indifferent between getting for sure and the lottery that yields with probability and with probability 1-p. SUBSTITUTABILITY: If an agent is indifferent between two lotteries A and B, then the agent is indifferent between two more complex lotteries that are the same except that B is substituted for A in one of them. This holds regardless of the probabilities and the other outcome(s) in the lotteries. MONOTONICITY: Suppose two lotteries have the same two possible outcomes, A and B. If an agent prefers A to B, then the agent must prefer the lottery that has a higher probability for A(and vice versa). DECOMPOSABILITY: Compound lotteries can be reduced to simpler ones using thelaws of probability. This has been called the “no fun in gambling” rule: as Figure16.1(b) shows, it compresses two consecutive lotteries into a single equivalentlottery.","Orderability, Transitivity, Continuity, Substitutability, Monotonicity, Decomposability"
chapter16_2,"As in game-playing, in a deterministic environment an agent needs only a preference ranking on states—the numbers don’t matter. This is called a value function or ordinal utility function.","Value function, Ordinal utility function"
chapter16_3,"If we want to build a decision-theoretic system that helps a human make decisions or acts on his or her behalf, we must first work out what the human’s utility function is. This process, often called preference elicitation, involves presenting choices to the human and using the observed preferences to pin down the underlying utility function.",Preference elicitation
chapter16_3,"Currently several agencies of the U.S. government, including the Environmental Protection Agency, the Food and Drug Administration, and the Department of Transportation, use the value of a statistical life to determine the costs and benefits of regulations and interventions. Typical values in 2019 are roughly $10 million. Some attempts have been made to find out the value that people place on their own lives. One common “currency” used in medical and safety analysis is the micromort, a one in a million chance of death. If you ask people how much they would pay to avoid a risk—for example, to avoid playing Russian roulette with a million-barreled revolver—they will respond with very large numbers, perhaps tens of thousands of dollars, but their actual behavior reflects a much lower monetary value for a micromort.","Value of a statistical life, Micromort"
chapter16_3,"It will usually be the case that an agent prefers more money to less, all other things being equal. We say that the agent exhibits a monotonic preference for more money. This does not mean that money behaves as a utility function, because it says nothing about preferences between lotteries involving money.",Monotonic preference
chapter16_3,"That is, agents with curves of this shape are risk-averse: they prefer a sure thing with a payoff that is less than the expected monetary value of a gamble. On the other hand, in the “desperate” region at large negative wealth in Figure 16.2(b), the behavior is risk-seeking. The value an agent will accept in lieu of a lottery is called the certainty equivalent of the lottery. Studies have shown that most people will accept about $400 in lieu of a gamble that gives $1000 half the time and $0 the other half—that is, the certainty equivalent of the lottery is $400, while the EMV is $500. The difference between the EMV of a lottery and its certainty equivalent is called the insurance premium. Risk aversion is the basis for the insurance industry, because it means that insurance premiums are positive. People would rather pay a small insurance premium than gamble the price of their house against the chance of a fire. From the insurance company’s point of view, the price of the house is very small compared with the firm’s total reserves. This means that the insurer’s utility curve is approximately linear over such a small region, and the gamble costs the company almost nothing.","Risk-averse, Risk-seeking, Certainty equivalent, Insurance premium"
chapter16_3,"Notice that for small changes in wealth relative to the current wealth, almost any curve will be approximately linear. An agent that has a linear curve is said to be risk-neutral. For gambles with small sums, therefore, we expect risk neutrality. In a sense, this justifies the simplified procedure that proposed small gambles to assess probabilities and to justify the axioms of probability in Section 12.2.3.",Risk-neutral
chapter16_3,"It is a straightforward matter to calculate the distribution of the maximum of the k estimates and hence quantify the extent of our disappointment. (This calculation is a special case of computing an order statistic, the distribution of any particular ranked element of a sample.) Suppose that each estimate Xi has a probability density function f(x) and cumulative distribution F(x).",order statistic
chapter16_3,"Decision theory is a normative theory: it describes how a rational agent should act. A descriptive theory, on the other hand, describes how actual agents—for example, humans— really do act. The application of economic theory would be greatly enhanced if the two coincided, but there appears to be some experimental evidence to the contrary. The evidence suggests that humans are “predictably irrational”","Normative theory, Descriptive theory"
chapter6_1,"A domain, Di, consists of a set of allowable values, {v1, …, vk}, for variable Xi. For example, a Boolean variable would have the domain {true, false}. Different variables can have different domains of different sizes. Each constraint Cj consists of a pair ⟨scope, rel⟩, where scope is a tuple of variables that participate in the constraint and rel is a relation that defines the values that those variables can take on. A relation can be represented as an explicit set of all tuples of values that satisfy the constraint, or as a function that can compute whether a tuple is a member of the relation.",Relation
chapter6_1,"CSPs deal with assignments of values to variables, {Xi = vi, Xj = vj, …}. An assignment that does not violate any constraints is called a consistent or legal assignment. A complete assignment is one in which every variable is assigned a value, and a solution to a CSP is a consistent, complete assignment. A partial assignment is one that leaves some variables unassigned, and a partial solution is a partial assignment that is consistent. Solving a CSP is an NP-complete problem in general, although there are important subclasses of CSPs that can be solved very efficiently.","Assignments, Consistent, Complete assignment, Solution, Partial assignment, Partial solution"
chapter6_1,"It can be helpful to visualize a CSP as a constraint graph. The nodes of the graph correspond to variables of the problem, and an edge connects any two variables that participate in a constraint.",Constraint graph
chapter6_1,"The simplest kind of CSP involves variables that have discrete, finite domains. Map-coloring problems and scheduling with time limits are both of this kind. The 8-queens problem can also be viewed as a finite-domain CSP, where the variables Q1, …, Q8 correspond to the queens in columns 1 to 8, and the domain of each variable specifies the possible row numbers for the queen in that column, Di = {1, 2, 3, 4, 5, 6, 7, 8}. The constraints say that no two queens can be in the same row or diagonal.","Discrete domain, Finite domain"
chapter6_1,"A discrete domain can be infinite, such as the set of integers or strings. (If we didn’t put a deadline on the job-scheduling problem, there would be an infinite number of start times for each variable.) With infinite domains, we must use implicit constraints like T1 + d1 ≤ T2 rather than explicit tuples of values. Special solution algorithms (which we do not discuss here) exist for linear constraints on integer variables—that is, constraints, such as the one just given, in which each variable appears only in linear form. It can be shown that no algorithm exists for solving general nonlinear constraints on integer variables—the problem is undecidable.","Infinite, Linear constraints, Nonlinear constraints"
chapter6_1,"In addition to examining the types of variables that can appear in CSPs, it is useful to look at the types of constraints. The simplest type is the unary constraint, which restricts the value of a single variable. For example, in the map-coloring problem it could be the case that South Australians won’t tolerate the color green; we can express that with the unary constraint ⟨(SA), SA ≠ green⟩. (The initial specification of the domain of a variable can also be seen as a unary constraint.) A binary constraint relates two variables. For example, SA ≠ NSW is a binary constraint. A binary CSP is one with only unary and binary constraints. A constraint involving an arbitrary number of variables is called a global constraint. (The name is traditional but confusing because a global constraint need not involve all the variables in a problem). One of the most common global constraints is Alldiff, which says that all of the variables involved in the constraint must have different values. In Sudoku problems (see Section 6.2.6), all variables in a row, column, or 3 × 3 box must satisfy an Alldiff constraint.",Unary constraint
chapter6_1,"The constraints we have described so far have all been absolute constraints, violation of which rules out a potential solution. Many real-world CSPs include preference constraints indicating which solutions are preferred. For example, in a university class-scheduling problem there are absolute constraints that no professor can teach two classes at the same time. But we also may allow preference constraints: Prof. R might prefer teaching in the morning, whereas Prof. N prefers teaching in the afternoon. A schedule that has Prof. R teaching at 2 p.m. would still be an allowable solution (unless Prof. R happens to be the department chair) but would not be an optimal one.",Preference constraints
chapter6_1,"Preference constraints can often be encoded as costs on individual variable assignments—for example, assigning an afternoon slot for Prof. R costs 2 points against the overall objective function, whereas a morning slot costs 1. With this formulation, CSPs with preferences can be solved with optimization search methods, either path-based or local. We call such a problem a constrained optimization problem, or COP. Linear programs are one class of COPs.",Constrained optimization problem
chapter6_2,"An atomic state-space search algorithm makes progress in only one way: by expanding a node to visit the successors. A CSP algorithm has choices. It can generate successors by choosing a new variable assignment, or it can do a specific type of inference called constraint propagation: using the constraints to reduce the number of legal values for a variable, which in turn can reduce the legal values for another variable, and so on. The idea is that this will leave fewer choices to consider when we make the next choice of a variable assignment. Constraint propagation may be intertwined with search, or it may be done as a preprocessing step, before search starts. Sometimes this preprocessing can solve the whole problem, so no search is required at all.",Constraint propagation
chapter6_2,"The key idea is local consistency. If we treat each variable as a node in a graph and each binary constraint as an edge, then the process of enforcing local consistency in each part of the graph causes inconsistent values to be eliminated throughout the graph. There are different types of local consistency, which we now cover in turn.",Local consistency
chapter6_2,"A single variable (corresponding to a node in the CSP graph) is node-consistent if all the values in the variable’s domain satisfy the variable’s unary constraints. For example, in the variant of the Australia map-coloring problem where South Australians dislike green, the variable SA starts with domain {red, green, blue}, and we can make it node consistent by eliminating green, leaving SA with the reduced domain {red, blue}. We say that a graph is node-consistent if every variable in the graph is node-consistent.",Node consistency
chapter6_2,"A variable in a CSP is arc-consistent1 if every value in its domain satisfies the variable’s binary constraints. More formally, Xi is arc-consistent with respect to another variable Xj if for every value in the current domain Di there is some value in the domain Dj that satisfies the binary constraint on the arc (Xi, Xj). A graph is arc-consistent if every variable is arc-consistent with every other variable. For example, consider the constraint Y = X2 where the domain of both X and Y is the set of decimal digits.",Arc consistency
chapter6_2,"Arc consistency tightens down the domains (unary constraints) using the arcs (binary constraints). To make progress on problems like map coloring, we need a stronger notion of consistency. Path consistency tightens the binary constraints by using implicit constraints that are inferred by looking at triples of variables.",Path consistency
chapter6_2,"Stronger forms of propagation can be defined with the notion of k-consistency. A CSP is k-consistent if, for any set of k − 1 variables and for any consistent assignment to those variables, a consistent value can always be assigned to any kth variable. 1-consistency says that, given the empty set, we can make any set of one variable consistent: this is what we called node consistency. 2-consistency is the same as arc consistency. For binary constraint graphs, 3-consistency is the same as path consistency.",k-consistency
chapter6_2,"Another important higher-order constraint is the resource constraint, sometimes called the Atmost constraint. For example, in a scheduling problem, let P1, …, P4 denote the numbers of personnel assigned to each of four tasks. The constraint that no more than 10 personnel are assigned in total is written as Atmost(10, P1, P2, P3, P4). We can detect an inconsistency simply by checking the sum of the minimum values of the current domains; for example, if each variable has the domain {3, 4, 5, 6}, the Atmost constraint cannot be satisfied. We can also enforce consistency by deleting the maximum value of any domain if it is not consistent with the minimum values of the other domains. Thus, if each variable in our example has the domain {2, 3, 4, 5, 6}, the values 5 and 6 can be deleted from each domain.",Resource constraint
chapter6_2,"For large resource-limited problems with integer values—such as logistical problems involving moving thousands of people in hundreds of vehicles—it is usually not possible to represent the domain of each variable as a large set of integers and gradually reduce that set by consistency-checking methods. Instead, domains are represented by upper and lower bounds and are managed by bounds propagation.",bounds propagation
chapter6_2,"We say that a CSP is bounds-consistent if for every variable X, and for both the lower--ound and upper-bound values of X, there exists some value of Y that satisfies the constraint between X and Y for every variable Y . This kind of bounds propagation is widely used in practical constraint problems.",Bounds-consistent
chapter6_3,"This intuitive idea—choosing the variable with the fewest “legal” values—is called the minimum-remaining-values (MRV) heuristic. It also has been called the “most constrained variable” or “fail-first” heuristic, the latter because it picks a variable that is most likely to cause a failure soon, thereby pruning the search tree. If some variable X has no legal values left, the MRV heuristic will select X and failure will be detected immediately—avoiding pointless searches through other variables. The MRV heuristic usually performs better than a random or static ordering, sometimes by orders of magnitude, although the results vary depending on the problem.",minimum-remaining-values
chapter6_3,"The MRV heuristic doesn’t help at all in choosing the first region to color in Australia, because initially every region has three legal colors. In this case, the degree heuristic comes in handy. It attempts to reduce the branching factor on future choices by selecting the variable that is involved in the largest number of constraints on other unassigned variables.The minimum- remaining-values heuristic is usually a more powerful guide, but the degree heuristic can be useful as a tie-breaker. Once a variable has been selected, the algorithm must decide on the order in which to examine its values. The least-constraining-value heuristic is effective for this. It prefers the value that rules out the fewest choices for the neighboring variables in the constraint graph.","Degree heuristic, Least-constraining-value"
chapter6_3,"One of the simplest forms of inference is called forward checking. Whenever a variable X is assigned, the forward-checking process establishes arc consistency for it: for each unassigned variable Y that is connected to X by a constraint, delete from Y ’s domain any value that is inconsistent with the value chosen for X.",forward-checking
chapter6_3,"The algorithm called MAC (for Maintaining Arc Consistency) detects inconsistencies like this. After a variable Xi is assigned a value, the INFERENCE procedure calls AC-3, but instead of a queue of all arcs in the CSP, we start with only the arcs (Xj, Xi) for all Xj that are unassigned variables that are neighbors of Xi. From there, AC-3 does constraint propagation in the usual way, and if any variable has its domain reduced to the empty set, the call to AC-3 fails and we know to backtrack immediately. We can see that MAC is strictly more powerful than forward checking because forward checking does the same thing as MAC on the initial arcs in MAC’s queue; but unlike MAC, forward checking does not recursively propagate constraints when changes are made to the domains of variables.",Maintaining Arc Consistency
chapter6_3,"The BACKTRACKING-SEARCH algorithm in Figure 6.5 has a very simple policy for what to do when a branch of the search fails: back up to the preceding variable and try a different value for it. This is called chronological backtracking because the most recent decision point is revisited. In this subsection, we consider better possibilities.",Chronological backtracking
chapter6_3,"A more intelligent approach is to backtrack to a variable that might fix the problem—a variable that was responsible for making one of the possible values of SA impossible. To do this, we will keep track of a set of assignments that are in conflict with some value for SA. The set (in this case {Q = red, NSW = green, V = blue}), is called the conflict set for SA. The backjumping method backtracks to the most recent assignment in the conflict set; in this case, backjumping would jump over Tasmania and try a new value for V . This method is easily implemented by a modification to BACKTRACK such that it accumulates the conflict set while checking for a legal value to assign. If no legal value is found, the algorithm should return the most recent element of the conflict set along with the failure indicator.","Conflict set, Backjumping"
chapter6_3,"This leads to a different–and deeper–notion of the conflict set for a variable such as NT: it is that set of preceding variables that caused NT, together with any subsequent variables, to have no consistent solution. In this case, the set is WA and NSW, so the algorithm should backtrack to NSW and skip over Tasmania. A backjumping algorithm that uses conflict sets defined in this way is called conflict-directed backjumping.",Conflict-directed backjumping
chapter6_3,"When we reach a contradiction, backjumping can tell us how far to back up, so we don’t waste time changing variables that won’t fix the problem. But we would also like to avoid running into the same problem again. When the search arrives at a contradiction, we know that some subset of the conflict set is responsible for the problem. Constraint learning is the idea of finding a minimum set of variables from the conflict set that causes the problem. This set of variables, along with their corresponding values, is called a no-good. We then record the no-good, either by adding a new constraint to the CSP to forbid this combination of assignments or by keeping a separate cache of no-goods.","Constraint learning, No-good"
chapter6_4,"Local search algorithms (see Section 4.1) turn out to be very effective in solving many CSPs. They use a complete-state formulation (as introduced in Section 4.1.1) where each state assigns a value to every variable, and the search changes the value of one variable at a time. As an example, we’ll use the 8-queens problem, as defined as a CSP on page 183. In Figure 6.8 we start on the left with a complete assignment to the 8 variables; typically this will violate several constraints. We then randomly choose a conflicted variable, which turns out to be Q8, the rightmost column. We’d like to change the value to something that brings us closer to a solution; the most obvious approach is to select the value that results in the minimum number of conflicts with other variables—the min-conflicts heuristic.",Min-conflicts
chapter6_4,"Another technique called constraint weighting aims to concentrate the search on the important constraints. Each constraint is given a numeric weight, initially all 1. At each step of the search, the algorithm chooses a variable/value pair to change that will result in the lowest total weight of all violated constraints. The weights are then adjusted by incrementing the weight of each constraint that is violated by the current assignment. This has two benefits: it adds topography to plateaus, making sure that it is possible to improve from the current state, and it also adds learning: over time the difficult constraints are assigned higher weights.",Constraint weighting
chapter6_5,"The only way we can possibly hope to deal with the vast real world is to decompose it into subproblems. Looking again at the constraint graph for Australia (Figure 6.1(b), repeated as Figure 6.12(a)), one fact stands out: Tasmania is not connected to the mainland.3 Intuitively, it is obvious that coloring Tasmania and coloring the mainland are independent subproblems—any solution for the mainland combined with any solution for Tasmania yields a solution for the whole map. Independence can be ascertained simply by finding connected components of the constraint graph.","Independent subproblems, Connected component"
chapter6_5,"Completely independent subproblems are delicious, then, but rare. Fortunately, some other graph structures are also easy to solve. For example, a constraint graph is a tree when any two variables are connected by only one path. We will show that any tree-structured CSP can be solved in time linear in the number of variables.4 The key is a new notion of consistency,  called directional arc consistency or DAC. A CSP is defined to be directional arc-consistent under an ordering of variables X1, X2, …, Xn if and only if every Xi is arc-consistent with each Xj for j > i. To solve a tree-structured CSP, first pick any variable to be the root of the tree, and choose an ordering of the variables such that each variable appears after its parent in the tree. Such an ordering is called a topological sort.","Directional arc consistency, Topological sort"
chapter6_5,"The second way to reduce a constraint graph to a tree is based on constructing a tree decomposition of the constraint graph: a transformation of the original graph into a tree where each node in the tree consists of a set of variables, as in Figure 6.13. A tree decomposition must satisfy these three requirements: Every variable in the original problem appears in at least one of the tree nodes. If two variables are connected by a constraint in the original problem, they must appear together (along with the constraint) in at least one of the tree nodes. If a variable appears in two nodes in the tree, it must appear in every node along the path connecting those nodes.",Tree decomposition
chapter6_5,"So far, we have looked at the structure of the constraint graph. There can also be important structure in the values of variables, or in the structure of the constraint relations themselves. Consider the map-coloring problem with d colors. For every consistent solution, there is actually a set of d! solutions formed by permuting the color names. For example, on the Australia map we know that WA, NT, and SA must all have different colors, but there are 3! = 6 ways to assign three colors to three regions. This is called value symmetry. We would like to reduce the search space by a factor of d! by breaking the symmetry in assignments. We do this by introducing a symmetry-breaking constraint. For our example, we might impose an arbitrary ordering constraint, NT < SA < WA, that requires the three values to be in alphabetical order. This constraint ensures that only one of the d! solutions is possible: {NT = blue, SA = green, WA = red}.","Value, symmetry, Symmetry-breaking, constraint"
chapter7_1,"Humans, it seems, know things; and what they know helps them do things. In AI, knowledge-based agents use a process of reasoning over an internal representation of knowledge to decide what actions to take.","Knowledge-based agents, Reasoning, Representation"
chapter7_1,"The central component of a knowledge-based agent is its knowledge base, or KB. A knowledge base is a set of sentences. (Here “sentence” is used as a technical term. It is related but not identical to the sentences of English and other natural languages.) Each sentence is expressed in a language called a knowledge representation language and represents some assertion about the world. When the sentence is taken as being given without being derived from other sentences, we call it an axiom.","Knowledge-base, Sentence, Knowledge representation language, Axiom"
chapter7_1,"There must be a way to add new sentences to the knowledge base and a way to query what is known. The standard names for these operations are TELL and ASK, respectively. Both operations may involve inference—that is, deriving new sentences from old. Inference must obey the requirement that when one ASKs a question of the knowledge base, the answer should follow from what has been told (or TELLed) to the knowledge base previously. Later in this chapter, we will be more precise about the crucial word “follow.” For now, take it to mean that the inference process should not make things up as it goes along.",Inference
chapter7_1,"Like all our agents, it takes a percept as input and returns an action. The agent maintains a knowledge base, KB, which may initially contain some background knowledge. Because of the definitions of TELL and ASK, however, the knowledge-based agent is not an arbitrary program for calculating actions. It is amenable to a description at the knowledge level, where we need specify only what the agent knows and what its goals are, in order to determine its behavior.","Background knowledge, Knowledge level"
chapter7_1,"A knowledge-based agent can be built simply by TELLing it what it needs to know. Starting with an empty knowledge base, the agent designer can TELL sentences one by one until the agent knows how to operate in its environment. This is called the declarative approach to system building. In contrast, the procedural approach encodes desired behaviors directly as program code. In the 1970s and 1980s, advocates of the two approaches engaged in heated debates. We now understand that a successful agent often combines both declarative and procedural elements in its design, and that declarative knowledge can often be compiled into more efficient procedural code.","Declarative, Procedural"
chapter7_3,"we said that knowledge bases consist of sentences. These sentences are expressed according to the syntax of the representation language, which specifies all the sentences that are well formed. A logic must also define the semantics, or meaning, of sentences. The semantics defines the truth of each sentence with respect to each possible world.","Semantics,Truth, Possible world"
chapter7_3,"When we need to be precise, we use the term model in place of “possible world.” Whereas possible worlds might be thought of as (potentially) real environments that the agent might or might not be in, models are mathematical abstractions, each of which has a fixed truth value (true or false) for every relevant sentence. Informally, we may think of a possible world as, for example, having x men and y women sitting at a table playing bridge, and the sentence x + y = 4 is true when there are four people in total. Formally, the possible models are just all possible assignments of nonnegative integers to the variables x and y. Each such assignment determines the truth of any sentence of arithmetic whose variables are x and y. If a sentence α is true in model m, we say that m satisfies α or sometimes m is a model of α. We use the notation M(α) to mean the set of all models of α.","Model, Satisfaction"
chapter7_3,"Now that we have a notion of truth, we are ready to talk about logical reasoning. This involves the relation of logical entailment between sentences—the idea that a sentence follows logically from another sentence.",Entailment
chapter7_3,"The preceding example not only illustrates entailment but also shows how the definition of entailment can be applied to derive conclusions—that is, to carry out logical inference. The inference algorithm illustrated in Figure 7.5 is called model checking, because it enumerates all possible models to check that is true in all models in which KB is true","Logical inference, Model checking"
chapter7_3,"An inference algorithm that derives only entailed sentences is called sound or truth-preserving. Soundness is a highly desirable property. An unsound inference procedure essentially makes things up as it goes along—it announces the discovery of nonexistent needles. It is easy to see that model checking, when it is applicable, is a sound procedure.","Sound, Truth-preserving"
chapter7_3,"The property of completeness is also desirable: an inference algorithm is complete if it can derive any sentence that is entailed. For real haystacks, which are finite in extent, it seems obvious that a systematic examination can always decide whether the needle is in the haystack. For many knowledge bases, however, the haystack of consequences is infinite, and completeness becomes an important issue.6 Fortunately, there are complete inference procedures for logics that are sufficiently expressive to handle many knowledge bases.",Completeness
chapter7_4,"We now present propositional logic. We describe its syntax (the structure of sentences) and its semantics (the way in which the truth of sentences is determined). From these, we derive a simple, syntactic algorithm for logical inference that implements the semantic notion of entailment.",Propositional logic
chapter7_4,"The syntax of propositional logic defines the allowable sentences. The atomic sentences consist of a single proposition symbol. Each such symbol stands for a proposition that can be true or false. We use symbols that start with an uppercase letter and may contain other letters or subscripts, for example: P, Q, R, W1, 3, and FacingEast. The names are arbitrary but are often chosen to have some mnemonic value—we use W1, 3 to stand for the proposition that the wumpus is in [1, 3]. (Remember that symbols such as W1, 3 are atomic, i.e., W, 1, and 3 are not meaningful parts of the symbol.) There are two proposition symbols with fixed meanings: True is the always-true proposition and False is the always-false proposition. Complex sentences are constructed from simpler sentences, using parentheses and operators called logical connectives. There are five connectives in common use:","Atomic sentences, Proposition symbol, Complex sentences, Logical connectives"
chapter7_4,"¬ (not). A sentence such as ¬W1, 3 is called the negation of W1, 3. A literal is either an atomic sentence (a positive literal) or a negated atomic sentence (a negative literal)","Negation, Literal"
chapter7_4,"∧ (and). A sentence whose main connective is ∧, such as W1, 3 ∧ P3, 1, is called a conjunction; its parts are the conjuncts. (The ∧ looks like an “A” for “And.”)",Conjunction
chapter7_4,"∨ (or). A sentence whose main connective is ∨, such as (W1, 3 ∧ P3, 1) ∨ W2, 2, is a disjunction; its parts are disjuncts—in this example, (W1, 3 ∧ P3, 1) and W2, 2.",Disjunction
chapter7_4,"⇒ (implies). A sentence such as (W1, 3 ∧ P3, 1) ⇒ ¬W2, 2 is called an implication (or conditional). Its premise or antecedent is (W1, 3 ∧ P3, 1), and its conclusion or consequent is ¬W2, 2. Implications are also known as rules or if–then statements. The implication symbol is sometimes written in other books as ⊃ or →.","Implication, Premise, Conclusion, Rules"
chapter7_4,"⇔ (if and only if). The sentence W1, 3 ⇔ ¬W2, 2 is a biconditional.",Biconditional
chapter7_4,"Having specified the syntax of propositional logic, we now specify its semantics. The semantics defines the rules for determining the truth of a sentence with respect to a particular model. In propositional logic, a model simply sets the truth value—true or false— for every proposition symbol.",Truth value
chapter7_4,"The rules can also be expressed with truth tables that specify the truth value of a complex sentence for each possible assignment of truth values to its components. Truth tables for the five connectives are given in Figure 7.8 . From these tables, the truth value of any sentence s can be computed with respect to any model m by a simple recursive evaluation.",Truth table
chapter7_5,"So far, we have shown how to determine entailment by model checking: enumerating models and showing that the sentence must hold in all models. In this section, we show how entailment can be done by theorem proving—applying rules of inference directly to the sentences in our knowledge base to construct a proof of the desired sentence without consulting models. If the number of models is large but the length of the proof is short, then theorem proving can be more efficient than model checking.",Theorem proving
chapter7_5,"Before we plunge into the details of theorem-proving algorithms, we will need some additional concepts related to entailment. The first concept is logical equivalence: two sentences and are logically equivalent if they are true in the same set of models. We write this as (Note that is used to make claims about sentences, while is used as part of a sentence.) For example, we can easily show (using truth tables) that and are logically equivalent; other equivalences are shown in Figure 7.11 . These equivalences play much the same role in logic as arithmetic identities do in ordinary mathematics. An alternative definition of equivalence is as follows: any two sentences and are equivalent if and only if each of them entails the other",Logical equivalence
chapter7_5,"The second concept we will need is validity. A sentence is valid if it is true in all models. For example, the sentence P ∨ ¬P is valid. Valid sentences are also known as tautologies—they are necessarily true. Because the sentence True is true in all models, every valid sentence is logically equivalent to True. What good are valid sentences? From our definition of entailment, we can derive the deduction theorem, which was known to the ancient Greeks.","Validity, Tautology, Deduction theorem"
chapter7_5,"The final concept we will need is satisfiability. A sentence is satisfiable if it is true in, or satisfied by, some model. For example, the knowledge base given earlier, ( R1 ∧ R2 ∧ R3 ∧ R4 ∧ R5), is satisfiable because there are three models in which it is true, as shown in Figure 7.9 . Satisfiability can be checked by enumerating the possible models until one is found that satisfies the sentence. The problem of determining the satisfiability of sentences in propositional logic—the SAT problem—was the first problem proved to be NP-complete. Many problems in computer science are really satisfiability problems. For example, all the constraint satisfaction problems in Chapter 6 ask whether the constraints are satisfiable by some assignment.","Satisfiability, SAT"
chapter7_5,"Proving β from α by checking the unsatisfiability of (α ∧ ¬β) corresponds exactly to the standard mathematical proof technique of reductio ad absurdum (literally, “reduction to an absurd thing”). It is also called proof by refutation or proof by contradiction. One assumes a sentence β to be false and shows that this leads to a contradiction with known axioms α. This contradiction is exactly what is meant by saying that the sentence (α ∧ ¬β) is unsatisfiable.","Reductio ad absurdum, Refutation, Contradiction"
chapter7_5,"This section covers inference rules that can be applied to derive a proof—a chain of conclusions that leads to the desired goal. Another useful inference rule is And-Elimination, which says that, from a conjunction, any of the conjuncts can be inferred","Inference rules, Proof, Modus Ponens, And-Elimination "
chapter7_5,"One final property of logical systems is monotonicity, which says that the set of entailed sentences can only increase as information is added to the knowledge base. For any sentences α and β, if KB ⊨ α then KB ∧ β ⊨ α .",Monotonicity
chapter7_5,A sentence expressed as a conjunction of clauses is said to be in conjunctive normal form or CNF,Conjunctive normal form
chapter7_5,"The completeness of resolution makes it a very important inference method. In many practical situations, however, the full power of resolution is not needed. Some real-world knowledge bases satisfy certain restrictions on the form of sentences they contain, which enables them to use a more restricted and efficient inference algorithm. One such restricted form is the definite clause, which is a disjunction of literals of which exactly one is positive.",Definite clause
chapter7_5,"Slightly more general is the Horn clause, which is a disjunction of literals of which at most one is positive. So all definite clauses are Horn clauses, as are clauses with no positive literals. these are called goal clauses. Horn clauses are closed under resolution: if you resolve two. Horn clauses, you get back a Horn clause. One more class is the k-CNF sentence, which is a CNF sentence where each clause has at most k literals.","Horn clause, Goal clauses"
chapter7_5,"In Horn form, the premise is called the body and the conclusion is called the head. A sentence consisting of a single positive literal, such as L1, 1, is called a fact.","Body, Head, Fact"
chapter7_5,"Inference with Horn clauses can be done through the forward-chaining and backward-chaining algorithms, which we explain next. Both of these algorithms are natural, in that the inference steps are obvious and easy for humans to follow. This type of inference is the basis for logic programming","Forward-chaining, Backward-chaining"
chapter7_5,"Forward chaining is an example of the general concept of data-driven reasoning—that is, reasoning in which the focus of attention starts with the known data. It can be used within an agent to derive conclusions from incoming percepts, often without a specific query in mind. For example, the wumpus agent might TELL its percepts to the knowledge base using an incremental forward-chaining algorithm in which new facts can be added to the agenda to initiate new inferences. In humans, a certain amount of data-driven reasoning occurs as new information arrives. For example, if I am indoors and hear rain starting to fall, it might occur to me that the picnic will be canceled. Yet it will probably not occur to me that the seventeenth petal on the largest rose in my neighbor’s garden will get wet; humans keep forward chaining under careful control, lest they be swamped with irrelevant consequences.",Data-driven
chapter7_5,"Backward chaining is a form of goal-directed reasoning. It is useful for answering specific questions such as “What shall I do now?” and “Where are my keys?” Often, the cost of backward chaining is much less than linear in the size of the knowledge base, because the process touches only relevant facts.",Goal-directed reasoning
chapter17_1,"a sequential decision problem for a fully observable, stochastic environment with a Markovian transition model and additive rewards is called a Markov decision process, or MDP, and consists of a set of states (with an initial state s0); a set ACTIONS(s) of actions in each state; a transition model P(s′|s, a); and a reward function R(s, a, s′). Methods for solving MDPs usually involve dynamic programming: simplifying a problem by recursively breaking it into smaller pieces and remembering the optimal solutions to the pieces.","Markov decision process, Dynamic programming"
chapter17_1,"To complete the definition of the task environment, we must specify the utility function for the agent. Because the decision problem is sequential, the utility function will depend on a sequence of states and actions—an environment history—rather than on a single state. Later in this section, we investigate the nature of utility functions on histories; for now, we simply stipulate that for every transition from s to s′ via action a, the agent receives a reward R(s, a, s′).",Reward
chapter17_1,"The next question is, what does a solution to the problem look like? No fixed action sequence can solve the problem, because the agent might end up in a state other than the goal. Therefore, a solution must specify what the agent should do for any state that the agent might reach. A solution of this kind is called a policy. It is traditional to denote a policy by π, and π(s) is the action recommended by the policy π for state s. No matter what the outcome of the action, the resulting state will be in the policy, and the agent will know what to do next.",Policy
chapter17_1,"Each time a given policy is executed starting from the initial state, the stochastic nature of the environment may lead to a different environment history. The quality of a policy is therefore measured by the expected utility of the possible environment histories generated by that policy. An optimal policy is a policy that yields the highest expected utility. We use π∗ to denote an optimal policy. Given π∗ , the agent decides what to do by consulting its current percept, which tells it the current state s, and then executing the action π∗(s). A policy represents the agent function explicitly and is therefore a description of a simple reflex agent, computed from the information used for a utility-based agent.",Optimal policy
chapter17_1,"So, with a finitehorizon, an optimal action in a given state may depend on how much time is left. A policy thatdepends on the time is called nonstationary. With no fixed time limit, on the other hand, there is no reason to behave differently in the same state at different times. Hence, an optimal action depends only on the current state, and the optimal policy is stationary. Policies for the infinite-horizon case are therefore simpler than those for the finite-horizon case, and we deal mainly with the infinite-horizon case in this chapter. (We will see later that for partially observable environments, the infinite-horizon case is not so simple.) Note that “infinite horizon” does not necessarily mean that all state sequences are infinite; it just means that there is no fixed deadline. There can be finite state sequences in an infinite-horizon MDP that contains a terminal state.","Nonstationary policy, Stationary policy"
chapter17_1,"where the discount factor γ is a number between 0 and 1. The discount factor describes the preference of an agent for current rewards over future rewards. When γ is close to 0, rewards in the distant future are viewed as insignificant. When γ is close to 1, an agent is more willing to wait for long-term rewards. When γ is exactly 1, discounted rewards reduce to the special case of purely additive rewards. Notice that additivity was used implicitly in our use of path cost functions in heuristic search algorithms (Chapter 3 ).","Discount factor, Additive reward"
chapter17_1,". If the environment contains terminal states and if the agent is guaranteed to get to one eventually, then we will never need to compare infinite sequences. A policy that is guaranteed to reach a terminal state is called a proper policy. With proper policies, we can use γ = 1 (i.e., additive undiscounted rewards). The first three policies shown in Figure 17.2(b)  are proper, but the fourth is improper. It gains infinite total reward by staying away from the terminal states when the reward for transitions between nonterminal states is positive. The existence of improper policies can cause the standard algorithms for solving MDPs to fail with additive rewards, and so provides a good reason for using discounted rewards. Infinite sequences can be compared in terms of the average reward obtained pertime step.","Proper policy, Average reward"
chapter17_1,"We have defined the utility of a state, , as the expected sum of discounted rewards from that point onwards. From this, it follows that there is a direct relationship between the utility of a state and the utility of its neighbors: the utility of a state is the expected reward for the next transition plus the discounted utility of the next state, assuming that the agent chooses the optimal action. This is called the Bellman equation.",Bellman equation
chapter17_1,"Another important quantity is the action-utility function, or Q-function: is the expected utility of taking a given action in a given state.",Q-function
chapter17_2,"In this section, we present four different algorithms for solving MDPs. The first three, value iteration, policy iteration, and linear programming, generate exact solutions offline. The fourth is a family of online approximate algorithms that includes Monte Carlo planning.",Monte Carlo planning
chapter17_2,"The Bellman equation (Equation (17.5) ) is the basis of the value iteration algorithm for solving MDPs. If there are n possible states, then there are n Bellman equations, one for each state. The n equations contain n unknowns—the utilities of the states. So we would like to solve these simultaneous equations to find the utilities. There is one problem: the equations are nonlinear, because the “max” operator is not a linear operator. Whereas systems of linear equations can be solved quickly using linear algebra techniques, systems of nonlinear equations are more problematic. One thing to try is an iterative approach. We start with arbitrary initial values for the utilities, calculate the right-hand side of the equation, and plug it into the left-hand side—thereby updating the utility of each state from the utilities of its neighbors. We repeat this until we reach an equilibrium.",Value iteration
chapter17_2,"The Bellman equation (Equation (17.5)) is the basis of the value iteration algorithm for solving MDPs. If there are n possible states, then there are n Bellman equations, one for each state. The n equations contain n unknowns—the utilities of the states. So we would like to solve these simultaneous equations to find the utilities. There is one problem: the equations are nonlinear, because the “max” operator is not a linear operator. Whereas systems of linear equations can be solved quickly using linear algebra techniques, systems of nonlinear equations are more problematic. One thing to try is an iterative approach. We start with arbitrary initial values for the utilities, calculate the right-hand side of the equation, and plug it into the left-hand side—thereby updating the utility of each state from the utilities of its neighbors. We repeat this until we reach an equilibrium. Let Ui(s) be the utility value for state s at the ith iteration. The iteration step, called a Bellman update","Value iteration, Bellman update"
chapter17_2,"The basic concept used in showing that value iteration converges is the notion of a contraction. Roughly speaking, a contraction is a function of one argument that, when applied to two different inputs in turn, produces two output values that are “closer together, ” by at least some constant factor, than the original inputs. For example, the function “divide by two” is a contraction, because, after we divide any two numbers by two, their difference is halved. Notice that the “divide by two” function has a fixed point, namely zero, that is unchanged by the application of the function. From this example, we can discern two important properties of contractions: A contraction has only one fixed point; if there were two fixed points they would not get closer together when the function was applied, so it would not be a contraction. When the function is applied to any argument, the value must get closer to the fixed point (because the fixed point does not move), so repeated application of a contraction always reaches the fixed point in the limit.",contraction
chapter17_2,"In the previous section, we observed that it is possible to get an optimal policy even when the utility function estimate is inaccurate. If one action is clearly better than all others, then the exact magnitude of the utilities on the states involved need not be precise. This insight suggests an alternative way to find optimal policies. The policy iteration algorithm alternates the following two steps, beginning from some initial policy π0: POLICY EVALUATION: given a policy πi, calculate Ui = Uπi , the utility of each state if πi were to be executed. POLICY IMPROVEMENT: Calculate a new MEU policy πi+1, using one-step look-ahead based on Ui","Policy iteration, Policy evaluation, Policy improvement"
chapter17_2,"The algorithms we have described so far require updating the utility or policy for all states at once. It turns out that this is not strictly necessary. In fact, on each iteration, we can pick any subset of states and apply either kind of updating (policy improvement or simplified value iteration) to that subset. This very general algorithm is called asynchronous policy iteration.",Asynchronous policy iteration
chapter17_2,"In the previous section, we observed that it is possible to get an optimal policy even when the utility function estimate is inaccurate. If one action is clearly better than all others, then the exact magnitude of the utilities on the states involved need not be precise. This insight suggests an alternative way to find optimal policies. The policy iteration algorithm alternates the following two steps, beginning from some initial policy π0: POLICY EVALUATION: given a policy πi, calculate Ui = Uπi , the utility of each state if πi were to be executed. POLICY IMPROVEMENT: Calculate a new MEU policy πi+1, using one-step look-ahead based on Ui","Policy iteration, Policy evaluation, Policy improvement"
chapter17_2,"The algorithms we have described so far require updating the utility or policy for all states at once. It turns out that this is not strictly necessary. In fact, on each iteration, we can pick any subset of states and apply either kind of updating (policy improvement or simplified value iteration) to that subset. This very general algorithm is called asynchronous policy iteration. Given certain conditions on the initial policy and initial utility function, asynchronous policy iteration is guaranteed to converge to an optimal policy. The freedom to choose any states to work on means that we can design much more efficient heuristic algorithms—for example, algorithms that concentrate on updating the values of states that are likely to be reached by a good policy. There’s no sense planning for the results of an action you will never do.",Asynchronous policy iteration
,"Linear programming or LP, which was mentioned briefly in Chapter 4 (page 121), is a general approach for formulating constrained optimization problems, and there are many industrial-strength LP solvers available. Given that the Bellman equations involve a lot of sums and maxes, it is perhaps not surprising that solving an MDP can be reduced to solving a suitably formulated linear program.",Linear programming
chapter17_2,"This general approach is called real-time dynamic programming (RTDP) and is quite analogous to LRTA* in Chapter 4. Algorithms of this kind can be quite effective in moderate-sized domains such as grid worlds; in larger domains such as Tetris, there are two issues. First, the state space is such that any manageable set of explored states contains very few repeated states, so one might as well use a simple expectimax tree. Second, a simple heuristic for frontier nodes may not be enough to guide the agent, particularly if rewards are sparse.",Real-time dynamic programming (RTDP)
chapter17_3,"There are several different definitions of bandit problems; one of the cleanest and most general is as follows: Each arm Mi is a Markov reward process or MRP, that is, an MDP with only one possible action ai. It has states Si, transition model Pi(s′|s, ai), and reward Ri(s, ai, s′). The arm defines a distribution over sequences of rewards Ri, 0, Ri, 1, Ri, 2, …, where each Ri, t is a random variable. The overall bandit problem is an MDP: the state space is given by the Cartesian product S = S1 × ⋯ × Sn; the actions are a1, …, an; the transition model updates the state of whichever arm Mi is selected, according to its specific transition model, leaving the other arms unchanged; and the discount factor is γ.","Bandit problems, Markov reward process"
chapter17_3,"In Las Vegas, a one-armed bandit is a slot machine. A gambler can insert a coin, pull the lever, and collect the winnings (if any). An n-armed bandit has n levers. Behind each lever is a fixed but unknown probability distribution of winnings; each pull samples from that unknown distribution.",N-armed bandit
chapter17_3,"This equation defines a kind of “value” for M in terms of its ability to deliver a stream of timely rewards; the numerator of the fraction represents a utility while the denominator can be thought of as a “discounted time, ” so the value describes the maximum obtainable utility per unit of discounted time. (It’s important to remember that T in the equation is a stopping time, which is governed by a rule for stopping rather than being a simple integer; it reduces to a simple integer only when M is a deterministic reward sequence.) The value defined in Equation (17.15) is called the Gittins index of M.",Gittins index
chapter17_3,"Suppose we augment M so that at each state in M, the agent has two choices: either continue with M as before, or quit and receive an infinite sequence of λ-rewards (see Figure 17.13(a) ). This turns M into an MDP, whose optimal policy is just the optimal stopping rule for M. Hence the value of an optimal policy in this new MDP is equal to the value of an infinite sequence of λ-rewards, that is, λ/(1 − γ). So we can just solve this MDP ... but, unfortunately, we don’t know the value of λ to put into the MDP, as this is precisely what we are trying to calculate. But we do know that, at the tipping point, an optimal policy is indifferent between M and Mλ, so we could replace the choice to get an infinite sequence of λ-rewards with the choice to go back and restart M from its initial state s. (More precisely, we add a new action in every state that has the same rewards and outcomes as the action available in s; see Exercise 17.KATV.) This new MDP Ms , called a restart MDP",restart MDP
chapter17_3,"Perhaps the simplest and best-known instance of a bandit problem is the Bernoulli bandit, where each arm Mi produces a reward of 0 or 1 with a fixed but unknown probability μi.",Bernoulli bandit
chapter17_3,"A second method, Thompson sampling (Thompson, 1933), chooses chooses an arm randomly according to the probability that the arm is in fact optimal, given the samples so far. Suppose that Pi(μi) is the current probability distribution for the true value of arm Mi. Then a simple way to implement Thompson sampling is to generate one sample from each Pi and then pick the best sample. This algorithm also has a regret that grows as O(logN).",Thompson sampling
chapter17_3,"The task of choosing the best option under these conditions is called a selection problem. Selection problems are ubiquitous in industrial and personnel contexts. One often must decide which supplier to use for a process; or which candidate employees to hire. Selection problems are superficially similar to the bandit problem but have different mathematical properties. In particular, no index function exists for selection problems. The proof of this fact requires showing any scenario where the optimal policy switches its preferences for two arms M1 and M2 when a third arm M3 is added (see Exercise 17.SELC).",Selection problem
chapter17_3,"An important generalization of the bandit process is the bandit superprocess or BSP, in which each arm is a full Markov decision process in its own right, rather than being a Markov reward process with only one possible action. All other properties remain the same: the arms are independent, only one (or a bounded number) can be worked on at a time, and there is a single discount factor.","Bandit superprocess, BSP"
chapter17_3,"Examples of BSPs include daily life, where one can attend to one task at a time, even though several tasks may need attention; project management with multiple projects; teaching with multiple pupils needing individual guidance; and so on. The ordinary term for this is multitasking. It is so ubiquitous as to be barely noticeable: when formulating a real-world decision problem, decision analysts rarely ask if their client has other, unrelated problems.",Multitasking
chapter17_3,"Instead, we can take advantage of the loose nature of the interaction between the arms. Thisinteraction arises only from the agent’s limited ability to attend to the arms simultaneously.To some extent, the interaction can be modeled by the notion of opportunity cost: howmuch utility is given up per time step by not devoting that time step to another arm. Thehigher the opportunity cost, the more necessary it is to generate early rewards in a givenarm. In some cases, an optimal policy in a given arm is unaffected by the opportunity cost.(Trivially, this is true in a Markov reward process because there is only one policy.) In thatcase, an optimal policy can be applied, converting that arm into a Markov reward process.",Opportunity cost
chapter17_3,"Such an optimal policy, if it exists, is called a dominating policy. It turns out that by addingactions to states, it is always possible to create a relaxed version of an MDP (see Section3.6.2 ) so that it has a dominating policy, which thus gives an upper bound on the value ofacting in the arm. A lower bound can be computed by solving each arm separately (whichmay yield a suboptimal policy overall) and then computing the Gittins indices. If the lowerbound for acting in one arm is higher than the upper bounds for all other actions, then theproblem is solved; if not, then a combination of look-ahead search and recomputation ofbounds is guaranteed to eventually identify an optimal policy for the BSP. With thisapproach, relatively large BSPs (1040 states or more) can be solved in a few seconds.",Dominating policy
chapter22_2,"each sequence, the algorithm calculates the observed reward-to-go for each state andupdates the estimated utility for that state accordingly, just by keeping a running average feach state in a table. in the limit of infinitely many trials, the sample average will convergeto the true expectation in equation (22.1)4",reward-to-go
chapter22_2,"by ignoring the connections between states, direct utility estimation misses opportunities forlearning. for example, the second of the three trials given earlier reaches the state (3, 2), which has not previously been visited. the next transition reaches (3, 3), which is knownfrom the first trial to have a high utility. the bellman equation suggests immediately that(3, 2) is also likely to have a high utility, because it leads to (3, 3), but direct utility estimationlearns nothing until the end of the trial. more broadly, we can view direct utility estimationas searching for u in a hypothesis space that is much larger than it needs to be, in that itincludes many functions that violate the bellman equations. for this reason, the algorithm 4",direct utility estimation
chapter22_2,"one solution to this problem, called experience replay, ensures that the car keeps relivingits youthful crashing behavior at regular intervals. the learning algorithm can retaintrajectories from the entire learning process and replay those trajectories to ensure that itsvalue function is still accurate for parts of the state space it no longer visits.",experience replay
chapter22_2,"as noted in the introduction to this chapter, real-world environments may have very sparserewards: many primitive actions are required to achieve any nonzero reward. for example, soccer-playing robot might send a hundred thousand motor control commands to its varioujoints before conceding a goal. now it has to work out what it did wrong. the technical tertfor this is the credit assignment problem. other than playing trillions of soccer games sothat the negative reward eventually propagates back to the actions responsible for it, is ther",credit assignment
chapter22_2,"one common method, originally used in animal training, is called reward shaping. thisinvolves supplying the agent with additional rewards, called pseudorewards, for “makingprogress.” for example, we might give pseudorewards to the robot for making contact withthe ball or for advancing it toward the goal. such rewards can speed up learning enormoushand are simple to provide, but there is a risk that the agent will learn to maximize thepseudorewards rather than the true rewards; for example, standing next to the ball and“vibrating” causes many contacts with the ball.",reward shaping
chapter22_1,"it turns out, however, that a little bit of expertise can go a long way in reinforcementlearning. the two examples in the preceding paragraph—the win/loss rewards for chess anracing—are what we call sparse rewards, because in the vast majority of states the agent isgiven no informative reward signal at all. in games such as tennis and cricket, we can easil}supply additional rewards for each point won or for each run scored. in car racing, we coulreward the agent for making progress around the track in the right direction. when learninto crawl, any forward motion is an achievement. these intermediate rewards make learnin;",sparse
chapter22_1,"iyan adaptive dynamic programming (or adp) agent takes advantage of the constraintsamong the utilities of states by learning the transition model that connects them and solvingthe corresponding markov decision process using dynamic programming. for a passivelearning agent, this means plugging the learned transition model p(s'|s, 1(s)) and theobserved rewards r(s, 7(s), s’) into equation (22.2) to calculate the utilities of the states. aswe remarked in our discussion of policy iteration in chapter 17, these bellman equationsare linear when the policy 7 is fixed, so thev can be solved using any linear algebra package.",adaptive dynamic programming
chapter22_1,"model-based reinforcement learning: in these approaches the agent uses atransition model of the environment to help interpret the reward signals and to makedecisions about how to act. the model may be initially unknown, in which case theagent learns the model from observing the effects of its actions, or it may already beknown—for example, a chess program may know the rules of chess even if it does notknow how to choose good moves. in partially observable environments, the transitionmodel is also useful for state estimation (see chapter 14'2). model-based reinforcementlearning systems often learn a utility function u(s), defined (as in chapter 17) in termsof the sum of rewards from state s onward.2",model-based reinforcement learning
chapter22_1,"» model-free reinforcement learning: in these approaches the agent neitherknows nor learns a transition model for the environment. instead, it learns a more directvttenveaceitaticus at lexus ies, khohkaue: ""this ancmmacin arracnithan: wareieac:",model-free reinforcement learning
chapter22_1,"here, f is the exploration function. the function f(u, n) determines how greed (preferenc:for high values of the utility u) is traded off against curiosity (preference for actions thathave not been tried often and have a low count n). the function should be increasing in uand decreasing in n. obviously, there are many possible functions that fit these conditions.one particularly simple definition is",exploration function
chapter22_2,"all temporal-difference methods work by adjusting the utility estimates toward the idealequilibrium that holds locally when the utility estimates are correct. in the case of passivelearning, the equilibrium is given by equation (22.2). now equation (22.3)© does in factcause the agent to reach the equilibrium given by equation (22.2), but there is somesubtlety involved. first, notice that the update involves only the observed successor s', whereas the actual equilibrium conditions involve all possible next states. one might thinkthat this causes an improperly large change in u™(s) when a very rare transition occurs: bu",temporal-difference
chapter22_3,"unfortunately, the real world is less forgiving. if you are a baby sunfish, your probabilitysurviving to adulthood is about 0.00000001. many actions are irreversible, in the sensedefined for online search agents in section 4.5'8: no subsequent sequence of actions carrestore the state to what it was before the irreversible action was taken. in the worst casthe agent enters an absorbing state where no actions have any effect and no rewards at",absorbing state
chapter22_1,"an alternative is reinforcement learning (rl), in which an agent interacts with the worldand periodically receives rewards (or, in the terminology of psychology, reinforcements)that reflect how well it is doing. for example, in chess the reward is 1 for winning, 0 forlosing, and 4 for a draw. we have already seen the concept of rewards in chapter 17 formarkov decision processes (mdps). indeed, the goal is the same in reinforcement learning:maximize the expected sum of rewards. reinforcement learning differs from “just solving anmdp” because the agent is not given the mdp as a problem to solve; the agent is in themdp. it may not know the transition model or the reward function, and it has to act in orderto learn more. imagine playing a new game whose rules you don’t know; after a hundred orso moves, the referee tells you “you lose.” that is reinforcement learning in a nutshell.",reinforcement learning
chapter22_3,"performance of a greedy adp agent that executes the action recommended by the optimal policy for thelearned model. (a) the root mean square (rms) error averaged across all nine nonterminal squares andthe policy loss in (1, 1). we see that the policy converges quickly, after just eight trials, to a suboptimalpolicy with a loss of 0.235. (b) the suboptimal policy to which the greedy agent converges in thisparticular sequence of trials. notice the down action in (1, 2).",greedy agent
chapter22_3,"the first approach, bayesian reinforcement learning, assumes a prior probability p(h) overhypotheses h about what the true model is; the posterior probability p(h|e) is obtained inthe usual way by bayes’ rule given the observations to date. then, if the agent has decidedto stop learning, the optimal policy is the one that gives the highest expected utility. let u7be the expected utility, averaged over all possible start states, obtained by executing policy 2in model h. then we have",bayesian reinforcement learning
chapter22_3,"we start with the simple case of a fully observable environment with a small number ofactions and states, in which an agent already has a fixed policy 7(s) that determines itsactions. the agent is trying to learn the utility function u""(s)—the expected total discountedreward if policy 7 is executed beginning in state s. we call this a passive learning agent.",passive learning agent
chapter22_3,"we begin in section 22.218 with passive reinforcement learning, where the agent's policy isfixed and the task is to learn the utilities of states (or of state—action pairs); this could alsoinvolve learning a model of the environment. (an understanding of markov decisionprocesses, as described in chapter 1714, is essential for this section.) section 22.38 coversactive reinforcement learning, where the agent must also figure out what to do. theprincipal issue is exploration: an agent must experience as much as possible of itsenvironment in order to learn how to behave in it. section 22.410 discusses how an agentcan use inductive learning (including deep learning methods) to learn much faster from itsexperiences. we also discuss other approaches that can help scale up rl to solve realproblems, including providing intermediate pseudorewards to guide the learner andorganizing behavior into a hierarchy of actions. section 22.519 covers methods for policysearch. in section 22.6, we explore apprenticeship learning: training a learning agentusing demonstrations rather than reward signals. finally, section 22.718 reports on",passive reinforcement learning
chapter22_3,"(according to my estimates)?” sarsa is an on-policy algorithm: it learns q-values thatanswer the question “what would this action be worth in this state, assuming i stick with mpolicy?” q-learning is more flexible than sarsa, in the sense that a q-learning agent canlearn how to behave well when under the control of a wide variety of exploration policies.on the other hand, sarsa is appropriate if the overall policy is even partly controlled byother agents or programs, in which case it is better to learn a q-function for what willactually happen rather than what would happen if the agent got to pick estimated bestactions. both q-learning and sarsa learn the optimal policy for the 4 x 3 world, but theydo so at a much slower rate than the adp agent. this is because the local updates do notenforce consistency among all the o-values via the model.",on-policy
chapter22_3,"the second approach, derived from robust control theory, allows for a set of possiblemodels h without assigning probabilities to them, and defines an optimal robust policy «one that gives the best outcome in the worst case over h:",robust control theory
chapter12_1,"variables in probability theory are called random variables, and their names begin with anuppercase letter. thus, in the dice example, total and die; are random variables. everyrandom variable is a function that maps from the domain of possible worlds 2 to somerange—the set of possible values it can take on. the range of total for two dice is the set{2, ..., 12} and the range of die; is {1, ..., 6}. names for values are always lowercase, so wmight write >, p(x = x) to sum over the values of x. a boolean random variable has therange {true, false}. for example, the proposition that doubles are rolled can be written asdoubles = true. (an alternative range for boolean variables is the set {0, 1}, in which casethe variable is said to have a bernoulli distribution.) by convention, propositions of theform a = true are abbreviated simply as a, while a = false is abbreviated as —a. (the usesof doubles, cavity, and toothache in the preceding section are abbreviations of this kind.)","bernoulli, random variable, range"
chapter12_1,ranges can be sets of arbitrary tokens. we might choose the range of age to be the setedumomiiadoen ntti ana tia-rance:nf uw eontheswmicht he: feun. sedm: dinad ennnk uhean:,range
chapter12_1,"practical ignorance: even if we know all the rules, we might be uncertain abouta particular patient because not all the necessary tests have been or can be run.",practical ignorance
chapter12_1,laziness: it is too much work to list the complete set of antecedents or consequentsneeded to ensure an exceptionless rule and too hard to use such rules.,laziness
chapter12_1,theoretical ignorance: medical science has no complete theory for the domain.,theoretical ignorance
chapter12_1,"there has been endless debate over the source and status of probability numbers. thefrequentist position is that the numbers can come only from experiments: if we test 100people and find that 10 of them have a cavity, then we can say that the probability of acavity is approximately 0.1. in this view, the assertion “the probability of a cavity is 0.1”means that 0.1 is the fraction that would be observed in the limit of infinitely many samplefrom any finite sample, we can estimate the true fraction and also calculate how accurateait eehmatea ic heals tr ha:",frequentist
chapter12_1,"like logical assertions, probabilistic assertions are about possible worlds. whereas logicalassertions say which possible worlds are strictly ruled out (all those in which the assertion isfalse), probabilistic assertions talk about how probable the various worlds are. in probabilitytheory, the set of all possible worlds is called the sample space. the possible worlds aremutually exclusive and exhaustive—two possible worlds cannot both be the case, and onepossible world must be the case. for example, if we are about to roll two (distinguishable)dice, there are 36 possible worlds to consider: (1, 1), (1, 2), ..., (6, 6). the greek letter 0(uppercase omega) is used to refer to the sample space, and w (lowercase omega) refers toelements of the space, that is, particular possible worlds.",sample space
chapter12_1,"it is important to understand that p(cavity) = 0.2 is still valid after toothache is observed;just isn’t especially useful. when making decisions, an agent needs to condition on all theevidence it has observed. it is also important to understand the difference betweenconditioning and logical implication. the assertion that p(cavity | toothache) = 0.6 does nmean “whenever toothache is true, conclude that cavity is true with probability 0.6” rathe",evidence
chapter12_1,"ve nave seen problem-solving and logical agents handle uncertainty dy keeping track of a belief state—a representation of the set of all possible world states that it might be in—and generating a contingency plan that handles every possible eventuality that its sensors may report during execution. this approach works on simple problems, but it has drawbacks:",uncertainty
,"practical ignorance: even if we know all the rules, we might be uncertain about a particular patient because not all the necessary tests have been or can be run.",practical ignorance
,laziness: it is too much work to list the complete set of antecedents or consequents needed to ensure an exceptionless rule and too hard to use such rules.,laziness
,"the connection between toothaches and cavities is not a strict logical consequence in eithe direction. this is typical of the medical domain, as well as most other judgmental domains: law, business, design, automobile repair, gardening, dating, and so on. the agent's knowledge can at best provide only a degree of belief in the relevant sentences. our main tool for dealing with degrees of belief is probability theory. in the terminology of section 8.14 the ontological commitments of logic and probability theory are the same—that the world is composed of facts that do or do not hold in any particular case—but the epistemological commitments are different: a logical agent believes each sentence to be true or false or has no opinion, whereas a probabilistic agent may have a numerical degree at haliaf hatwaan (fae cantancbe that are gartainie tales) and 1 (eartainle tee).","degree of belief, probability theory"
,"nonetheless, in some sense ago is in fact the right thing to do. what do we mean by this? as we discussed in chapter 212, we mean that out of all the plans that could be executed, ago is expected to maximize the agent's performance measure (where the expectation is relative to the agent’s knowledge about the environment). the performance measure includes getting to the airport in time for the flight, avoiding a long, unproductive wait at the airport, and avoiding speeding tickets along the way. the agent’s knowledge cannot guarantee any of these outcomes for ago, but it can provide some degree of belief that they will be achieved. other plans, such as ajo, might increase the agent's belief that it will get to the airport on time, but also increase the likelihood of a long, boring wait. the right thing to do—the rational decision—therefore depends on both the relative importance of various goals and the likelihood that, and degree to which, they will be achieved. the remainder of this section hones these ideas, in preparation for the development of the general theories of uncertain reasoning and rational decisions that we present in this and subsequent chapters.","degree of belief, outcome"
,"the utility of a state is relative to an agent. for example, the utility of a state in which white has checkmated black in a game of chess is obviously high for the agent playing white, but low for the agent playing black. but we can’t go strictly by the scores of 1, 1/2, and 0 that are dictated by the rules of tournament chess—some players (including the authors) might be thrilled with a draw against the world champion, whereas other players (including the former world champion) might not. there is no accounting for taste or preferences: you might think that an agent who prefers jalapefio bubble-gum ice cream to chocolate chip is odd, but you could not say the agent is irrational. a utility function can account for any set of preferences—quirky or typical, noble or perverse. note that utilities can account for altruism, simply by including the welfare of others as one of the factors.",preference
,"to make such choices, an agent must first have preferences among the different possible outcomes of the various plans. an outcome is a completely specified state, including such factors as whether the agent arrives on time and the length of the wait at the airport. we use utility theory to represent preferences and reason quantitatively with them. (the term utility is used here in the sense of “the quality of being useful, ” not in the sense of the electric company or water works.) utility theory says that every state (or state sequence) has a degree of usefulness, or utility, to an agent and that the agent will prefer states with highe1","outcome, utility theory, preference"
,"the fundamental idea of decision theory is that an agent is rational if and only if it chooses th action that yields the highest expected utility, averaged over all the possible outcomes of the action this is called the principle of maximum expected utility (meu). here, “expected” means the “average, ” or “statistical mean” of the outcome utilities, weighted by the probability of the outcome. we saw this principle in action in chapter 5'8 when we touched briefly on optimal decisions in backgammon; it is in fact a completely general principle for single- agent decision making.","decision theory, maximum expected utility (meu)"
,"figure 12.1 sketches the structure of an agent that uses decision theory to select actions. the agent is identical, at an abstract level, to the agents described in chapters 419 and 70 that maintain a belief state reflecting the history of percepts to date. the primary difference is that the decision-theoretic agent's belief state represents not just the possibilities for world states but also their probabilities. given the belief state and some knowledge of the effects of actions, the agent can make probabilistic predictions of action outcomes and hence select the action with the highest expected utility.",decision theory
,"like logical assertions, probabilistic assertions are about possible worlds. whereas logical assertions say which possible worlds are strictly ruled out (all those in which the assertion is false), probabilistic assertions talk about how probable the various worlds are. in probability theory, the set of all possible worlds is called the sample space. the possible worlds are mutually exclusive and exhaustive—two possible worlds cannot both be the case, and one possible world must be the case. for example, if we are about to roll two (distinguishable) dice, there are 36 possible worlds to consider: (1, 1), (1, 2), ..., (6, 6). the greek letter 0 (uppercase omega) is used to refer to the sample space, and w (lowercase omega) refers to elements of the space, that is, particular possible worlds.",sample space
slp_4,"another thing we might want to know about a text is the language it’s written in. texts on social media, for example, can be in any number of languages and we'll need to apply different processing. the task of language id is thus the first step in most language processing pipelines. related text classification tasks like au- thorship attribution— determining a text’s author— are also relevant to the digital hiimanitiec encial ecriencreac and farancic linonicticre",language id
slp_4,"many language processing tasks involve classification, although luckily our classes are much easier to define than those of borges. in this chapter we introduce the naive bayes algorithm and apply it to text categorization, the task of assigning a label or category to an entire text or document.",categorization
slp_4,"spam detection is another important commercial application, the binary clas sification task of assigning an email to one of the two classes spam or not-spam many lexical and other features can be used to perform this classification. for ex ample you might quite reasonably be suspicious of an email containing phrases lik« “online pharmaceutical” or “without any cost” or “dear winner’.",spam detection
slp_4,"in this section we introduce the multinomial naive bayes classifier, so called be- cause it is a bayesian classifier that makes a simplifying (naive) assumption about",naive bayes classifier
slp_4,"rules can be fragile, however, as situations or data change over time, and for some tasks humans aren’t necessarily good at coming up with the rules. most cases of classification in language processing are instead done via supervised machine learning, and this will be the subject of the remainder of this chapter. in supervised learning, we have a data set of input observations, each associated with some correct output (a ‘supervision signal’). the goal of the algorithm is to learn how to map from a new observation to a correct output.",supervised machine learning
slp_4,"the intuition of the classifier is shown in fig. 4.1. we represent a text documen as if it were a bag-of-words, that is, an unordered set of words with their positior ignored, keeping only their frequency in the document. in the example in the figure instead of representing the word order in all the phrases like “i love this movie” anc “t would recommend it”, we simply note that the word j occurred 5 times in the entire excerpt, the word it 6 times, the words love, recommend, and movie once, anc co on",bag-of-words
slp_4,this idea of bayesian inference has been known since the work of bayes (1763) and was first applied to text classification by mosteller and wallace (1964). th intuition of bayesian classification is to use bayes’ rule to transform eq. 4.1 int other probabilities that have some useful properties. bayes’ rule is presented i eg. 4.2; it gives us a way to break down any conditional probability p(x|y) int three other probabilities:,bayesian inference
slp_4,"we call naive bayes a generative model because we can read eq. 4.4 as stating a kind of implicit assumption about how a document is generated: first a class is sampled from p(c), and then the words are generated by sampling from p(d|c). (in fact we could imagine generating artificial documents, or at least their word counts, by following this process). we’ll say more about this intuition of generative models in chanter 5.",assumption
slp_4,"by considering features in log space, eq. 4.10 computes the predicted class as a | ear function of input features. classifiers that use a linear combination of the inpt to make a classification decision —like naive bayes and also logistic regression are called linear classifiers.",linear
slp_4,se es se reet ee ns rn meee ee et eee: what do we do about words that occur in our test data but are not in our vocab- ulary at all because they did not occur in any training document in any class? the solution for such unknown words is to ignore them—remove them from the test document and not include any probability for them at all.,unknown word
slp_4,"seaululiiuiieigl malve dayus ul meal y lined. ll valid it uses lo salle 1a. “. 1) caco that for each document we remove all duplicate words before concatenating them into the single big document. fig. 4.3 shows an example in which a set of four documents (shortened and text-normalized for this example) are remapped to binary, with the modified counts shown in the table on the right. the example is worked without add-1 smoothing to make the differences clearer. note that the results counts need not be 1; the word great has a count of 2 even for binary nb, because it appears in multiple documents.",binary nb
slp_4,"in the previous section we pointed out that naive bayes doesn’t require that our classifier use all the words in the training data as features. in fact features in naive bayes can express any property of the input text we want. consider the task of spam detection, deciding if a particular piece of email is",spam detection
slp_4,"and negative word features from sentiment lexicons, lists of words that are pre annotated with positive or negative sentiment. four popular lexicons are the genera inquirer (stone et al., 1966), liwc (pennebaker et al., 2007), the opinion lexico",sentiment lexicons
slp_4,"to introduce the methods for evaluating text classification, let’s first consider some simple binary detection tasks. for example, in spam detection, our goal is to label every text as being in the spam category (“positive”) or not in the spam category (“negative”). for each item (email document) we therefore need to know whether our system called it spam or not. we also need to know whether the email is actually spam or not, i.e. the human-defined labels for each document that we are trying to match. we will refer to these human labels as the gold labels.",gold labels
 slp_4,"in both cases, we need a metric for knowing how well our spam detector (or pie-tweet-detector) is doing. to evaluate any system for detecting things, we start by building a confusion matrix like the one shown in fig. 4.4. a confusion matrix is a table for visualizing how an algorithm performs with respect to the human gold labels, using two dimensions (system output and gold labels), and each cell labeling a set of possible outcomes. in the spam detection case, for example, true positives are documents that are indeed spam (indicated by human-created gold labels) that our system correctly said were spam. false negatives are documents that are indeed spam but our system incorrectly labeled as non-spam.",confusion matrix
slp_4,f-measure comes from a weighted harmonic mean of precision and recall. th harmonic mean of a set of numbers is the reciprocal of the arithmetic mean of recip tncale:,"precision, f-measure"
slp_4,recall measures the percentage of items actually present in the input that were correctly identified by the system. recall is defined as,recall
slp_4,"in cross-validation, we choose a number k, and partition our data into k disjoint subsets called folds. now we choose one of those k folds as a test set, train our classifier on the remaining k — | folds, and then compute the error rate on the test set. then we repeat with another fold as the test set, again training on the other k — 1 folds. we do this sampling process k times and average the test set error rate from these k runs to get an average error rate. if we choose k = 10, we would train 10 different models (each on 90% of our data), test the model 10 times, and average these 10 values. this is called 10-fold cross-validation.",folds
slp_4,"we would like to know if 6(x) > 0, meaning that our logistic regression classifier has a higher f, than our naive bayes classifier on x. 5(x) is called the effect size; a bigger 6 means that a seems to be way better than b; a small 5 means a seems to be only a little better.",effect size
slp_4,"so in our example, this p-value is the probability that we would see 5(x) assuming a is not better than b. if 5(x) is huge (let’s say a has a very respectable f; of .9 and b has a terrible f; of only .2 on x), we might be surprised, since that would be extremely unlikely to occur if ho were in fact true, and so the p-value would be low",p-value
slp_4,"we do this by creating a random variable x ranging over all test sets. now we ask how likely is it, if the null hypothesis ho was correct, that among these test sets we would encounter the value of 5(x) that we found. we formalize this likelihood as the p-value: the probability, assuming the null hypothesis hp is true, of seeing the 5(x) that we saw or one even greater",null hypothesis
slp_4,"the bootstrap test (efron and tibshirani, 1993) can apply to any metric; from pre cision, recall, or fl to the bleu metric used in machine translation. the wor bootstrapping refers to repeatedly drawing large numbers of smaller samples with replacement (called bootstrap samples) from an original larger sample. the intu ition of the bootstrap test is that we can create many virtual test sets from an observec test set by repeatedly sampling from it. the method only makes the assumption tha the sample is representative of the population.",bootstrapping
slp_4,"inere are two common non-parametric tests used i nla. approximate ran- domization (noreen, 1989) and the bootstrap test. we will describe bootstrap below, showing the paired version of the test, which again is most common in nlp. paired tests are those in which we compare two sets of observations that are aligned: each observation in one set can be paired with an observation in another. this hap- pens naturally when we are comparing the performance of two systems on the same test set; we can pair the performance of system a on an individual observation x; with the performance of system b on the same xj.","paired, bootstrap test"
slp_4,"a very small p-value means that the difference we observed is very unlikely under the null hypothesis, and we can reject the null hypothesis. what counts as very small? it is common to use values like .05 or .01 as the thresholds. a value of .01 means that if the p-value (the probability of observing the 6 we saw assuming hp is true) is less than .01, we reject the null hypothesis and assume that a is indeed better than b. we say that a result (e.g., “a is better than b”) is statistically significant if the 5 we saw has a probability that is below the threshold and we therefore reject this null hypothesis.",statistically significant
slp_4,"example due to biases in the human labelers), by the resources used (like lexicons. or model components like pretrained embeddings), or even by model architecture (like what the model is trained to optimize). while the mitigation of these biases (for example by carefully considering the training data sources) is an important ared of research, we currently don’t have general solutions. for this reason it’s important. when introducing any nlp model, to study these these kinds of factors and make them clear. one way to do this is by releasing a model card (mitchell et al., 2019) for each version of a model. a model card documents a machine learning mode",model card
slp_4,"in this chapter we’ll introduce feedforward networks as classifiers, and also ap- ply them to the simple task of language modeling: assigning probabilities to word sequences and predicting upcoming words. in subsequent chapters we’ ll introduce many other aspects of neural models, such as recurrent neural networks and the transformer (chapter 9), contextual embeddings like bert (chapter 11), and encoder-decoder models and attention (chapter 10).",feedforward
slp_4,"instead, a modern neural network is a network of small computing units, ea of which takes a vector of input values and produces a single output value. in tl chapter we introduce the neural net applied to classification. the architecture \ introduce is called a feedforward network because the computation proceeds it atively from one layer of units to the next. the use of modern neural nets is oft called deep learning, because modern networks are often deep (have many layer",deep learning
slp_4,"at its heart, a neural unit is taking a weighted sum of its inputs, with one addi- tional term in the sum called a bias term. given a set of inputs x)...x, , a unit has a set of corresponding weights w, ...w, and a bias b, so the weighted sum z can be represented as:",bias term
slp_4,"we'll discuss three popular non-linear functions f() below (the sigmoid, the tanh, and the rectified linear unit or relu) but it’s pedagogically convenient to start with the sigmoid function since we saw it in chapter 5:",sigmoid
slp_4,"ded eee eoe re sog ger sen ae aor rr nr finally, instead of using z, a linear function of x, as the output, neural unit apply a non-linear function f to z. we will refer to the output of this function a the activation value for the unit, a. since we are just modeling a single unit, th activation for the node is in fact the final output of the network, which we’ ll general call y. so the value y is defined as:",activation
slp_4,"often it’s more convenient to express this weighted sum using vector notation; recall from linear algebra that a vector is, at heart, just a list or array of numbers. thus we’ll talk about z in terms of a weight vector w, a scalar bias b, and an input vector x, and we’ll replace the sum with the convenient dot product:",vector
,"the simplest activation function, and perhaps the most commonly used, is the rec- tified linear unit, also called the relu, shown in fig. 7.3b. it’s just the same as z when z is positive, and 0 otherwise:",relu
,"these activation functions have different properties that make them useful for differ- ent language applications or network architectures. for example, the tanh function has the nice properties of being smoothly differentiable and mapping outlier values toward the mean. the rectifier function, on the other hand has nice properties that",tanh
,"this example was first shown for the perceptron, which is a very simple neural init that has a binary output and does not have a non-linear activation function. the yutput y of a perceptron is 0 or 1, and is computed as follows (using the same weight v, input x, and bias b as in ea. 7.2):",perceptron
,"result from it being very close to linear. in the sigmoid or tanh functions, very high values of z result in values of y that are saturated, i.e., extremely close to 1, and have derivatives very close to 0. zero derivatives cause problems for learning, because as we’ll see in section 7.6, we’ll train networks by propagating an error signal back- wards, multiplying gradients (partial derivatives) from each layer of the network; gradients that are almost 0 cause the error signal to get smaller and smaller until it is too small to be used for training, a problem called the vanishing gradient problem. rectifiers don’t have this problem, since the derivative of relu for high values of z is | rather than very close to 0.",vanishing gradient
,"fig. 7.5 shows the possible logical inputs (00, 01, 10, and 11) and the line drawn by one possible set of parameters for an and and an or classifier. notice that there is simply no way to draw a line that separates the positive cases of xor (01 and 10) from the negative cases (00 and 11). we say that xor is not a linearly separable function. of course we could draw a boundary with a curve, or some other function, but not a single line.",linearly separable
,"let’s now walk through a slightly more formal presentation of the simplest kind of neural network, the feedforward network. a feedforward network is a multilayer network in which the units are connected with no cycles; the outputs from units in each layer are passed to units in the next higher layer, and no outputs are passed back to lower layers. (in chapter 9 we’ll introduce networks with cycles, called recurrent neural networks.)",feedforward network
,"the hidden layer forming a new representation of the input. (b) shows th representation of the hidden layer, h, compared to the original input representation x in (a notice that the input point [0, 1] has been collapsed with the input point [1, 0], making | ssible to linearly separate the positive and negative cases of xor. after goodfellow et a",hidden layer
,"as a the input layer x is a vector of simple scalar values just as we saw in fig. 7.2. the core of the neural network is the hidden layer h formed of hidden units h;, each of which is a neural unit as described in section 7.1, taking a weighted sum of its inputs and then applying a non-linearity. in the standard architecture, each layer is fully-connected, meaning that each unit in each layer takes as input the outputs from all the units in the previous layer, and there is a link between every pair of units from two adjacent layers. thus each hidden unit sums over all the input units. recall that a cinole hidden nnit hac ac narametere a weicght vectar and ahiac wa",fully-connected
,"let’s see how this happens. like the hidden layer, the output layer has a weigt matrix (let’s call it u), but some models don’t include a bias vector b in the output",output layer
,"‘let’ s introduce some constants to represent the dimensionalities of these vectors and matrices. we’ll refer to the input layer as layer 0 of the network, and have no represent the number of inputs, so x is a vector of real numbers of dimension no, or more formally x € r"", a column vector of dimensionality [no, 1]. let’s call the hidden layer layer 1 and the output layer layer 2. the hidden layer has dimensional- ity nj, so h € r” and also b € r"" (since each hidden unit can take a different bias value). and the weight matrix w has dimensionality w € r""*""®, i.e. [ny , no]. take a moment to convince vourself that the matrix multiplication in eq. 7.8 will","7.8 will, input layer"
,"however, z can’t be the output of the classifier, since it’s a vector of real-valued 1umbers, while what we need for classification is a vector of probabilities. there is 1 convenient function for normalizing a vector of real values, by which we mean sonverting it to a vector that encodes a probability distribution (all the numbers lie xetween 0 and | and sum to 1): the softmax function that we saw on page ?? of chapter 5. more generally for any vector z of dimensionality d, the softmax is jefined ac:",normalizing
,"the need for non-linear activation functions one of the reasons we use non- linear activation functions for each layer in a neural network is that if we did not, the resulting network is exactly equivalent to a single-layer network. let’s see why this is true. imagine the first two layers of such a network of purely linear layers:",the need for non-linear activation functions
,"replacing the bias unit in describing networks, we will often use a slightly sim- plified notation that represents exactly the same function without referring to an ex- plicit bias node b. instead, we add a dummy node ao to each layer whose value will always be 1. thus layer 0, the input layer, will have a dummy node al! = 1, layer 1 will have al}! = 1, and so on. this dummy node still has an associated weight, and that weight represents the bias value b. for example instead of an equation like",replacing the bias unit
,"most neural nlp applications do something different, however. instead of us- ing hand-built human-engineered features as the input to our classifier, we draw on deep learning’s ability to learn features from the data by representing words as embeddings, like the word2vec or glove embeddings we saw in chapter 6. there are various ways to represent an input for classification. one simple baseline is to apply some sort of pooling function to the embeddings of all the words in the in- put. for example, for a text with n input words/tokens w), ..., wn», we can turn the n embeddings e(w1), ..., e(wn) (each of dimensionality d) into a single embedding also of dimensionality d by just summing the embeddings, or by taking their mean obhaos ae astes x",pooling
,"an embedding representation for our input words—is called pretraining. usin; pretrained embedding representations, whether simple static word embeddings lik« word2vec or the much more powerful contextual embeddings we’ll introduce ir chapter 11, is one of the central ideas of deep learning. (it’s also, possible, however to train the word embeddings as part of an nlp task; we’ ll talk about how to do thi: in section 7.7 in the context of the neural language modeling task.)",pretraining
,selecting the embedding vector for word vs by multiplying the embedding . e with a one-hot vector with a | in index 5.,one-hot vector
,"first, we’ll need a loss function that models the distance between the system output and the gold output, and it’s common to use the loss function used for logistic regression, the cross-entropy loss.",cross-entropy
,"the chain rule extends to more than two functions. if computing the derivative of a composite function f(x) = u(v(w(x))), the derivative of f(x) is:",chain rule
,"tuning of hyperparameters is also important. the parameters of a neural network are the weights w and biases b; those are learned by gradient descent. the hyperparameters are things that are chosen by the algorithm designer; optimal values are tuned on a devset rather than by gradient descent learning on the training set. hyperparameters include the learning rate 7, the mini-batch size, the model architecture (the number of layers, the number of hidden nodes per layer, the choice of activation functions), how to regularize, and so on. gradient descent itself also has many architectural anal ti. anal be a ae",hyperparameter
,"for some tasks, it’s ok to freeze the embedding layer e with initial word2vec val- ues. freezing means we use word2vec or some other pretraining algorithm to com- pute the initial embedding matrix e, and then hold it constant while we only modify w, u, and b, i.e., we don’t update e during language model training. however, often we'd like to learn the embeddings simultaneously with training the network. this is useful when the task the network is designed for (like sentiment classification, trans- lation, or parsing) places strong constraints on what makes a good representation fot words.",freeze
,‘various forms of regularization are used to prevent overfitting. one of the most mportant is dropout: randomly dropping some units and their connections from he network during training (hinton et al. 2012. srivastava et al. 2014). tuning,dropout
slp-7,"of the threshold into a bias, a notation we still use (widrow and hoff, 1960). the field of neural networks declined after it was shown that a single perceptron unit was unable to model functions as simple as xor (minsky and papert, 1969). while some small amount of work continued during the next two decades, a major revival for the field didn’t come until the 1980s, when practical tools for building deeper networks like error backpropagation became widespread (rumelhart et al., 1986). during the 1980s a wide variety of neural network and related architec- tures were developed, particularly for applications in psychology and cognitive sci- ence (rumelhart and mcclelland 1986b, mcclelland and elman 1986, rumelhart and mcclelland 1986a, elman 1990), for which the term connectionist or paral-",connectionist
slp-7,"se aee ae ne se ee) oe sd. nr ees: a wt in this chapter we introduce an algorithm that is admirably suited for discovering the link between features or cues and some particular outcome: logistic regression. indeed, logistic regression is one of the most important analytic tools in the social and natural sciences. in natural language processing, logistic regression is the base- line supervised machine learning algorithm for classification, and also has a very close relationship with neural networks. as we will see in chapter 7, a neural net- work can be viewed as a series of logistic regression classifiers stacked on top of each other. thus the classification and machine learning techniques introduced here will pnlav an important role throuchout the hook.",logistic regression
slp-7,"cohiponrchts of a plodadinsuc taachine icar tiie classier., = likc halve daye logistic regression is a probabilistic classifier that makes use of supervised machine learning. machine learning classifiers require a training corpus of m input/output pairs (x ‘ yl), (we’ il use superscripts in parentheses to refer to individual instances in the training set—for sentiment classification each instance might be an individual document to be classified.) a machine learning system for classification then has four comnonentc:",logistic regression
slp-7,"a generative model like naive bayes makes use of this likelihood term, which expresses how to generate the features of a document if we knew it was of class c. by contrast a discriminative model in this text categorization scenario attempts to directly compute p(c|d). perhaps it will learn to assign a high weight to document features that directly improve its ability to discriminate between possible classes, even if it couldn’t generate an example of one of the classes.",discriminative model
slp-7,"in the rest of the book we’ll represent such sums using the dot product notation from linear algebra. the dot product of two vectors a and b, written as a-b is the sum of the products of the corresponding elements of each vector. (notice that we represent vectors using the boldface notation b). thus the following is an equivalent farmationn tag fa 5 9°",dot product
slp-7,"aha ly = ux) is wie provadly wat ie gocutmeht nas ncsauve sschument, logistic regression solves this task by learning, from a training set, a vector of weights and a bias term. each weight w; is a real number, and is associated with one of the input features x;. the weight w; represents how important that input feature is to the classification decision, and can be positive (providing evidence that the in- stance being classified belongs in the positive class) or negative (providing evidence that the instance being classified belongs in the negative class). thus we might expect in a sentiment task the word awesome to have a high positive weight, and abysmal to have a very negative weight. the bias term, also called the intercept, is another real number that’s added to the weighted inputs.",intercept
slp-7,‘the sigmoid function 0(z) = y_ = takes a real value and maps it to the rang 1]. it is nearly linear around 0 but outlier values get squashed toward 0 or 1.,the sigmoid function
slp-7,"sa ee eee ee nema ey me ne re how do we make a decision about which class to apply to a test instance x? for a given x, we say yes if the probability p(y = 1|x) is more than .5, and no otherwise. we call .5 the decision boundary:",decision boundary
slp-7,designing features: features are generally designed by examining the training set with an eye to linguistic intuitions and the linguistic literature on the domain. a careful error analysis on the training set or devset of an early version of a system often provides insights into features.,designing features:
slp-7,"scaling input features: when different input features have extremely differen ranges of values, it’s common to rescale them so they have comparable ranges. we standardize input values by centering them to result in a zero mean and a standarc deviation of one (this transformation is sometimes called the z-score). that is, if uw is the mean of the values of feature x; across the m observations in the input dataset and 0; is the standard deviation of the values of features x; across the input dataset we can renlace each feature x: by a new feature x’ comnuted as follows:",standardize
slp-7,"for some tasks it is especially helpful to build complex features that are combi- nations of more primitive features. we saw such a feature for period disambiguation above, where a period on the word st. was less likely to be the end of the sentence if the previous word was capitalized. for logistic regression and naive bayes these combination features or feature interactions have to be designed by hand.","logistic regression, feature interactions"
slp-7,"in such cases we use multinomial logistic regression, also called softmax re- gression (in older nlp literature you will sometimes see the name maxent classi- fier). in multinomial logistic regression we want to label each observation with a class k from a set of k classes, under the stipulation that only one of these classes is the correct one (sometimes called hard classification; an observation can not be in multiple classes). let’s use the following representation: the output y for each input x will be a vector of length k. if class c is the correct class, we’ll set y. = 1, and set all the other elements of y to be 0, i-e., y. = 1 andy; =0 vj #c. a vector like this y, with one value=1 and the rest 0, is called a one-hot vector. the job of the classifier is produce an estimate vector §. for each class k, the value j, will be the classifier’s estimate of the probability p(y, = 1|x).","logistic regression, multinomial logistic regression"
slp-7,"the multinomial logistic classifier uses a generalization of the sigmoid, called the softmax function, to compute p(y; = 1|x). the softmax function takes a vector z= (z), 2, ..., zx]| of k arbitrary values and maps them to a probability distribution, with each value in the range (0, 1), and all the values summing to 1. like the sigmoid, it is an exponential function.",softmax
slp-7,"features in multinomial logistic regression function similarly to binary logistic re- gression, with the difference mentioned above that we’ll need separate weight vec- tors and biases for each of the k classes. recall our binary exclamation point feature",logistic regression
slp-7,"binary versus multinomial logistic regression. binary logistic regression uses a single weight vector w, and has a scalar output ¥. in multinomial logistic regression we have k separate weight vectors corresponding to the k classes, all packed into a single weight matrix w and a vector antnit 7","weight vector, output"
slp-7,"how shall we find the minimum of this (or any) loss function? gradient descent is « method that finds a minimum of a function by figuring out in which direction (in the space of the parameters @) the function’s slope is rising the most steeply, and movins in the opposite direction. the intuition is that if you are hiking in a canyon and tryins to descend most quickly down to the river at the bottom, you might look arounc yourself 360 degrees, find the direction where the ground is sloping the steepest and walk downhill in that direction.","gradient descent, gradient descent"
slp-7,"ine lf bw), or the amount to move in gradient gescent is the value of the slope alf ( x;w), y) weighted by a learning rate n. a higher (faster) learning rate means that we should move w more on each step. the change we make in our parameter is the learning rate times the gradient (or the slope, in our single-variable example):",learning rate
slp-7,"tion has just one minimum; there are no local minima to get stuck in, so gradient descent starting from any point is guaranteed to find the minimum. (by contrast, the loss for multi-layer neural networks is non-convex, and gradient descent may get stuck in local minima for neural network training and never find the global opti- mum.)",gradient
slp-7,"the learning rate 7 is a hyperparameter that must be adjusted. if it’s too high, the learner will take steps that are too large, overshooting the minimum of the loss function. if it’s too low, the learner will take steps that are too small, and take too long to get to the minimum. it is common to start with a higher learning rate and then slowly decrease it, so that it is a function of the iteration k of training; the notation mn, can be used to mean the value of the learning rate at iteration k.",hyperparameter
slp-7,"stochastic gradient descent is called stochastic because it chooses a single random example at a time, moving the weights so as to improve performance on that single example. that can result in very choppy movements, so it’s common to compute the gradient over batches of training instances rather than a single instance.",gradient descent
slp-7,"a compromise is mini-batch training: we train on a group of m examples (per- haps 512, or 1024) that is less than the whole dataset. (if m is the size of the dataset, then we are doing batch gradient descent; if m = 1, we are back to doing stochas- tic gradient descent). mini-batch training also has the advantage of computational efficiency. the mini-batches can easily be vectorized, choosing the size of the mini- batch based on the computational resources. this allows us to process all the exam- ples in one mini-batch in parallel and then accumulate the loss, something that’s not possible with individual or batch training.","batch training, mini-batch"
slp-7,"there is a problem with learning weights that make the model perfectly match the training data. if a feature is perfectly predictive of the outcome because it happens to only occur in one class, it will be assigned a very high weight. the weights for features will attempt to perfectly fit details of the training set, in fact too perfectly, modeling noisy factors that just accidentally correlate with the class. this problem is called overfitting. a good model should be able to generalize well from the training data to the unseen test set, but a model that overfits will have poor generalization. to avoid overfitting, a new regularization term r(@) is added to the objective","regularization, regularization"
slp-7,"the new regularization term r(@) is used to penalize large weights. thus a setting of the weights that matches the training data perfectly— but uses many weights with high values to do so—will be penalized more than a setting that matches the data < little less well, but does so using smaller weights. there are two common ways to compute this regularization term r(@). l2 regularization is a quadratic function of the weight values, named because it uses the (square of the) l2 norm of the weight values. the l2 norm, ||6||2, is the same as the euclidean distance of the vector 6 from the origin. if @ consists of n weights, then:",regularization
slp-7,the loss function for multinomial logistic regression generalizes the loss function for binary logistic regression from 2 to k classes. recall that that the cross-entropy loss for binary logistic regression (repeated from eq. 5.22) is:,logistic regression
slp-7,"often we want to know more than just the correct classification of an observation. we want to know why the classifier made the decision it did. that is, we want out decision to be interpretable. interpretability can be hard to define strictly, but the core idea is that as humans we should know why our algorithms reach the conclu- sions they do. because the features to logistic regression are often human-designed. one way to understand a classifier’s decision is to understand the role each feature plays in the decision. logistic regression can be combined with statistical tests (the likelihood ratio test, or the wald test); investigating whether a particular feature is significant by one of these tests, or inspecting its magnitude (how large is the weight",interpretable
slp-7,"in this section we give the derivation of the gradient of the cross-entropy loss func- tion lcg for logistic regression. let’s start with some quick calculus refreshers. first, the derivative of in(x):",logistic regression
slp-7,"finally, the chain rule of derivatives. suppose we are computing the derivative of a composite function f(x) = u(v(x)). the derivative of f(x) is the derivative of u(x) with respect to v(x) times the derivative of v(x) with respect to x",chain rule
slp-7,"speech processing, both of which had made use of regression, and both of which lent many other statistical techniques to nlp. indeed a very early use of logistic regression for document routing was one of the first nlp applications to use (lsi) embeddings as word representations (schiitze et al., 1995).",logistic regression
,"to visualize how perplexity can be computed as a function of the probabilities our lm will compute for each new word, we can use the chain rule to expand the com- putation of probability of the test set:",perplexity
,predict the next word (rather than feeding the model its best case from the previou: time step) is called teacher forcing.,teacher forcing
,"the final layer matrix v provides a way to score the likelihood of each word in the vocabulary given the evidence present in the final hidden layer of the network through the calculation of vh. this results in a dimensionality |v| x dy. that is, the rows of v provide a second set of learned word embeddings that capture relevant aspects of word meaning and function. this leads to an obvious question — is it even necessary to have both? weight tying is a method that dispenses with this redundancy and simply uses a single set of embeddings at the input and softmax layers. that is, we dispense with v and use £ in both the start and end of the computation.","weight tying, embeddings"
,"sion to depend on information from hundreds of words in the past. the transfor offers new mechanisms (self-attention and positional encodings) that help repre time and help focus on how words relate to each other over long distances. v see how to apply both models to the task of language modeling, to sequence n eling tasks like part-of-speech tagging, and to text classification tasks like sentin analysis.","words, self-attention"
,"in sequence labeling, the network’s task is to assign a label chosen from a smal fixed set of labels to each element of a sequence, like the part-of-speech tagging anc named entity recognition tasks from chapter 8. in an rnn approach to sequence labeling, inputs are word embeddings and the outputs are tag probabilities generatec by a softmax layer over the given tagset, as illustrated in fig. 9.7.",embeddings
,"ihe weights in the network are adjusted to minimize the average ch loss over the training sequence via gradient descent. fig. 9.6 illustrates this training regimen. careful readers may have noticed that the input embedding matrix e and the final layer matrix v, which feeds the output softmax, are quite similar. the columns of e represent the word embeddings for each word in the vocabulary learned during the training process with the goal that words that have similar meaning and function will have similar embeddings. and, since the length of these embeddings corresponds to the size of the hidden layer dj, the shape of the embedding matrix e is d, x |v].","words, embeddings"
,"recall that we evaluate language models by examining how well they predict unseen text. intuitively, good models are those that assign higher probabilities to unseen data (are less surprised when encountering the new words).",words
,rated = part-of-speech tagging as sequence labeling with a simple rnn. pre-traine word embeddings serve as inputs and a softmax layer provides a probability distribution ove the part-of-speech tags as output at each time step.,embeddings
,"simplified sketch of a feedforward neural language model moving through a text. at each time step ¢ the network converts n context words, each to a d-dimensional embedding, and concatenates the n embeddings together to get the nd x 1 unit input vector x for the network. the output of the network is a probability distribution over the vocabulary representing the model’s belief with respect to each word being the next possible word.","words, embeddings"
,"in this figure, the inputs at each time step are pre-trained word embeddings cor- responding to the input tokens. the rnn block is an abstraction that represents an unrolled simple recurrent network consisting of an input layer, hidden layer, and output layer at each time step, as well as the shared u, v and w weight matrices that comprise the network. the outputs of the network at each time step represent the distribution over the pos tagset generated by a softmax layer.",embeddings
,"another option, instead of using just the last token h, to represent the whole sequence, is to use some sort of pooling function of all the hidden states h; for each word i in the sequence. for example, we can create a representation that pools all the n hidden states by taking their element-wise mean:",pooling
,"stacked rnns generally outperform single-layer networks. one reason for thi success seems to be that the network induces representations at differing levels o abstraction across layers. just as the early stages of the human visual system detec edges that are then used for finding larger regions and shapes, the initial layers o stacked networks can induce representations that serve as useful abstractions fo further layers—representations that might prove difficult to induce in a single rnn the optimal number of stacked rnns is specific to each application and to eac training set. however, as the number of stacks is increased the training costs ris quickly.",stacked rnns
,"in our examples thus far, the inputs to our rnns have consisted of sequences o word or character embeddings (vectors) and the outputs have been vectors useful fo: predicting words, tags or sequence labels. however, nothing prevents us from usin; the entire sequence of outputs from one rnn as an input sequence to another one stacked rnns consist of multiple networks where the output of one layer serves a: the input to a subsequent layer, as shown in fig. 9.10.",stacked rnns
,"the first gate we’ll consider is the forget gate. the purpose of this gate to delet information from the context that is no longer needed. the forget gate computes < weighted sum of the previous state’s hidden layer and the current input and passe: that through a sigmoid. this mask is then multiplied element-wise by the contex vector to remove the information from context that is no longer required. element wise multiplication of two vectors (represented by the operator ©, and sometime: called the hadamard product) is the vector of the same dimension as the two inpu vectors, where each element i is the product of element i in the two input vectors:",forget gate
,the final gate we’ ll use is the output gate which is used to decide what informa- tion is required for the current hidden state (as opposed to what information needs to be preserved for future decisions).,output gate
,"transformers map sequences of input vectors (x1, ..., x, ) to sequences of out- put vectors (y1, ..., yn) of the same length. transformers are made up of stacks of transformer blocks, which are multilayer networks made by combining simple lin- ear layers, feedforward networks, and self-attention layers, the key innovation of transformers. self-attention allows a network to directly extract and use information from arbitrarily large contexts without the need to pass it through intermediate re- current connections as in rnns. we’ll start by describing how self-attention works and then return to how it fits into larger transformer blocks.","self-attention, transformers"
,"fig. 9.15 illustrates the flow of information in a single causal, or backward look. ing, self-attention layer. as with the overall transformer, a self-attention layer map: input sequences (x1, ..., x, ) to output sequences of the same length (y1, ..., ¥n). wher processing each item in the input, the model has access to all of the inputs up to anc including the one under consideration, but no access to information about input: beyond the current one. in addition, the computation performed for each item i:",self-attention
,"while the addition of gates allows lstms to handle more distant information thar rnns, they don’t completely solve the underlying problem: passing informatior through an extended series of recurrent connections leads to information loss and difficulties in training. moreover, the inherently sequential nature of recurrent net- works makes it hard to do computation in parallel. these considerations led to the development of transformers — an approach to sequence processing that eliminates recurrent connections and returns to architectures reminiscent of the fully connected networks described earlier in chapter 7.",transformers
,"layer normalization (or layer norm) is one of many forms of normalization that can be used to improve training performance in deep neural networks by keeping the values of a hidden layer in a range that facilitates gradient-based training. layet norm is a variation of the standard score, or z-score, from statistics applied to a single hidden layer. the first step in layer normalization is to calculate the mean, lu. and standard deviation, o, over the elements of the vector to be normalized. given a hidden layer with dimensionality d;, , these values are calculated as follows.",layer norm
,"transformers can also be used for sequence labeling tasks (like part-of-speech tag ging or named entity tagging) and sequence classification tasks (like sentiment clas. sification), as we’ll see in detail in chapter 11. just to give a preview, however, we don’t directly train a raw transformer on these tasks. instead, we use a technique called pretraining, in which we first train a transformer language model on a largé corpus of text, in a normal self-supervised way, and only afterwards add a linear o: feedforward layer on top that we finetune on a smaller dataset hand-labeled wit nart-of-sneech or sentiment labels. pretrainine on larce amounts of data via the","finetune, pretraining"
,"the role of context is also important in the similarity of a less biological kind of organism: the word. words that occur in similar contexts tend to have similar meanings. this link between similarity in how words are distributed and similarity in what they mean is called the distributional hypothesis. the hypothesis was first formulated in the 1950s by linguists like joos (1950), harris (1954), and firth",distributional hypothesis
,synonymy one important component of word meaning is the relationship be- tween word senses. for example when one word has a sense whose meaning is,synonymy
,"here the form mouse is the lemma, also called the citation form. the form mouse would also be the lemma for the word mice; dictionaries don’t have separate definitions for inflected forms like mice. similarly sing is the lemma for sing, sang, sung. in many languages the infinitive form is used as the lemma for the verb, so spanish dormir “to sleep” is the lemma for duermes “you sleep”. the specific forms sung or carpets or sing or duermes are called wordforms.","wordform, citation form, lemma"
,"in this section we summarize some of these desiderata, drawing on results in the linguistic study of word meaning, which is called lexical semantics; we’|l return to and expand on this list in chapter 18 and chapter 10.",lexical semantics
,"word similarity while words don’t have many synonyms, most words do have lots of similar words. cat is not a synonym of dog, but cats and dogs are certainly similar words. in moving from synonymy to similarity, it will be useful to shift fron talking about relations between word senses (like synonymy) to relations betweer words (like similarity). dealing with words avoids having to commit to a particula: representation of word senses, which will turn out to simplify our task.",similarity
,one common kind of relatedness between words is if they belong to the same semantic field. a semantic field is a set of words which cover a particular semantic,semantic field
,"tenets of semanucs, called he principle of contrast (glrard 1/16, dreal 167/, clark 1987), states that a difference in linguistic form is always associated with some dif- ference in meaning. for example, the word h20 is used in scientific contexts and would be inappropriate in a hiking guide—water would be more appropriate— and this genre difference is part of the meaning of the word. in practice, the word syn- onyn is therefore used to describe a relationship of approximate or rough synonymy.",principle of contrast
,"semantic frames and roles closely related to semantic fields is the idea of a semantic frame. a semantic frame is a set of words that denote perspectives o1 participants in a particular type of event. a commercial transaction, for example, is a kind of event in which one entity trades money to another entity in return fot some good or service, after which the good changes hands or perhaps the service is performed. this event can be encoded lexically by using verbs like buy (the event from the perspective of the buyer), se// (from the perspective of the seller), pay (focusing on the monetary aspect), or nouns like buyer. frames have semantic roles (like buyer, seller, goods, money), and words in a sentence can take on these roles.","semantic frame, semantic frames and roles"
,"connotation finally, words have affective meanings or connotations. the word connotation has different meanings in different fields, but here we use it to mean the aspects of a word’s meaning that are related to a writer or reader’s emotions, sentiment, opinions, or evaluations. for example some words have positive conno- tations (happy) while others have negative connotations (sad). even words whose meanings are similar in other ways can vary in connotation; consider the difference in connotations between fake, knockoff, forgery, on the one hand, and copy, replica, reproduction on the other, or innocent (positive connotation) and naive (negative connotation). some words describe positive evaluation (great, love) and others neg- ative evaluation (terrible, hate). positive or negative evaluation language is called sentiment, as we saw in chapter 4, and word sentiment plays a role in important tasks like sentiment analysis, stance detection, and applications of nlp to the lan- guage of politics and consumer reviews.","connotations, sentiment, connotation"
,"domain and bear structured relations with each other. for example, words might be related by being in the semantic field of hospitals (surgeon, scalpel, nurse, anes- thetic, hospital), restaurants (waiter, menu, plate, food, chef), or houses (door, roof, kitchen, family, bed). semantic fields are also related to topic models, like latent dirichlet allocation, lda, which apply unsupervised learning on large sets of texts to induce sets of associated words from text. semantic fields and topic models are",topic models
,"vectors semantics is the standard way to represent word meaning in nlp, helpin; us model many of the aspects of word meaning we saw in the previous section. th« roots of the model lie in the 1950s when two big ideas converged: osgood’s 195’ idea mentioned above to use a point in three-dimensional space to represent thé connotation of a word, and the proposal by linguists like joos (1950), harris (1954) and firth (1957) to define the meanino of a word hv its distri tian in lanonaos",semantics
,"the idea of vector semantics is to represent a word as a point in a multidimen- sional semantic space that is derived (in ways we’ll see) from the distributions of word neighbors. vectors for representing words are called embeddings (although the term is sometimes more strictly applied only to dense vectors like word2vec (section 6.8), rather than sparse tf-idf or ppmi vectors (section 6.3-section 6.6)). the word “embedding” derives from its mathematical sense as a mapping from one space or structure to another, although the meaning has shifted; see the end of the chapter.","vector semantics, embeddings, vector semantics"
,"vector or distributional models of meaning are generally based on a co-occurrence matrix, a way of representing how often words co-occur. we’ ll look at two popular matrices: the term-document matrix and the term-term matrix.",term-document matrix
,"ee meet neuen, serena ce rae enc a eee rr rs ar avr er rrr na a: term-document matrices were originally defined as a means of finding similar documents for the task of document information retrieval. two documents that are similar will tend to have similar words, and if two documents have similar words their column vectors will tend to be similar. the vectors for the comedies as you like it [1, 114, 36, 20] and twelfth night [0, 80, 58, 15] look a lot more like each other (more fools and wit than battles) than they look like julius caesar [7, 62, 1, 2] or henry v [13, 89, 4, 3]. this is clear with the raw numbers; in the first dimension (battle) the comedies have low numbers and the others have high numbers, and we can see it visually in fig. 6.4; we’ll see very shortly how to quantify this intuition more formally.",vector
,"nes sits oe a wg: bor ste ass the ordering of the numbers in a vector space indicates different meaningful di- mensions on which documents vary. thus the first dimension for both these vectors corresponds to the number of times the word battle occurs, and we can compare each dimension, noting for example that the vectors for as you like it and twelfth night have similar values (1 and 0, respectively) for the first dimension.",vector space
,"the term-document matrix of fig. 6.2 was first defined as part of the vector space model of information retrieval (salton, 1971). in this model, a document is represented as a count vector, a column in fig. 6.3. vg to review some basic linear algebra, a vector is, at heart, just a list or array of 6 el",vector space model
,"number of documents can be enormous (think about all the pages on the web). information retrieval (ir) is the task of finding the document d from the d documents in some collection that best matches a query g. for ir we’ ll therefore also represent a query by a vector, also of length |v|, and we’ll need a way to compare two vectors to find how similar they are. (doing ir will also require efficient ways to store and manipulate these vectors by making use of the convenient fact that these vectors are sparse. i.e.. mostly zeros).",information retrieval
,"we’ve seen that documents can be represented as vectors in a vector space. bu vector semantics can also be used to represent the meaning of words. we do thi by associating each word with a word vector— a row vector rather than a colum vector, hence with different dimensions, as shown in fig. 6.5. the four dimension of the vector for fool, [36, 58, 1, 4], correspond to the four shakespeare plays. wor counts in the same four dimensions are used to form the vectors for the other | words: wit, [20, 15, 2, 3]; battle, [1, 0, 7, 13]; and good [114, 80, 62, 89].",row vector
,"to measure similarity between two target words v and w, we need a metric that takes two vectors (of the same dimensionality, either both with words as dimensions, hence of length |v|, or both with documents as dimensions as documents, of length |d|) and gives a measure of their similarity. by far the most common similarity metric is the cosine of the angle between the vectors.",cosine
,"this raw dot product, however, has a problem as a similarity metric: it favors long vectors. the vector length is defined as",vector length
,"for some applications we pre-normalize each vector, by dividing it by its length, creating a unit vector of length 1. thus we could compute a unit vector from a by dividing it by |a|. for unit vectors, the dot product is the same as the cosine. the cosine value ranges from | for vectors pointing in the same direction, through 0 for orthogonal vectors, to -1 for vectors pointing in opposite directions. but since raw frequency values are non-negative, the cosine for these vectors ranges from 0-1. neste a tocorcs coe face ih el bn cur esnan",unit vector
,"cei a a ut, lu te) lh dt, a ou ud the second factor in tf-idf is used to give a higher weight to words that occur only in a few documents. terms that are limited to a few documents are useful for discriminating those documents from the rest of the collection; terms that occur frequently across the entire collection aren’t as helpful. the document frequency df, of a term f is the number of documents it occurs in. document frequency is not the same as the collection frequency of a term, which is the total number of times the word appears in the whole collection in any document. consider in the collection of shakespeare’s 37 plays the two words romeo and action. the words have identical collection frequencies (they both occur 113 times in all the plays) but very different document frequencies, since romeo only occurs in a single play. if our goal is to find documents about the romantic tribulations of romeo, the word romeo should be highly weighted, but not action:",document frequency
,"st is the term frequency (luhn, 1957): the frequency of the word f in the 1. we can just use the raw count as the term frequency:",term frequency
,"the tf-idf weighting is the way for weighting co-occurrence matrices in infor mation retrieval, but also plays a role in many other aspects of natural languag processing. it’s also a great baseline, the simple thing to try first. we'll look at othe weightings like ppmi (positive pointwise mutual information) in section 6.6.",tf-idf
,"in a confusion of terminology, fano used the phrase mutual information to refer to what we now cal pointwise mutual information and the phrase expectation of the mutual information for what we now cal pase ei a pe",pointwise mutual information
,"thus for example we could compute ppmi(information, data), assuming we pre- ended that fig. 6.6 encompassed all the relevant word contexts/dimensions, as fol-",ppmi
,"the tf-idf model of meaning is often used for document functions like deciding if two documents are similar. we represent a document by taking the vectors of all the words in the document, and computing the centroid of all those vectors. the centroid is the multidimensional version of the mean; the centroid of a set of vectors is a single vector that has the minimum sum of squared distances to each of the vectors in the set. given k word vectors w1, w2, ..., wk, the centroid document vector d","centroid, document vector"
,"the revolutionary intuition here is that we can just use running text as implicitly supervised training data for such a classifier; a word c that occurs near the targe word apricot acts as gold ‘correct answer’ to the question “is word c likely to shov up near apricot?” this method, often called self-supervision, avoids the need fo: any sort of hand-labeled supervision signal. this idea was first proposed in the task of neural language modeling, when bengio et al. (2003) and collobert et al. (2011 showed that a neural language model (a neural network that learned to predict th«",self-supervision
,"visualizing embeddings is an important goal in helping understand, apply, and improve these models of word meaning. but how can we visualize a (for example) 100-dimensional vector?",visualizing embeddings
,"then a skipgram embedding is learned for each constituent n-gram, and the word where is represented by the sum of all of the embeddings of its constituent n-grams. unknown words can then be presented only by the sum of the constituent n-grams. a fasttext open-source library, including pretrained embeddings for 157 languages, ee exmrenn past in aylilll aie dread. eek",fasttext
,in this section we briefly summarize some of the semantic properties of embeddings that have been studied.,semantic properties of embeddings
,"aus diso oleh usclul lo gisums uist ewo kings of stlab ity of dssoclduoll delwccil words (schiitze and pedersen, 1993). two words have first-order co-occurrence ‘sometimes called syntagmatic association) if they are typically nearby each other. thus wrote is a first-order associate of book or poem. two words have second-order co-occurrence (sometimes called paradigmatic association) if they have similar aeighbors. thus wrote is a second-order associate of words like said or remarked.",first-order co-occurrence
,"analogy/relational similarity: © another semantic property of embeddings 1s their ability to capture relational meanings. in an important early vector space model of cognition, rumelhart and abrahamson (1973) proposed the parallelogram model for solving simple analogy problems of the form a is to b as a* is to what?. in such problems, a system given a problem like apple:tree::grape: ?, i.e., apple is to tree as grape is to , and must fill in the word vine. in the parallelogram model, illus- trated in fig. 6.15, the vector from the word apple to the word tree (= tree — apple) is added to the vector for grape (grapé); the nearest word to that point is returned. in early work with sparse embeddings, scholars showed that sparse vector mod-",parallelogram model
,"hh ee dee 5 ee ee eee eee). ee ee se father’ is to ‘doctor’ as ‘mother’ is to ‘nurse’. this could result in what crawford 2017) and blodgett et al. (2020) call an allocational harm, when a system allo- ates resources (jobs or credit) unfairly to different groups. for example algorithms hat use embeddings as part of a search for hiring potential programmers or doctors night thus incorrectly downweight documents with women’s names. it turns out that embeddings don’t just reflect the statistics of their input, but also",allocational harm
,"(gonen and goldberg, 2019), and this remains an open problem. historical embeddings are also being used to measure biases in the past. garg et al. (2018) used embeddings from historical texts to measure the association be- tween embeddings for occupations and embeddings for names of various ethnici- ties or genders (for example the relative cosine similarity of women’s names versus men’s to occupation words like ‘librarian’ or ‘carpenter’) across the 20th century they found that the cosines correlate with the empirical historical percentages ot women or ethnic groups in those occupations. historical embeddings also repli- cated old surveys of ethnic stereotypes; the tendency of experimental participants ir 1933 to associate adjectives like ‘industrious’ or ‘superstitious’ with, e.g., chinese ethnicity, correlates with the cosine between chinese last names and those adjectives using embeddings trained on 1930s text. they also were able to document historical gender biases, such as the fact that embeddings for adjectives related to competence (‘smart’, ‘wise’, ‘thoughtful’, ‘resourceful’) had a higher cosine with male than fe- male words, and showed that this bias has been slowly decreasing since 1960. we return in later chapters to this question about the role of bias in natural language processing.",garg
,"recent research focuses on ways to try to remove these kinds of biases, for example by developing a transformation of the embedding space that removes gen- der stereotypes but preserves definitional gender (bolukbasi et al. 2016, zhao et al. 2017) or changing the training procedure (zhao et al., 2018). however, although these sorts of debiasing may reduce bias in in embeddings, they do not eliminate it",debiasing
,"mechanical indexing, now known as information retrieval. in what became known as the vector space model for information retrieval (salton 1971, sparck jones 1986), researchers demonstrated new ways to define the meaning of words in terms of vectors (switzer, 1965), and refined methods for word similarity based on mea- sures of statistical association between words like mutual information (giuliano, 1965) and idf (sparck jones, 1972), and showed that the meaning of documents could be represented in the same vector spaces used for words.",mechanical indexing
,"the dimensions used by vector models of meaning to define words, however, are only abstractly related to this idea of a small fixed number of hand-built dimensions. nonetheless, there has been some attempt to show that certain dimensions of em- bedding models do contribute some specific compositional aspect of meaning like these early semantic features.",semantic feature
19_1,"this chapter assumes little prior knowledge on the part of the agent: it starts from scratch and learns from the data. in section 21.7.2'5 we consider transfer learning, in which knowledge from one domain is transferred to a new domain, so that learning can proceed faster with less data. we do assume, however, that the designer of the system chooses a model framework that can lead to effective learning.",prior knowledge
,"when the output is one of a finite set of values (such as sunny/cloudy/rainy or true/false), the learning problem is called classification. when it is a number (such as tomorrow’s temperature, measured either as an integer or a real number), the learning problem has t! (admittedly obscure!) name reeression.",classification
,"1a better name would have been function approximation or numeric prediction. but in 1886 francis galton wrote an influential article on the concept of regression to the mean (e.g., the children of tall parents are likely to be taller than average, but not as tall as the parents). galton showed plots with what he called “regression lines, ” and readers came to associate the word “regression” with the statistical technique of function approximation rather than with the topic of regression to the mean.",regression
,"in supervised learning the agent observes input-output pairs and learns a function that maps from input to output. for example, the inputs could be camera images, each one accompanied by an output saying “bus” or “pedestrian, ” etc. an output like this is callec a label. the agent learns a function that, when given a new image, predicts the appropriate label. in the case of braking actions (component 1 above), an input is the current state (speed and direction of the car, road condition), and an output is the distance it took to stop. in this case a set of output values can be obtained by the agent from its own percepts (after the fact); the environment is the teacher, and the agent learns a function that maps states to stopping distance.","supervised learning, label"
,"In unsupervised learning the agent learns patterns in the input without any explicit feedback. The most common unsupervised learning task is clustering: detecting potentially useful clusters of input examples. For example, when shown millions of images taken from the Internet, a computer vision system can identify a large cluster of similar images which an English speaker would call “cats.”","unsupervised learning, feedback"
,"In reinforcement learning the agent learns trom a series of reintorcements. rewards « punishments. for example, at the end of a chess game the agent is told that it has wo (a reward) or lost (a punishment). it is up to the agent to decide which of the actions prior to the reinforcement were most responsible for it, and to alter its actions to aim towardc more rewards in the filtiure.","reinforcement learning, Feedback"
,"the function h is called a hypothesis about the world. it is drawn from a hypothesis space h of possible functions. for example, the hypothesis space might be the set of polynomials of degree 3: or the set of javascript functions: or the set of 3-sat boolean logic formulas.",hypothesis space
,how do we choose a good hypothesis from within the hypothesis space? we could hope for a consistent hypothesis: and h such that each 2; in the training set has h(«;) = y;. with continuous-valued outputs we can’t expect an exact match to the ground truth; instead we look for a best-fit function for which each h(z;) is close to y; (in a way that we will formalize in section 19.4.2!b).,"ground truth, consistent hypothesis"
,"How do we choose a hypothesis space? We might have some prior knowledge about the process that generated the data. If not, we can perform exploratory data analysis: examining the data with statistical tests and visualizations—histograms, scatter plots, box plots—to get a feel for the data, and some insight into what hypothesis space might be appropriate. Or we can just try multiple hypothesis spaces and evaluate which one works best.",exploratory data analysis
,"(he true measure of a hypothesis is not how it does on the training set, but rather how we t handles inputs it has not yet seen. we can evaluate that with a second sample of (x;, y;) airs called a test set. we say that h generalizes well if it accurately predicts the outputs o:",test set
,one way to analyze hypothesis spaces is by the bias they impose (regardless of the training data eat) and the vwannancea they pradice (fram one traiminoe eat tr anather),bias
,"by bias we mean (loosely) the tendency of a predictive hypothesis to deviate from the expected value when averaged over different training sets. bias often results from restrictions imposed by the hypothesis space. for example, the hypothesis space of linear functions induces a strong bias: it only allows functions consisting of straight lines. if there are any patterns in the data other than the overall slope of a line, a linear function will not be able to represent those patterns. we say that a hypothesis is underfitting when it fails t find a pattern in the data. on the other hand, the piecewise linear function has low bias; th shape of the function is driven by the data.",underfitting
,"by variance we mean the amount of change in the hypotnesis due to fluctuation in the training data. the two rows of figure 19.1 represent data sets that were each sampled from the same f(x) function. the data sets turned out to be slightly different. for the first three columns, the small difference in the data set translates into a small difference in the hypothesis. we call that low variance. but the degree-12 polynomials in the fourth column have high variance: look how different the two functions are at both ends of the a-axis. clearly, at least one of these polynomials must be a poor approximation to the true f(«). w say a function is overfitting the data when it pays too much attention to the particular data sp i an ela a om ca","variance, overfitting"
,"a decision tree is a representation of a function that maps a vector of attribute values to a single output value—a “decision.” a decision tree reaches its decision by performing a sequence of tests, starting at the root and following the appropriate branch until a leaf is reached. each internal node in the tree corresponds to a test of the value of one of the input attributes, the branches from the node are labeled with the possible values of the attribute, and the leaf nodes specify what value is to be returned bv the function.",decision tree
,"in general, the input and output values can be discrete or conunuous, but for now we will consider only inputs consisting of discrete values and outputs that are either true (a positive example) or false (a negative example). we call this boolean classification. we will use j to index the examples (x; is the input vector for the jth example and yj; is the output), and xj, for the ith attribute of the 7th example.","negative, positive"
,"we can evaluate the performance of a learning algorithm with a learning curve, as shown in figure 19.7. for this figure we have 100 examples at our disposal, which we split randomly into a training set and a test set. we learn a hypothesis h with the training set and measure its accuracy with the test set. we can do this starting with a training set of size 1 and increasing one at a time up to size 99. for each size, we actually repeat the process of randomly splitting into training and test sets 20 times, and average the results of the 20 trials. the curve shows that as the training set size grows, the accuracy increases. (for this reason, learning curves are also called happy graphs.) in this graph we reach 95% accuracy, and it looks as if the curve might continue to increase if we had more data.","learning curve, happy graphs"
,"the decision tree learning algorithm chooses the attribute with the highest importance. we will now show how to measure importance, using the notion of information gain, which is defined in terms of entropy, which is the fundamental quantity in information theory (shannon and weaver 1646)","entropy, information gain"
,"for decision trees, a technique called decision tree pruning combats overfitting. pruning works by eliminating nodes that are not clearly relevant. we start with a full tree, as generated by learn-decision-tree. we then look at a test node that has only leaf nodes as descendants. if the test appears to be irrelevant—detecting only noise in the data—then we eliminate the test, replacing it with a leaf node. we repeat this process, considering each test with only leaf descendants, until each one has either been pruned or accepted as is.",decision tree pruning
,"we can answer this question dy using a statistical significance test. ouch a test begins dy assuming that there is no underlying pattern (the so-called null hypothesis). then the actual data are analyzed to calculate the extent to which they deviate from a perfect absen« of pattern. if the degree of deviation is statistically unlikely (usually taken to mean a 5% probability or less), then that is considered to be good evidence for the presence of a significant pattern in the data. The probabilities are calculated from standard distributions of
the amount of deviation one would expect to see in random sampling.","significance test, null hypothesis"
,"one final warning: you might think that x” pruning and information gain look similar, so why not combine them using an approach called early stopping—have the decision tree algorithm stop generating nodes when there is no good attribute to split on, rather than going to all the trouble of generating nodes and then pruning them away. the problem wit early stopping is that it stops us from recognizing situations where there is no one good attribute, but there are combinations of attributes that are informative. for example, consider the xor function of two binary attributes. if there are roughly equal numbers of examples for all four combinations of input values, then neither attribute will be informative, yet the correct thing to do is to split on one of the attributes (it doesn’t matter which one), and then at the second level we will get splits that are very informative. early stopping would miss this, but generate-and-then-prune handles it correctly.",early stopping
,"a better way to deal with continuous values is a split point test—an inequality test on the value of an attribute. for example, at a given node in the tree, it might be the case that testing on weight > 160 gives the most information. efficient methods exist for finding good split points: start by sorting the values of the attribute, and then conside only split points that are between two examples in sorted order that have different classifications, while keeping track of the running totals of positive and negative examples on each side of the split point. splitting is the most expensive part of real- world decision tree learning applications. hd as aes his ere ste bae eis. meres egearreed spe cas sh bh, ere moe eeiccratacs, series tases",split point
,"continuous-valued output attribute: if we are trying to predict a numerical output value, such as the price of an apartment, then we need a regression tree rather than a classification tree. a regression tree has at each leaf a linear functio: of some subset of numerical attributes, rather than a single output value. for example the branch for two-bedroom apartments might end with a linear function of square footage and number of bathrooms. the learning algorithm must decide when to stop splitting and begin applying linear regression (see section 19.612) over the attributes. the name cart, standing for classification and regression trees, is used to cover both classes.","cart, regression tree"
,"decision trees have a lot going for them: ease of understanding, scalability to large data sets, and versatility in handling discrete and continuous inputs as well as classification and regression. however, they can have suboptimal accuracy (largely due to the greedy search), and if trees are very deep, then getting a prediction for a new example can be expensive in run time. decision trees are also unstable in that adding just one new example can change the test at the root, which changes the entire tree. in section 19.8.2! we will see that the random forest model can fix some of these issues.",unstable
,"suppose a researcher generates a hypotheses for one setting of the x” pruning hyperparameter, measures the error rates on the test set, and then tries different hyperparameters. no individual hypothesis has peeked at the test set data, but the o process did, through the researcher.","hyperparameters, error rate"
,"If there are no attributes left, but both positive and negative examples, it means that
these examples have exactly the same description, but different classifications. This
can happen because there is an error or noise in the data; because the domain is
nondeterministic; or because we can’t observe an attribute that would distinguish
the examples. The best we can do is return the most common output value of the
remaining examples.",noise
,"First we will make the assumption that the future examples will be like the past. We call this the stationarity assumption; without it, all bets are off. We assume that each example has the same prior probability distribution",Stationarity
,"If we are only going to create one hypothesis, then this approach is sufficient. But often we will end up creating multiple hypotheses: we might want to compare two completely different machine learning models, or we might want to adjust the various “knobs” within one model. For example, we could try different thresholds for χ2 pruning of decision trees,  or different degrees for polynomials. We call these “knobs” hyperparameters—parameters of the model class, not of the individual model.",hyperparameters
,"A validation set, also known as a development set or dev set, to evaluate the
candidate models and choose the best one.",Validation set
,"What if we don’t have enough data to make all three of these data sets? We can squeeze
more out of the data using a technique called k-fold cross-validation. The idea is that each
example serves double duty—as training data and validation data—but not at the same time.
First we split the data into k equal subsets. We then perform k rounds of learning; on each
round 1/k of the data are held out as a validation set and the remaining examples are used
as the training set. The average test set score of the k rounds should then be a better
estimate than a single score. Popular values for k are 5 and 10—enough to give an estimate
that is statistically likely to be accurate, at a cost of 5 to 10 times longer computation time.
The extreme is k = n, also known as leave-one-out cross-validation or LOOCV. Even with
cross-validation, we still need a separate test set.","K-fold cross-validation, LOOCV"
,"part of model selection is qualitative and subjective: we might select polynomials rathet than decision trees based on something that we know about the problem. and part is quantitative and empirical: within the class of polynomials, we might select degree = 2 because that value performs best on the validation data set.",model selection