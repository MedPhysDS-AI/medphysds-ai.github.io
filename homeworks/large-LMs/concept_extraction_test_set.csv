Book,Chapter,Text,Concepts
SLP,,"The final layer matrix v provides a way to score the likelihood of each word in the vocabulary given the evidence present in the final hidden layer of the network through the calculation of vh. This results in a dimensionality |v| x dy. That is, the rows of v provide a second set of learned word embeddings that capture relevant aspects of word meaning and function. This leads to an obvious question — is it even necessary to have both? Weight tying is a method that dispenses with this redundancy and simply uses a single set of embeddings at the input and softmax layers. That is, we dispense with v and use £ in both the start and end of the computation.","weight tying, embeddings"
SLP,,"For some applications, we pre-normalize each vector by dividing it by its length, creating a unit vector of length 1. Thus, we could compute a unit vector from a by dividing it by |a|. For unit vectors, the dot product is the same as the cosine. The cosine value ranges from | for vectors pointing in the same direction, through 0 for orthogonal vectors, to -1 for vectors pointing in opposite directions. But since raw frequency values are non-negative, the cosine for these vectors ranges from 0-1.",unit vector
AIMA,chapter6_2,"The key idea is local consistency. If we treat each variable as a node in a graph and each binary constraint as an edge, then the process of enforcing local consistency in each part of the graph causes inconsistent values to be eliminated throughout the graph. There are different types of local consistency, which we now cover in turn.",Local consistency
AIMA,chapter7_1,"There must be a way to add new sentences to the knowledge base and a way to query what is known. The standard names for these operations are TELL and ASK, respectively. Both operations may involve inference—that is, deriving new sentences from old. Inference must obey the requirement that when one ASKs a question of the knowledge base, the answer should follow from what has been told (or TELLed) to the knowledge base previously. Later in this chapter, we will be more precise about the crucial word “follow.” For now, take it to mean that the inference process should not make things up as it goes along.",Inference
SLP,slp_4,"So, in our example, this p-value is the probability that we would see 5(x) assuming a is not better than b. If 5(x) is huge (let’s say a has a very respectable f-score of .9 and b has a terrible f-score of only .2 on x), we might be surprised, since that would be extremely unlikely to occur if H_0 were in fact true, and so the p-value would be low.",p-value
SLP,slp-7,"A compromise is mini-batch training: we train on a group of m examples (perhaps 512, or 1024) that is less than the whole dataset. (If m is the size of the dataset, then we are doing batch gradient descent; if m = 1, we are back to doing stochastic gradient descent). Mini-batch training also has the advantage of computational efficiency. The mini-batches can easily be vectorized, choosing the size of the minibatch based on the computational resources. This allows us to process all the examples in one mini-batch in parallel and then accumulate the loss, something that’s not possible with individual or batch training.","batch training, mini-batch"
AIMA,chapter2_3,"EPISODIC VS. SEQUENTIAL: In an episodic task environment, the agent’s experience is divided into atomic episodes. In each episode the agent receives a percept and then performs a single action. Crucially, the next episode does not depend on the actions taken in previous episodes. Many classification tasks are episodic. For example, an agent that has to spot defective parts on an assembly line bases each decision on the current part, regardless of previous decisions; moreover, the current decision doesn’t affect whether the next part is defective. In sequential environments, on the other hand, the current decision could affect all future decisions.4 Chess and taxi driving are sequential: in both cases, short-term actions can have long-term consequences. Episodic environments are much simpler than sequential environments because the agent does not need to think ahead.","Episodic, Sequential, Static, Dynamic"
AIMA,chapter2_2,"A rational agent is one that does the right thing. Obviously, doing the right thing is better than doing the wrong thing, but what does it mean to do the right thing?",Rational agent
AIMA,chapter22_1,"» model-free reinforcement learning: in these approaches the agent neitherknows nor learns a transition model for the environment. instead, it learns a more directvttenveaceitaticus at lexus ies, khohkaue: ""this ancmmacin arracnithan: wareieac:",model-free reinforcement learning
SLP,slp-7,"In the rest of the book we'll represent such sums using the dot product notation from linear algebra. The dot product of two vectors a and b, written as a•b is the sum of the products of the corresponding elements of each vector. (Notice that we represent vectors using the boldface notation b). Thus the following is an equivalent formation tag fa 5 9°.",dot product
SLP,slp-7,"Speech processing, both of which had made use of regression, and both of which lent many other statistical techniques to NLP. Indeed, a very early use of logistic regression for document routing was one of the first NLP applications to use (LSI) embeddings as word representations (Schiitze et al., 1995).",logistic regression
SLP,slp_4,Recall measures the percentage of items actually present in the input that were correctly identified by the system. Recall is defined as,recall
AIMA,chapter6_3,"One of the simplest forms of inference is called forward checking. Whenever a variable X is assigned, the forward-checking process establishes arc consistency for it: for each unassigned variable Y that is connected to X by a constraint, delete from Y ’s domain any value that is inconsistent with the value chosen for X.",forward-checking
AIMA,chapter2_3,"STATIC VS. DYNAMIC: If the environment can change while an agent is deliberating, then we say the environment is dynamic for that agent; otherwise, it is static. Static environments are easy to deal with because the agent need not keep looking at the world while it is deciding on an action, nor need it worry about the passage of time. Dynamic environments, on the other hand, are continuously asking the agent what it wants to do; if it hasn’t decided yet, that counts as deciding to do nothing. If the environment itself does not change with the passage of time but the agent’s performance score does, then we say the environment is semidynamic. Taxi driving is clearly dynamic: the other cars and the taxi itself keep moving while the driving algorithm dithers about what to do next. Chess, when played with a clock, is semidynamic. Crossword puzzles are static.",Semidynamic
AIMA,chapter17_2,"The basic concept used in showing that value iteration converges is the notion of a contraction. Roughly speaking, a contraction is a function of one argument that, when applied to two different inputs in turn, produces two output values that are “closer together, ” by at least some constant factor, than the original inputs. For example, the function “divide by two” is a contraction, because, after we divide any two numbers by two, their difference is halved. Notice that the “divide by two” function has a fixed point, namely zero, that is unchanged by the application of the function. From this example, we can discern two important properties of contractions: A contraction has only one fixed point; if there were two fixed points they would not get closer together when the function was applied, so it would not be a contraction. When the function is applied to any argument, the value must get closer to the fixed point (because the fixed point does not move), so repeated application of a contraction always reaches the fixed point in the limit.",contraction
AIMA,chapter17_2,"The Bellman equation (Equation (17.5) ) is the basis of the value iteration algorithm for solving MDPs. If there are n possible states, then there are n Bellman equations, one for each state. The n equations contain n unknowns—the utilities of the states. So we would like to solve these simultaneous equations to find the utilities. There is one problem: the equations are nonlinear, because the “max” operator is not a linear operator. Whereas systems of linear equations can be solved quickly using linear algebra techniques, systems of nonlinear equations are more problematic. One thing to try is an iterative approach. We start with arbitrary initial values for the utilities, calculate the right-hand side of the equation, and plug it into the left-hand side—thereby updating the utility of each state from the utilities of its neighbors. We repeat this until we reach an equilibrium.",Value iteration
SLP,slp-7,"How shall we find the minimum of this (or any) loss function? Gradient descent is a method that finds a minimum of a function by figuring out in which direction (in the space of the parameters @) the function’s slope is rising the most steeply, and moving in the opposite direction. The intuition is that if you are hiking in a canyon and trying to descend most quickly down to the river at the bottom, you might look around yourself 360 degrees, find the direction where the ground is sloping the steepest and walk downhill in that direction.","gradient descent, gradient descent"
AIMA,chapter2_2,"What is rational at any given time depends on four things: The performance measure that defines the criterion of success. The agent’s prior knowledge of the environment. The actions that the agent can perform. The agent’s percept sequence to date. This leads to a definition of a rational agent: For each possible percept sequence, a rational agent should select an action that is expected to maximize its performance measure, given the evidence provided by the percept sequence and whatever built-in knowledge the agent has.",Definition of a rational agent
AIMA,chapter17_3,"Such an optimal policy, if it exists, is called a dominating policy. It turns out that by addingactions to states, it is always possible to create a relaxed version of an MDP (see Section3.6.2 ) so that it has a dominating policy, which thus gives an upper bound on the value ofacting in the arm. A lower bound can be computed by solving each arm separately (whichmay yield a suboptimal policy overall) and then computing the Gittins indices. If the lowerbound for acting in one arm is higher than the upper bounds for all other actions, then theproblem is solved; if not, then a combination of look-ahead search and recomputation ofbounds is guaranteed to eventually identify an optimal policy for the BSP. With thisapproach, relatively large BSPs (1040 states or more) can be solved in a few seconds.",Dominating policy
SLP,,"Then a skipgram embedding is learned for each constituent n-gram, and the word ""where"" is represented by the sum of all of the embeddings of its constituent n-grams. Unknown words can then be presented only by the sum of the constituent n-grams. A fasttext open-source library, including pretrained embeddings for 157 languages, can be found here: exmrenn past in aylilll aie dread. eek",fasttext
AIMA,chapter3_2,"Automatic assembly sequencing of complex objects (such as electric motors) by a robot has been standard industry practice since the 1970s. Algorithms first find a feasible assembly sequence and then work to optimize the process. Minimizing the amount of manual human labor on the assembly line can produce significant savings in time and cost. In assembly problems, the aim is to find an order in which to assemble the parts of some object. If the wrong order is chosen, there will be no way to add some part later in the sequence without undoing some of the work already done. Checking an action in the sequence for feasibility is a difficult geometrical search problem closely related to robot navigation. Thus, the generation of legal actions is the expensive part of assembly sequencing. Any practical algorithm must avoid exploring all but a tiny fraction of the state space. One important assembly problem is protein design, in which the goal is to find a sequence of amino acids that will fold into a three-dimensional protein with the right properties to cure some disease.","Automatic assembly sequencing, Protein design"
AIMA,,"Linear programming or LP, which was mentioned briefly in Chapter 4 (page 121), is a general approach for formulating constrained optimization problems, and there are many industrial-strength LP solvers available. Given that the Bellman equations involve a lot of sums and maxes, it is perhaps not surprising that solving an MDP can be reduced to solving a suitably formulated linear program.",Linear programming
SLP,slp_4,"At its heart, a neural unit is taking a weighted sum of its inputs, with one additional term in the sum called a bias term. Given a set of inputs x)…x, a unit has a set of corresponding weights w)…w, and a bias b, so the weighted sum z can be represented as:",bias term
AIMA,chapter2_3,"One final note: the word stochastic is used by some as a synonym for “nondeterministic, ” but we make a distinction between the two terms; we say that a model of the environment is stochastic if it explicitly deals with probabilities (e.g., “there’s a 25% chance of rain tomorrow”) and “nondeterministic” if the possibilities are listed without being quantified (e.g., “there’s a chance of rain tomorrow”).",Stochastic
AIMA,chapter6_1,"CSPs deal with assignments of values to variables, {Xi = vi, Xj = vj, …}. An assignment that does not violate any constraints is called a consistent or legal assignment. A complete assignment is one in which every variable is assigned a value, and a solution to a CSP is a consistent, complete assignment. A partial assignment is one that leaves some variables unassigned, and a partial solution is a partial assignment that is consistent. Solving a CSP is an NP-complete problem in general, although there are important subclasses of CSPs that can be solved very efficiently.","Assignments, Consistent, Complete assignment, Solution, Partial assignment, Partial solution"
AIMA,chapter1_2,"To say that a program thinks like a human, we must know how humans think. We can learn about human thought in three ways: introspection—trying to catch our own thoughts as they go by; psychological experiments—observing a person in action; brain imaging—observing the brain in action.","Introspection, Psychological experiments, Brain imaging"
SLP,slp-7,"Often we want to know more than just the correct classification of an observation. We want to know why the classifier made the decision it did. That is, we want out decision to be interpretable. Interpretability can be hard to define strictly, but the core idea is that as humans we should know why our algorithms reach the conclusions they do. Because the features to logistic regression are often human-designed, one way to understand a classifier’s decision is to understand the role each feature plays in the decision. Logistic regression can be combined with statistical tests (the likelihood ratio test, or the wald test); investigating whether a particular feature is significant by one of these tests, or inspecting its magnitude (how large is the weight)",interpretable
AIMA,chapter22_1,"iyan adaptive dynamic programming (or adp) agent takes advantage of the constraintsamong the utilities of states by learning the transition model that connects them and solvingthe corresponding markov decision process using dynamic programming. for a passivelearning agent, this means plugging the learned transition model p(s'|s, 1(s)) and theobserved rewards r(s, 7(s), s’) into equation (22.2) to calculate the utilities of the states. aswe remarked in our discussion of policy iteration in chapter 17, these bellman equationsare linear when the policy 7 is fixed, so thev can be solved using any linear algebra package.",adaptive dynamic programming
AIMA,chapter6_1,"The constraints we have described so far have all been absolute constraints, violation of which rules out a potential solution. Many real-world CSPs include preference constraints indicating which solutions are preferred. For example, in a university class-scheduling problem there are absolute constraints that no professor can teach two classes at the same time. But we also may allow preference constraints: Prof. R might prefer teaching in the morning, whereas Prof. N prefers teaching in the afternoon. A schedule that has Prof. R teaching at 2 p.m. would still be an allowable solution (unless Prof. R happens to be the department chair) but would not be an optimal one.",Preference constraints
SLP,,"The role of context is also important in the similarity of a less biological kind of organism: the word. Words that occur in similar contexts tend to have similar meanings. This link between similarity in how words are distributed and similarity in what they mean is called the distributional hypothesis. The hypothesis was first formulated in the 1950s by linguists like Joos (1950), Harris (1954), and Firth.",distributional hypothesis
AIMA,chapter1_4,"In the “laws of thought” approach to AI, the emphasis was on correct inferences. Making correct inferences is sometimes part of being a rational agent, because one way to act rationally is to deduce that a given action is best and then to act on that conclusion. On the other hand, there are ways of acting rationally that cannot be said to involve inference. For example, recoiling from a hot stove is a reflex action that is usually more successful than a slower action taken after careful deliberation. All the skills needed for the Turing test also allow an agent to act rationally. Knowledge representation and reasoning enable agents to reach good decisions. We need to be able to generate comprehensible sentences in natural language to get by in a complex society. We need learning not only for erudition, but also because it improves our ability to generate effective behavior, especially in circumstances that are new. The rational-agent approach to AI has two advantages over the other approaches. First, it is more general than the “laws of thought” approach because correct inference is just one of several possible mechanisms for achieving rationality. Second, it is more amenable to scientific development. The standard of rationality is mathematically well defined and completely general. We can often work back from this specification to derive agent designs that provably achieve it—something that is largely impossible if the goal is to imitate human behavior or thought processes. For these reasons, the rational-agent approach to AI has prevailed throughout most of the field’s history. In the early decades, rational agents were built on logical foundations and formed definite plans to achieve specific goals. Later, methods based on probability theory and machine learning allowed the creation of agents that could make decisions under uncertainty to attain the best expected outcome. In a nutshell, AI has focused on the study and construction of agents that do the right thing. What counts as the right thing is defined by the objective that we provide to the agent. This general paradigm is so pervasive that we might call it the standard model. It prevails not only in AI, but also in control theory, where a controller minimizes a cost function; in operations research, where a policy maximizes a sum of rewards; in statistics, where a decision rule minimizes a loss function; and in economics, where a decision maker maximizes utility or some measure of social welfare.","Do the right thing, Standard model"
AIMA,chapter7_5,"One final property of logical systems is monotonicity, which says that the set of entailed sentences can only increase as information is added to the knowledge base. For any sentences α and β, if KB ⊨ α then KB ∧ β ⊨ α .",Monotonicity
AIMA,chapter7_4,"We now present propositional logic. We describe its syntax (the structure of sentences) and its semantics (the way in which the truth of sentences is determined). From these, we derive a simple, syntactic algorithm for logical inference that implements the semantic notion of entailment.",Propositional logic
SLP,,"How do we choose a hypothesis space? We might have some prior knowledge about the process that generated the data. If not, we can perform exploratory data analysis: examining the data with statistical tests and visualizations—histograms, scatter plots, box plots—to get a feel for the data, and some insight into what hypothesis space might be appropriate. Or we can just try multiple hypothesis spaces and evaluate which one works best.",exploratory data analysis
AIMA,chapter6_3,"The BACKTRACKING-SEARCH algorithm in Figure 6.5 has a very simple policy for what to do when a branch of the search fails: back up to the preceding variable and try a different value for it. This is called chronological backtracking because the most recent decision point is revisited. In this subsection, we consider better possibilities.",Chronological backtracking
SLP,,"In reinforcement learning, the agent learns from a series of reinforcements (rewards or punishments). For example, at the end of a chess game, the agent is told that it has won (a reward) or lost (a punishment). It is up to the agent to decide which of the actions prior to the reinforcement were most responsible for it, and to alter its actions to aim toward more rewards in the future.","reinforcement learning, Feedback"
AIMA,chapter22_3,"performance of a greedy adp agent that executes the action recommended by the optimal policy for thelearned model. (a) the root mean square (rms) error averaged across all nine nonterminal squares andthe policy loss in (1, 1). we see that the policy converges quickly, after just eight trials, to a suboptimalpolicy with a loss of 0.235. (b) the suboptimal policy to which the greedy agent converges in thisparticular sequence of trials. notice the down action in (1, 2).",greedy agent
SLP,slp_4,"Instead, a modern neural network is a network of small computing units, each of which takes a vector of input values and produces a single output value. In this chapter we introduce the neural net applied to classification. The architecture we introduce is called a feedforward network because the computation proceeds sequentially from one layer of units to the next. The use of modern neural nets is often called deep learning, because modern networks are often deep (have many layers).",deep learning
AIMA,chapter12_1,ranges can be sets of arbitrary tokens. we might choose the range of age to be the setedumomiiadoen ntti ana tia-rance:nf uw eontheswmicht he: feun. sedm: dinad ennnk uhean:,range
AIMA,chapter17_2,"In this section, we present four different algorithms for solving MDPs. The first three, value iteration, policy iteration, and linear programming, generate exact solutions offline. The fourth is a family of online approximate algorithms that includes Monte Carlo planning.",Monte Carlo planning